{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pylab import *\n",
    "import shutil #for split into train, val, test\n",
    "import random\n",
    "import seaborn as sns #visualizing the data\n",
    "import sklearn as sk\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[Errno 17] File exists: 'data/train/Healthy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "Input \u001b[0;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m test_ratio \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m \u001b[38;5;66;03m# 5% of total images -> test\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01min\u001b[39;00m class_dir:\n\u001b[0;32m---> 13\u001b[0m     \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroot_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(root_dir \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m     15\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(root_dir \u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mFileExistsError\u001b[0m: [Errno 17] File exists: 'data/train/Healthy'"
     ]
    }
   ],
   "source": [
    "class_dir = ['Healthy', 'Brain_Tumor'] #labels\n",
    "\n",
    "# source 1: https://www.geeksforgeeks.org/python-shutil-copy-method/\n",
    "# source 2: https://www.kaggle.com/questions-and-answers/102677\n",
    "\n",
    "# directory for images (jpg)\n",
    "root_dir = 'data/' # data root path\n",
    "\n",
    "val_ratio = 0.15 # 15% of total images -> val  \n",
    "test_ratio = 0.05 # 5% of total images -> test\n",
    "\n",
    "for cls in class_dir:\n",
    "    os.makedirs(root_dir +'train/' + cls)\n",
    "    os.makedirs(root_dir +'val/' + cls)\n",
    "    os.makedirs(root_dir +'test/' + cls)\n",
    "\n",
    "    # Creating partitions of the data after shuffeling\n",
    "    src = root_dir + cls # Folder to copy images from\n",
    "    allFileNames = os.listdir(src)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                          [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n",
    "                                                           int(len(allFileNames)* (1 - test_ratio))])\n",
    "\n",
    "    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "\n",
    "    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "\n",
    "    print(\"\\n*****************************\")\n",
    "    print('Total images: ', len(allFileNames), cls)\n",
    "    print('Training: ', len(train_FileNames), cls)\n",
    "    print('Validation: ', len(val_FileNames), cls)\n",
    "    print('Testing: ', len(test_FileNames), cls)\n",
    "    print(\"*****************************\")\n",
    "\n",
    "    try:\n",
    "        # Copy-pasting folders\n",
    "        for name in train_FileNames:\n",
    "            shutil.copy(name, root_dir +'train/' + cls)\n",
    "\n",
    "        print(\"Train copied successfully.\")\n",
    "\n",
    "        for name in val_FileNames:\n",
    "            shutil.copy(name, root_dir +'val/' + cls)\n",
    "\n",
    "        print(\"Val copied successfully.\")\n",
    "\n",
    "        for name in test_FileNames:\n",
    "            shutil.copy(name, root_dir +'test/' + cls)\n",
    "\n",
    "        print(\"Test copied successfully.\")\n",
    "\n",
    "    # If source and destination are same\n",
    "    except shutil.SameFileError:\n",
    "        print(\"Source and destination represents the same file.\")\n",
    "\n",
    "    # if there is any permission issue\n",
    "    except PermissionError:\n",
    "        print(\"Permission denied.\")\n",
    "\n",
    "    # For other errors\n",
    "    except:\n",
    "        print(\"Error occurred while copying file.\")\n",
    "        \n",
    "print(\"Successfully copied files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3679 images belonging to 2 classes.\n",
      "Found 690 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train = ImageDataGenerator(rescale=1/255)\n",
    "test = ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_dataset = train.flow_from_directory(\"/home/nickknapton/Documents/School/BMEN415/project/BMEN415_Final_Project/Classification/GroupImageModel/data/train/\",\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size = 32,\n",
    "                                          class_mode = 'binary',\n",
    "                                          shuffle=False)\n",
    "                                         \n",
    "test_dataset = test.flow_from_directory(\"/home/nickknapton/Documents/School/BMEN415/project/BMEN415_Final_Project/Classification/GroupImageModel/data/val/\",\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size =32,\n",
    "                                          class_mode = 'binary',\n",
    "                                           shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = test_dataset.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# Convolutional layer and maxpool layer 1\n",
    "model.add(keras.layers.Conv2D(32,(3,3),activation='relu',input_shape=(150,150,3)))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer and maxpool layer 2\n",
    "model.add(keras.layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer and maxpool layer 3\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# Convolutional layer and maxpool layer 4\n",
    "model.add(keras.layers.Conv2D(128,(3,3),activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "\n",
    "# This layer flattens the resulting image array to 1D array\n",
    "model.add(keras.layers.Flatten())\n",
    "\n",
    "# Hidden layer with 512 neurons and Rectified Linear Unit activation function \n",
    "model.add(keras.layers.Dense(512,activation='relu'))\n",
    "\n",
    "# Output layer with single neuron which gives 0 for Cat or 1 for Dog \n",
    "#Here we use sigmoid activation function which makes our model output to lie between 0 and 1\n",
    "model.add(keras.layers.Dense(1,activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "115/115 [==============================] - 85s 736ms/step - loss: 0.7495 - accuracy: 0.5134 - val_loss: 0.6921 - val_accuracy: 0.5464\n",
      "Epoch 2/10\n",
      "115/115 [==============================] - 120s 1s/step - loss: 0.6908 - accuracy: 0.6235 - val_loss: 0.6666 - val_accuracy: 0.5739\n",
      "Epoch 3/10\n",
      "115/115 [==============================] - 127s 1s/step - loss: 0.6786 - accuracy: 0.5904 - val_loss: 0.6759 - val_accuracy: 0.6159\n",
      "Epoch 4/10\n",
      " 28/115 [======>.......................] - ETA: 1:44 - loss: 0.6586 - accuracy: 0.6266"
     ]
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=test_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 231 images belonging to 2 classes.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/nickknapton/Documents/School/BMEN415/project/BMEN415_Final_Project/Classification/GroupImageModel/data/val/Brain_Tumor/Cancer (100).jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [58]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m test2w_dataset \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mflow_from_directory(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/nickknapton/Documents/School/BMEN415/project/BMEN415_Final_Project/Classification/GroupImageModel/data/test/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      2\u001b[0m                                           target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m150\u001b[39m,\u001b[38;5;241m150\u001b[39m),\n\u001b[1;32m      3\u001b[0m                                           batch_size \u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m      4\u001b[0m                                           class_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      7\u001b[0m Y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_dataset)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(Y_pred)\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1354\u001b[0m, in \u001b[0;36mModel.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1351\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_data_handler\n\u001b[1;32m   1352\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1353\u001b[0m   \u001b[38;5;66;03m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[0;32m-> 1354\u001b[0m   data_handler \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataHandler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1355\u001b[0m \u001b[43m      \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1356\u001b[0m \u001b[43m      \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1357\u001b[0m \u001b[43m      \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1358\u001b[0m \u001b[43m      \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1359\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1360\u001b[0m \u001b[43m      \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1361\u001b[0m \u001b[43m      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1362\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m      \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1364\u001b[0m \u001b[43m      \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[43m      \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1366\u001b[0m \u001b[43m      \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_steps_per_execution\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1368\u001b[0m \u001b[38;5;66;03m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:1100\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1097\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_per_execution_value \u001b[38;5;241m=\u001b[39m steps_per_execution\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m   1099\u001b[0m adapter_cls \u001b[38;5;241m=\u001b[39m select_data_adapter(x, y)\n\u001b[0;32m-> 1100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter \u001b[38;5;241m=\u001b[39m \u001b[43madapter_cls\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistribution_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mds_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_strategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1114\u001b[0m strategy \u001b[38;5;241m=\u001b[39m ds_context\u001b[38;5;241m.\u001b[39mget_strategy()\n\u001b[1;32m   1115\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adapter\u001b[38;5;241m.\u001b[39mget_dataset()\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:902\u001b[0m, in \u001b[0;36mKerasSequenceAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, shuffle, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keras_sequence \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    901\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enqueuer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 902\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mKerasSequenceAdapter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Shuffle is handed in the _make_callable override.\u001b[39;49;00m\n\u001b[1;32m    905\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:779\u001b[0m, in \u001b[0;36mGeneratorDataAdapter.__init__\u001b[0;34m(self, x, y, sample_weights, workers, use_multiprocessing, max_queue_size, model, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;28msuper\u001b[39m(GeneratorDataAdapter, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(x, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    777\u001b[0m \u001b[38;5;66;03m# Since we have to know the dtype of the python generator when we build the\u001b[39;00m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;66;03m# dataset, we have to look at a batch to infer the structure.\u001b[39;00m\n\u001b[0;32m--> 779\u001b[0m peek, x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_peek_and_restore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    780\u001b[0m peek \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_standardize_batch(peek)\n\u001b[1;32m    781\u001b[0m peek \u001b[38;5;241m=\u001b[39m _process_tensorlike(peek)\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/tensorflow/python/keras/engine/data_adapter.py:913\u001b[0m, in \u001b[0;36mKerasSequenceAdapter._peek_and_restore\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_peek_and_restore\u001b[39m(x):\n\u001b[0;32m--> 913\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, x\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py:65\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_index_array()\n\u001b[1;32m     63\u001b[0m index_array \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex_array[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m idx:\n\u001b[1;32m     64\u001b[0m                                \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m*\u001b[39m (idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)]\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_batches_of_transformed_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_array\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/keras_preprocessing/image/iterator.py:227\u001b[0m, in \u001b[0;36mBatchFromFilesMixin._get_batches_of_transformed_samples\u001b[0;34m(self, index_array)\u001b[0m\n\u001b[1;32m    225\u001b[0m filepaths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepaths\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(index_array):\n\u001b[0;32m--> 227\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mload_img\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepaths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcolor_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m                   \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpolation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m img_to_array(img, data_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_format)\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# Pillow images should be closed after `load_img`,\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# but not PIL images.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/bmen415/lib/python3.9/site-packages/keras_preprocessing/image/utils.py:113\u001b[0m, in \u001b[0;36mload_img\u001b[0;34m(path, grayscale, color_mode, target_size, interpolation)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pil_image \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCould not import PIL.Image. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    112\u001b[0m                       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe use of `load_img` requires PIL.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    114\u001b[0m     img \u001b[38;5;241m=\u001b[39m pil_image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(f\u001b[38;5;241m.\u001b[39mread()))\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m color_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrayscale\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;66;03m# if image is not already an 8-bit, 16-bit or 32-bit grayscale image\u001b[39;00m\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;66;03m# convert it to an 8-bit grayscale image.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/nickknapton/Documents/School/BMEN415/project/BMEN415_Final_Project/Classification/GroupImageModel/data/val/Brain_Tumor/Cancer (100).jpg'"
     ]
    }
   ],
   "source": [
    "test2w_dataset = test.flow_from_directory(\"/home/nickknapton/Documents/School/BMEN415/project/BMEN415_Final_Project/Classification/GroupImageModel/data/test/\",\n",
    "                                          target_size=(150,150),\n",
    "                                          batch_size =32,\n",
    "                                          class_mode = 'binary')\n",
    "print(model.evaluate(test_dataset, verbose=1))\n",
    "\n",
    "Y_pred = model.predict(test_dataset)\n",
    "\n",
    "print(Y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[374   3]\n",
      " [  8 305]]\n"
     ]
    }
   ],
   "source": [
    "conf_mat = confusion_matrix(test_dataset.classes, Y_pred)\n",
    "print(conf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion = sb.heatmap(conf_mat, annot=True, cmap='Blues')\n",
    "confusion.set_title(\"Fetal Health Classification: Decision Tree\")\n",
    "confusion.set_xlabel(\"\\nPredicted Values\")\n",
    "confusion.set_ylabel(\"Actual Values\")\n",
    "confusion.xaxis.set_ticklabels(['Brain Tumor', 'Healthy'])\n",
    "confusion.yaxis.set_ticklabels(['Brain Tumor', 'Healthy'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
