{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa17add0",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65b545",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f236ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "329c6876",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19e07b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5874a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       2.116693                1.364192   \n",
      "1                       1.781763                1.577277   \n",
      "2                       2.423065                1.424487   \n",
      "3                       4.657487                1.366376   \n",
      "4                       3.795704                1.701514   \n",
      "...                          ...                     ...   \n",
      "4221                    3.332053                2.220377   \n",
      "4222                    4.258130               -2.535943   \n",
      "4223                    7.826457                2.169780   \n",
      "4224                   -0.702316                2.439427   \n",
      "4225                   -2.373678               -3.566134   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_lateralorbitofrontal_thickness  SurfaceHoles  \\\n",
      "0                   1.509748                          -2.002599     -1.880989   \n",
      "1                   1.751934                          -1.118212     -1.487208   \n",
      "2                   1.583753                          -1.542553     -1.247020   \n",
      "3                   1.174610                          -0.637378     -1.459954   \n",
      "4                   2.226974                          -1.242180     -1.389867   \n",
      "...                      ...                                ...           ...   \n",
      "4221                0.517929                           1.503938      0.767466   \n",
      "4222                1.742708                          -2.389973      1.916514   \n",
      "4223                3.996082                          -1.862587      1.556203   \n",
      "4224                7.148798                           0.140983      2.688554   \n",
      "4225                2.435484                          -2.369545      1.958245   \n",
      "\n",
      "      CC_Posterior  rh_entorhinal_thickness  CC_Posterior  Right-Caudate  \\\n",
      "0         2.280240                -1.649017     -0.013002      -0.463273   \n",
      "1         2.081013                -1.815469     -0.373819      -0.921829   \n",
      "2         1.777330                -2.457862     -0.636127      -1.267629   \n",
      "3         2.261775                -1.243540     -0.939366      -1.091465   \n",
      "4         2.826780                -1.685151     -0.289005      -0.690919   \n",
      "...            ...                      ...           ...            ...   \n",
      "4221     -0.078864                 0.285237     -2.282439      -0.606196   \n",
      "4222      0.217294                 0.945350     -2.431716       0.590846   \n",
      "4223      2.804735                 1.447188      0.570148       1.838543   \n",
      "4224      5.136362                 1.891687      0.900260       4.575784   \n",
      "4225      2.429440                 1.593801     -0.549375       2.195518   \n",
      "\n",
      "      MaskVol-to-eTIV  rh_frontalpole_thickness  MaskVol-to-eTIV  \\\n",
      "0            1.806703                 -0.687286         0.117551   \n",
      "1            1.988084                 -0.633688         0.555333   \n",
      "2            1.766556                 -0.745162         0.523402   \n",
      "3            1.569311                 -0.561350         0.049202   \n",
      "4            1.806288                 -0.863777         0.423945   \n",
      "...               ...                       ...              ...   \n",
      "4221        -0.867602                  0.550215        -0.503496   \n",
      "4222        -0.278129                  0.730407        -1.502642   \n",
      "4223        -0.100049                  3.701022        -0.546472   \n",
      "4224         1.187504                  0.736916         1.809611   \n",
      "4225         0.182275                 -0.215507        -0.988619   \n",
      "\n",
      "      Right-Cerebellum-White-Matter  MaskVol-to-eTIV  Right-vessel  \\\n",
      "0                         -1.133781         1.583529     -1.050681   \n",
      "1                         -0.921768         1.505786     -1.468032   \n",
      "2                         -0.491618         1.793263     -1.035388   \n",
      "3                         -0.504380         1.885197     -1.431387   \n",
      "4                         -1.061839         2.109071     -0.636920   \n",
      "...                             ...              ...           ...   \n",
      "4221                       0.938337        -1.214341      2.116130   \n",
      "4222                      -0.382447        -1.472173      1.128646   \n",
      "4223                      -0.908998        -2.387716      0.815211   \n",
      "4224                      -0.150258        -2.661770      1.033795   \n",
      "4225                      -0.858342        -1.603765      0.253080   \n",
      "\n",
      "      non-WM-hypointensities  lh_caudalanteriorcingulate_thickness  \\\n",
      "0                  -0.350294                              0.050377   \n",
      "1                  -0.656735                             -0.563480   \n",
      "2                  -0.753502                             -0.326453   \n",
      "3                  -0.058165                             -0.323348   \n",
      "4                  -0.555999                             -0.498166   \n",
      "...                      ...                                   ...   \n",
      "4221               -0.296830                              0.293508   \n",
      "4222               -1.211392                              0.280078   \n",
      "4223               -0.604093                              1.441280   \n",
      "4224               -1.666462                              1.241977   \n",
      "4225               -1.446136                              2.164543   \n",
      "\n",
      "      5th-Ventricle  non-WM-hypointensities  non-WM-hypointensities  \n",
      "0          0.654754               -0.249875                0.314114  \n",
      "1          0.387211               -0.164285                0.567488  \n",
      "2         -0.013440                0.064524                0.011248  \n",
      "3          0.436006               -0.034263                0.049757  \n",
      "4          0.889112               -0.223978                0.735370  \n",
      "...             ...                     ...                     ...  \n",
      "4221      -1.616314               -0.573543                0.725155  \n",
      "4222       0.172139               -0.456002                2.524588  \n",
      "4223       0.541871                0.841686                0.873512  \n",
      "4224      -0.237540                1.064036                1.041861  \n",
      "4225       0.578607                1.179590                0.583684  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_lateralorbitofrontal_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_entorhinal_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-Cerebellum-White-Matter</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>lh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.116693</td>\n",
       "      <td>1.364192</td>\n",
       "      <td>1.509748</td>\n",
       "      <td>-2.002599</td>\n",
       "      <td>-1.880989</td>\n",
       "      <td>2.280240</td>\n",
       "      <td>-1.649017</td>\n",
       "      <td>-0.013002</td>\n",
       "      <td>-0.463273</td>\n",
       "      <td>1.806703</td>\n",
       "      <td>-0.687286</td>\n",
       "      <td>0.117551</td>\n",
       "      <td>-1.133781</td>\n",
       "      <td>1.583529</td>\n",
       "      <td>-1.050681</td>\n",
       "      <td>-0.350294</td>\n",
       "      <td>0.050377</td>\n",
       "      <td>0.654754</td>\n",
       "      <td>-0.249875</td>\n",
       "      <td>0.314114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.781763</td>\n",
       "      <td>1.577277</td>\n",
       "      <td>1.751934</td>\n",
       "      <td>-1.118212</td>\n",
       "      <td>-1.487208</td>\n",
       "      <td>2.081013</td>\n",
       "      <td>-1.815469</td>\n",
       "      <td>-0.373819</td>\n",
       "      <td>-0.921829</td>\n",
       "      <td>1.988084</td>\n",
       "      <td>-0.633688</td>\n",
       "      <td>0.555333</td>\n",
       "      <td>-0.921768</td>\n",
       "      <td>1.505786</td>\n",
       "      <td>-1.468032</td>\n",
       "      <td>-0.656735</td>\n",
       "      <td>-0.563480</td>\n",
       "      <td>0.387211</td>\n",
       "      <td>-0.164285</td>\n",
       "      <td>0.567488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.423065</td>\n",
       "      <td>1.424487</td>\n",
       "      <td>1.583753</td>\n",
       "      <td>-1.542553</td>\n",
       "      <td>-1.247020</td>\n",
       "      <td>1.777330</td>\n",
       "      <td>-2.457862</td>\n",
       "      <td>-0.636127</td>\n",
       "      <td>-1.267629</td>\n",
       "      <td>1.766556</td>\n",
       "      <td>-0.745162</td>\n",
       "      <td>0.523402</td>\n",
       "      <td>-0.491618</td>\n",
       "      <td>1.793263</td>\n",
       "      <td>-1.035388</td>\n",
       "      <td>-0.753502</td>\n",
       "      <td>-0.326453</td>\n",
       "      <td>-0.013440</td>\n",
       "      <td>0.064524</td>\n",
       "      <td>0.011248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.657487</td>\n",
       "      <td>1.366376</td>\n",
       "      <td>1.174610</td>\n",
       "      <td>-0.637378</td>\n",
       "      <td>-1.459954</td>\n",
       "      <td>2.261775</td>\n",
       "      <td>-1.243540</td>\n",
       "      <td>-0.939366</td>\n",
       "      <td>-1.091465</td>\n",
       "      <td>1.569311</td>\n",
       "      <td>-0.561350</td>\n",
       "      <td>0.049202</td>\n",
       "      <td>-0.504380</td>\n",
       "      <td>1.885197</td>\n",
       "      <td>-1.431387</td>\n",
       "      <td>-0.058165</td>\n",
       "      <td>-0.323348</td>\n",
       "      <td>0.436006</td>\n",
       "      <td>-0.034263</td>\n",
       "      <td>0.049757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.795704</td>\n",
       "      <td>1.701514</td>\n",
       "      <td>2.226974</td>\n",
       "      <td>-1.242180</td>\n",
       "      <td>-1.389867</td>\n",
       "      <td>2.826780</td>\n",
       "      <td>-1.685151</td>\n",
       "      <td>-0.289005</td>\n",
       "      <td>-0.690919</td>\n",
       "      <td>1.806288</td>\n",
       "      <td>-0.863777</td>\n",
       "      <td>0.423945</td>\n",
       "      <td>-1.061839</td>\n",
       "      <td>2.109071</td>\n",
       "      <td>-0.636920</td>\n",
       "      <td>-0.555999</td>\n",
       "      <td>-0.498166</td>\n",
       "      <td>0.889112</td>\n",
       "      <td>-0.223978</td>\n",
       "      <td>0.735370</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    2.116693                1.364192                1.509748   \n",
       "1                    1.781763                1.577277                1.751934   \n",
       "2                    2.423065                1.424487                1.583753   \n",
       "3                    4.657487                1.366376                1.174610   \n",
       "4                    3.795704                1.701514                2.226974   \n",
       "\n",
       "   lh_lateralorbitofrontal_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                          -2.002599     -1.880989      2.280240   \n",
       "1                          -1.118212     -1.487208      2.081013   \n",
       "2                          -1.542553     -1.247020      1.777330   \n",
       "3                          -0.637378     -1.459954      2.261775   \n",
       "4                          -1.242180     -1.389867      2.826780   \n",
       "\n",
       "   rh_entorhinal_thickness  CC_Posterior  Right-Caudate  MaskVol-to-eTIV  \\\n",
       "0                -1.649017     -0.013002      -0.463273         1.806703   \n",
       "1                -1.815469     -0.373819      -0.921829         1.988084   \n",
       "2                -2.457862     -0.636127      -1.267629         1.766556   \n",
       "3                -1.243540     -0.939366      -1.091465         1.569311   \n",
       "4                -1.685151     -0.289005      -0.690919         1.806288   \n",
       "\n",
       "   rh_frontalpole_thickness  MaskVol-to-eTIV  Right-Cerebellum-White-Matter  \\\n",
       "0                 -0.687286         0.117551                      -1.133781   \n",
       "1                 -0.633688         0.555333                      -0.921768   \n",
       "2                 -0.745162         0.523402                      -0.491618   \n",
       "3                 -0.561350         0.049202                      -0.504380   \n",
       "4                 -0.863777         0.423945                      -1.061839   \n",
       "\n",
       "   MaskVol-to-eTIV  Right-vessel  non-WM-hypointensities  \\\n",
       "0         1.583529     -1.050681               -0.350294   \n",
       "1         1.505786     -1.468032               -0.656735   \n",
       "2         1.793263     -1.035388               -0.753502   \n",
       "3         1.885197     -1.431387               -0.058165   \n",
       "4         2.109071     -0.636920               -0.555999   \n",
       "\n",
       "   lh_caudalanteriorcingulate_thickness  5th-Ventricle  \\\n",
       "0                              0.050377       0.654754   \n",
       "1                             -0.563480       0.387211   \n",
       "2                             -0.326453      -0.013440   \n",
       "3                             -0.323348       0.436006   \n",
       "4                             -0.498166       0.889112   \n",
       "\n",
       "   non-WM-hypointensities  non-WM-hypointensities  \n",
       "0               -0.249875                0.314114  \n",
       "1               -0.164285                0.567488  \n",
       "2                0.064524                0.011248  \n",
       "3               -0.034263                0.049757  \n",
       "4               -0.223978                0.735370  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f677fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d236454d",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abed8c6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 32)                672       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 16)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 4)                 36        \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 4)                 0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,377\n",
      "Trainable params: 1,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(16))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1416a",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d01c439b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "53/53 [==============================] - 1s 9ms/step - loss: 10.6953 - msle: 10.6953 - val_loss: 6.5334 - val_msle: 6.5334\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 4.5292 - msle: 4.5292 - val_loss: 2.6496 - val_msle: 2.6496\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 1.8335 - msle: 1.8335 - val_loss: 1.0753 - val_msle: 1.0753\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.8002 - msle: 0.8002 - val_loss: 0.5077 - val_msle: 0.5077\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.4170 - msle: 0.4170 - val_loss: 0.2983 - val_msle: 0.2983\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.2711 - msle: 0.2711 - val_loss: 0.2202 - val_msle: 0.2202\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.2126 - msle: 0.2126 - val_loss: 0.1888 - val_msle: 0.1888\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1857 - msle: 0.1857 - val_loss: 0.1730 - val_msle: 0.1730\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1700 - msle: 0.1700 - val_loss: 0.1628 - val_msle: 0.1628\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1591 - msle: 0.1591 - val_loss: 0.1545 - val_msle: 0.1545\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1501 - msle: 0.1501 - val_loss: 0.1476 - val_msle: 0.1476\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1424 - msle: 0.1424 - val_loss: 0.1416 - val_msle: 0.1416\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1357 - msle: 0.1357 - val_loss: 0.1361 - val_msle: 0.1361\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1296 - msle: 0.1296 - val_loss: 0.1309 - val_msle: 0.1309\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.1240 - msle: 0.1240 - val_loss: 0.1262 - val_msle: 0.1262\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1189 - msle: 0.1189 - val_loss: 0.1219 - val_msle: 0.1219\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1140 - msle: 0.1140 - val_loss: 0.1177 - val_msle: 0.1177\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.1095 - msle: 0.1095 - val_loss: 0.1139 - val_msle: 0.1139\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1053 - msle: 0.1053 - val_loss: 0.1103 - val_msle: 0.1103\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.1013 - msle: 0.1013 - val_loss: 0.1067 - val_msle: 0.1067\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0976 - msle: 0.0976 - val_loss: 0.1034 - val_msle: 0.1034\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0941 - msle: 0.0941 - val_loss: 0.1000 - val_msle: 0.1000\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0908 - msle: 0.0908 - val_loss: 0.0971 - val_msle: 0.0971\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0876 - msle: 0.0876 - val_loss: 0.0944 - val_msle: 0.0944\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0847 - msle: 0.0847 - val_loss: 0.0914 - val_msle: 0.0914\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0820 - msle: 0.0820 - val_loss: 0.0890 - val_msle: 0.0890\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0793 - msle: 0.0793 - val_loss: 0.0864 - val_msle: 0.0864\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0769 - msle: 0.0769 - val_loss: 0.0839 - val_msle: 0.0839\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0745 - msle: 0.0745 - val_loss: 0.0817 - val_msle: 0.0817\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0723 - msle: 0.0723 - val_loss: 0.0795 - val_msle: 0.0795\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0703 - msle: 0.0703 - val_loss: 0.0775 - val_msle: 0.0775\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0683 - msle: 0.0683 - val_loss: 0.0755 - val_msle: 0.0755\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0665 - msle: 0.0665 - val_loss: 0.0738 - val_msle: 0.0738\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0648 - msle: 0.0648 - val_loss: 0.0720 - val_msle: 0.0720\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0632 - msle: 0.0632 - val_loss: 0.0704 - val_msle: 0.0704\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0616 - msle: 0.0616 - val_loss: 0.0689 - val_msle: 0.0689\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0601 - msle: 0.0601 - val_loss: 0.0675 - val_msle: 0.0675\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0588 - msle: 0.0588 - val_loss: 0.0661 - val_msle: 0.0661\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0575 - msle: 0.0575 - val_loss: 0.0648 - val_msle: 0.0648\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0562 - msle: 0.0562 - val_loss: 0.0636 - val_msle: 0.0636\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0551 - msle: 0.0551 - val_loss: 0.0625 - val_msle: 0.0625\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0541 - msle: 0.0541 - val_loss: 0.0616 - val_msle: 0.0616\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0529 - msle: 0.0529 - val_loss: 0.0605 - val_msle: 0.0605\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0520 - msle: 0.0520 - val_loss: 0.0596 - val_msle: 0.0596\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0510 - msle: 0.0510 - val_loss: 0.0586 - val_msle: 0.0586\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0500 - msle: 0.0500 - val_loss: 0.0577 - val_msle: 0.0577\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0491 - msle: 0.0491 - val_loss: 0.0570 - val_msle: 0.0570\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0483 - msle: 0.0483 - val_loss: 0.0563 - val_msle: 0.0563\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0475 - msle: 0.0475 - val_loss: 0.0556 - val_msle: 0.0556\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0466 - msle: 0.0466 - val_loss: 0.0551 - val_msle: 0.0551\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0458 - msle: 0.0458 - val_loss: 0.0543 - val_msle: 0.0543\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0451 - msle: 0.0451 - val_loss: 0.0537 - val_msle: 0.0537\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0444 - msle: 0.0444 - val_loss: 0.0531 - val_msle: 0.0531\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0437 - msle: 0.0437 - val_loss: 0.0526 - val_msle: 0.0526\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0430 - msle: 0.0430 - val_loss: 0.0520 - val_msle: 0.0520\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0423 - msle: 0.0423 - val_loss: 0.0513 - val_msle: 0.0513\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0417 - msle: 0.0417 - val_loss: 0.0509 - val_msle: 0.0509\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0411 - msle: 0.0411 - val_loss: 0.0502 - val_msle: 0.0502\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0404 - msle: 0.0404 - val_loss: 0.0499 - val_msle: 0.0499\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0398 - msle: 0.0398 - val_loss: 0.0493 - val_msle: 0.0493\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0392 - msle: 0.0392 - val_loss: 0.0488 - val_msle: 0.0488\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0386 - msle: 0.0386 - val_loss: 0.0483 - val_msle: 0.0483\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0380 - msle: 0.0380 - val_loss: 0.0477 - val_msle: 0.0477\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0374 - msle: 0.0374 - val_loss: 0.0471 - val_msle: 0.0471\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0368 - msle: 0.0368 - val_loss: 0.0468 - val_msle: 0.0468\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0363 - msle: 0.0363 - val_loss: 0.0462 - val_msle: 0.0462\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0357 - msle: 0.0357 - val_loss: 0.0457 - val_msle: 0.0457\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0351 - msle: 0.0351 - val_loss: 0.0452 - val_msle: 0.0452\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0345 - msle: 0.0345 - val_loss: 0.0447 - val_msle: 0.0447\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0339 - msle: 0.0339 - val_loss: 0.0442 - val_msle: 0.0442\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0335 - msle: 0.0335 - val_loss: 0.0436 - val_msle: 0.0436\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0329 - msle: 0.0329 - val_loss: 0.0431 - val_msle: 0.0431\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0323 - msle: 0.0323 - val_loss: 0.0425 - val_msle: 0.0425\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0319 - msle: 0.0319 - val_loss: 0.0422 - val_msle: 0.0422\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0313 - msle: 0.0313 - val_loss: 0.0415 - val_msle: 0.0415\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0309 - msle: 0.0309 - val_loss: 0.0409 - val_msle: 0.0409\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0303 - msle: 0.0303 - val_loss: 0.0405 - val_msle: 0.0405\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0298 - msle: 0.0298 - val_loss: 0.0400 - val_msle: 0.0400\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 0s 9ms/step - loss: 0.0293 - msle: 0.0293 - val_loss: 0.0395 - val_msle: 0.0395\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0289 - msle: 0.0289 - val_loss: 0.0390 - val_msle: 0.0390\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0283 - msle: 0.0283 - val_loss: 0.0385 - val_msle: 0.0385\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0279 - msle: 0.0279 - val_loss: 0.0382 - val_msle: 0.0382\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0274 - msle: 0.0274 - val_loss: 0.0377 - val_msle: 0.0377\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0270 - msle: 0.0270 - val_loss: 0.0370 - val_msle: 0.0370\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0265 - msle: 0.0265 - val_loss: 0.0368 - val_msle: 0.0368\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0261 - msle: 0.0261 - val_loss: 0.0361 - val_msle: 0.0361\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0258 - msle: 0.0258 - val_loss: 0.0358 - val_msle: 0.0358\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0253 - msle: 0.0253 - val_loss: 0.0357 - val_msle: 0.0357\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0250 - msle: 0.0250 - val_loss: 0.0350 - val_msle: 0.0350\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0245 - msle: 0.0245 - val_loss: 0.0347 - val_msle: 0.0347\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0242 - msle: 0.0242 - val_loss: 0.0343 - val_msle: 0.0343\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 0s 7ms/step - loss: 0.0240 - msle: 0.0240 - val_loss: 0.0340 - val_msle: 0.0340\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0236 - msle: 0.0236 - val_loss: 0.0336 - val_msle: 0.0336\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0232 - msle: 0.0232 - val_loss: 0.0332 - val_msle: 0.0332\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0228 - msle: 0.0228 - val_loss: 0.0330 - val_msle: 0.0330\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0226 - msle: 0.0226 - val_loss: 0.0326 - val_msle: 0.0326\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0223 - msle: 0.0223 - val_loss: 0.0322 - val_msle: 0.0322\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0221 - msle: 0.0221 - val_loss: 0.0319 - val_msle: 0.0319\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 0s 6ms/step - loss: 0.0218 - msle: 0.0218 - val_loss: 0.0319 - val_msle: 0.0319\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 0s 5ms/step - loss: 0.0216 - msle: 0.0216 - val_loss: 0.0314 - val_msle: 0.0314\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15291ade",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14e02625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8541050029733335\n",
      "Max Error: 24.369579315185547\n",
      "Mean absolute error: 5.893319309882398\n",
      "Mean squared error: 58.17547062534669\n",
      "Root Mean squared error: 7.627284616778549\n",
      "R2: 0.8516267727641924\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3179223e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(100))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='train_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d03c5f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
