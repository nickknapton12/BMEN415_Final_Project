{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aae7e772",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. Introduction\n",
    "2. Import\n",
    "3. Analysis & Preprocessing\n",
    "4. Model\n",
    "5. Training\n",
    "6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d7b25a",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "References:\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46a461e",
   "metadata": {},
   "source": [
    "# 2. Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4ef77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53e3cbe",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39a7da5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73af9e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       2.116693                1.364190   \n",
      "1                       1.781763                1.577275   \n",
      "2                       2.423065                1.424485   \n",
      "3                       4.657487                1.366376   \n",
      "4                       3.795704                1.701512   \n",
      "...                          ...                     ...   \n",
      "4221                    3.332053                2.220377   \n",
      "4222                    4.258130               -2.535943   \n",
      "4223                    7.826457                2.169782   \n",
      "4224                   -0.702316                2.439426   \n",
      "4225                   -2.373678               -3.566136   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_lateralorbitofrontal_thickness  SurfaceHoles  \\\n",
      "0                   1.509735                          -2.002578     -1.881492   \n",
      "1                   1.751892                          -1.118331     -1.487501   \n",
      "2                   1.583649                          -1.542496     -1.247366   \n",
      "3                   1.174538                          -0.636979     -1.460461   \n",
      "4                   2.226901                          -1.242589     -1.389941   \n",
      "...                      ...                                ...           ...   \n",
      "4221                0.517937                           1.504673      0.766550   \n",
      "4222                1.742697                          -2.390315      1.916712   \n",
      "4223                3.995932                          -1.862053      1.555016   \n",
      "4224                7.148833                           0.140203      2.689785   \n",
      "4225                2.435569                          -2.370131      1.958828   \n",
      "\n",
      "      CC_Posterior  rh_entorhinal_thickness  CC_Posterior  Right-Caudate  \\\n",
      "0         2.276572                -1.645030     -0.009227      -0.467878   \n",
      "1         2.076867                -1.810514     -0.369168      -0.924453   \n",
      "2         1.777017                -2.451846     -0.641076      -1.269868   \n",
      "3         2.257651                -1.233161     -0.938898      -1.098905   \n",
      "4         2.822799                -1.680302     -0.281152      -0.694993   \n",
      "...            ...                      ...           ...            ...   \n",
      "4221     -0.088997                 0.290471     -2.289088      -0.612352   \n",
      "4222      0.221553                 0.944025     -2.433969       0.597607   \n",
      "4223      2.800646                 1.462646      0.592141       1.851180   \n",
      "4224      5.133822                 1.887534      0.901335       4.573220   \n",
      "4225      2.427621                 1.592082     -0.536020       2.220690   \n",
      "\n",
      "      MaskVol-to-eTIV  rh_frontalpole_thickness  MaskVol-to-eTIV  \\\n",
      "0            1.780338                 -0.678076         0.104236   \n",
      "1            1.963588                 -0.608463         0.532925   \n",
      "2            1.752599                 -0.742285         0.507290   \n",
      "3            1.541935                 -0.560708        -0.000520   \n",
      "4            1.776571                 -0.838985         0.372740   \n",
      "...               ...                       ...              ...   \n",
      "4221        -0.869837                  0.542364        -0.541044   \n",
      "4222        -0.287829                  0.721196        -1.499183   \n",
      "4223        -0.130896                  3.684309        -0.664302   \n",
      "4224         1.182440                  0.825502         1.827236   \n",
      "4225         0.171571                 -0.212720        -1.032363   \n",
      "\n",
      "      Right-Cerebellum-White-Matter  MaskVol-to-eTIV  Right-vessel  \\\n",
      "0                         -1.136586         1.557921     -1.217632   \n",
      "1                         -0.925209         1.449863     -1.664904   \n",
      "2                         -0.505602         1.736077     -1.242800   \n",
      "3                         -0.535672         1.803339     -1.700276   \n",
      "4                         -1.032947         2.091808     -0.938495   \n",
      "...                             ...              ...           ...   \n",
      "4221                       0.812408        -1.186446      2.231076   \n",
      "4222                      -0.333542        -1.390265      1.228593   \n",
      "4223                      -0.906199        -2.319060      0.894743   \n",
      "4224                      -0.142873        -2.601252      1.149711   \n",
      "4225                      -0.831135        -1.560786      0.263226   \n",
      "\n",
      "      non-WM-hypointensities  rh_isthmuscingulate_thickness  5th-Ventricle  \\\n",
      "0                  -0.361074                       0.093541      -0.607278   \n",
      "1                  -0.732255                      -0.454900      -0.178368   \n",
      "2                  -0.778175                      -0.224387       0.163232   \n",
      "3                  -0.218092                      -0.398867      -0.112105   \n",
      "4                  -0.633126                      -0.571152      -0.583998   \n",
      "...                      ...                            ...            ...   \n",
      "4221               -0.370543                       0.593842       1.450524   \n",
      "4222               -0.988906                       0.627392      -0.494235   \n",
      "4223               -0.407672                       1.631083      -0.486716   \n",
      "4224               -1.589036                       1.466316       0.266096   \n",
      "4225               -1.038218                       2.320703      -0.422253   \n",
      "\n",
      "      Right-vessel  non-WM-hypointensities  \n",
      "0        -0.254079                0.364582  \n",
      "1        -0.309817                0.624537  \n",
      "2        -0.041908                0.119473  \n",
      "3        -0.065013                0.098741  \n",
      "4        -0.215627                0.704340  \n",
      "...            ...                     ...  \n",
      "4221     -0.995905                0.856639  \n",
      "4222     -0.728665                2.379413  \n",
      "4223      0.537620                0.919199  \n",
      "4224      1.283877                0.957714  \n",
      "4225      1.040264                0.669358  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_lateralorbitofrontal_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_entorhinal_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-Cerebellum-White-Matter</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.116693</td>\n",
       "      <td>1.364190</td>\n",
       "      <td>1.509735</td>\n",
       "      <td>-2.002578</td>\n",
       "      <td>-1.881492</td>\n",
       "      <td>2.276572</td>\n",
       "      <td>-1.645030</td>\n",
       "      <td>-0.009227</td>\n",
       "      <td>-0.467878</td>\n",
       "      <td>1.780338</td>\n",
       "      <td>-0.678076</td>\n",
       "      <td>0.104236</td>\n",
       "      <td>-1.136586</td>\n",
       "      <td>1.557921</td>\n",
       "      <td>-1.217632</td>\n",
       "      <td>-0.361074</td>\n",
       "      <td>0.093541</td>\n",
       "      <td>-0.607278</td>\n",
       "      <td>-0.254079</td>\n",
       "      <td>0.364582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.781763</td>\n",
       "      <td>1.577275</td>\n",
       "      <td>1.751892</td>\n",
       "      <td>-1.118331</td>\n",
       "      <td>-1.487501</td>\n",
       "      <td>2.076867</td>\n",
       "      <td>-1.810514</td>\n",
       "      <td>-0.369168</td>\n",
       "      <td>-0.924453</td>\n",
       "      <td>1.963588</td>\n",
       "      <td>-0.608463</td>\n",
       "      <td>0.532925</td>\n",
       "      <td>-0.925209</td>\n",
       "      <td>1.449863</td>\n",
       "      <td>-1.664904</td>\n",
       "      <td>-0.732255</td>\n",
       "      <td>-0.454900</td>\n",
       "      <td>-0.178368</td>\n",
       "      <td>-0.309817</td>\n",
       "      <td>0.624537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.423065</td>\n",
       "      <td>1.424485</td>\n",
       "      <td>1.583649</td>\n",
       "      <td>-1.542496</td>\n",
       "      <td>-1.247366</td>\n",
       "      <td>1.777017</td>\n",
       "      <td>-2.451846</td>\n",
       "      <td>-0.641076</td>\n",
       "      <td>-1.269868</td>\n",
       "      <td>1.752599</td>\n",
       "      <td>-0.742285</td>\n",
       "      <td>0.507290</td>\n",
       "      <td>-0.505602</td>\n",
       "      <td>1.736077</td>\n",
       "      <td>-1.242800</td>\n",
       "      <td>-0.778175</td>\n",
       "      <td>-0.224387</td>\n",
       "      <td>0.163232</td>\n",
       "      <td>-0.041908</td>\n",
       "      <td>0.119473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.657487</td>\n",
       "      <td>1.366376</td>\n",
       "      <td>1.174538</td>\n",
       "      <td>-0.636979</td>\n",
       "      <td>-1.460461</td>\n",
       "      <td>2.257651</td>\n",
       "      <td>-1.233161</td>\n",
       "      <td>-0.938898</td>\n",
       "      <td>-1.098905</td>\n",
       "      <td>1.541935</td>\n",
       "      <td>-0.560708</td>\n",
       "      <td>-0.000520</td>\n",
       "      <td>-0.535672</td>\n",
       "      <td>1.803339</td>\n",
       "      <td>-1.700276</td>\n",
       "      <td>-0.218092</td>\n",
       "      <td>-0.398867</td>\n",
       "      <td>-0.112105</td>\n",
       "      <td>-0.065013</td>\n",
       "      <td>0.098741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.795704</td>\n",
       "      <td>1.701512</td>\n",
       "      <td>2.226901</td>\n",
       "      <td>-1.242589</td>\n",
       "      <td>-1.389941</td>\n",
       "      <td>2.822799</td>\n",
       "      <td>-1.680302</td>\n",
       "      <td>-0.281152</td>\n",
       "      <td>-0.694993</td>\n",
       "      <td>1.776571</td>\n",
       "      <td>-0.838985</td>\n",
       "      <td>0.372740</td>\n",
       "      <td>-1.032947</td>\n",
       "      <td>2.091808</td>\n",
       "      <td>-0.938495</td>\n",
       "      <td>-0.633126</td>\n",
       "      <td>-0.571152</td>\n",
       "      <td>-0.583998</td>\n",
       "      <td>-0.215627</td>\n",
       "      <td>0.704340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    2.116693                1.364190                1.509735   \n",
       "1                    1.781763                1.577275                1.751892   \n",
       "2                    2.423065                1.424485                1.583649   \n",
       "3                    4.657487                1.366376                1.174538   \n",
       "4                    3.795704                1.701512                2.226901   \n",
       "\n",
       "   lh_lateralorbitofrontal_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                          -2.002578     -1.881492      2.276572   \n",
       "1                          -1.118331     -1.487501      2.076867   \n",
       "2                          -1.542496     -1.247366      1.777017   \n",
       "3                          -0.636979     -1.460461      2.257651   \n",
       "4                          -1.242589     -1.389941      2.822799   \n",
       "\n",
       "   rh_entorhinal_thickness  CC_Posterior  Right-Caudate  MaskVol-to-eTIV  \\\n",
       "0                -1.645030     -0.009227      -0.467878         1.780338   \n",
       "1                -1.810514     -0.369168      -0.924453         1.963588   \n",
       "2                -2.451846     -0.641076      -1.269868         1.752599   \n",
       "3                -1.233161     -0.938898      -1.098905         1.541935   \n",
       "4                -1.680302     -0.281152      -0.694993         1.776571   \n",
       "\n",
       "   rh_frontalpole_thickness  MaskVol-to-eTIV  Right-Cerebellum-White-Matter  \\\n",
       "0                 -0.678076         0.104236                      -1.136586   \n",
       "1                 -0.608463         0.532925                      -0.925209   \n",
       "2                 -0.742285         0.507290                      -0.505602   \n",
       "3                 -0.560708        -0.000520                      -0.535672   \n",
       "4                 -0.838985         0.372740                      -1.032947   \n",
       "\n",
       "   MaskVol-to-eTIV  Right-vessel  non-WM-hypointensities  \\\n",
       "0         1.557921     -1.217632               -0.361074   \n",
       "1         1.449863     -1.664904               -0.732255   \n",
       "2         1.736077     -1.242800               -0.778175   \n",
       "3         1.803339     -1.700276               -0.218092   \n",
       "4         2.091808     -0.938495               -0.633126   \n",
       "\n",
       "   rh_isthmuscingulate_thickness  5th-Ventricle  Right-vessel  \\\n",
       "0                       0.093541      -0.607278     -0.254079   \n",
       "1                      -0.454900      -0.178368     -0.309817   \n",
       "2                      -0.224387       0.163232     -0.041908   \n",
       "3                      -0.398867      -0.112105     -0.065013   \n",
       "4                      -0.571152      -0.583998     -0.215627   \n",
       "\n",
       "   non-WM-hypointensities  \n",
       "0                0.364582  \n",
       "1                0.624537  \n",
       "2                0.119473  \n",
       "3                0.098741  \n",
       "4                0.704340  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e640af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9f804e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_10 (Dense)            (None, 32)                672       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 4)                 0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,377\n",
      "Trainable params: 1,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(16))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e997d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e47f3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 15:53:23.010687: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - ETA: 0s - loss: 11.2678 - msle: 11.2678"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 15:53:25.789472: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 4s 43ms/step - loss: 11.2678 - msle: 11.2678 - val_loss: 7.3187 - val_msle: 7.3187\n",
      "Epoch 2/100\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 5.0874 - msle: 5.0874 - val_loss: 3.0893 - val_msle: 3.0893\n",
      "Epoch 3/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 2.1620 - msle: 2.1620 - val_loss: 1.3176 - val_msle: 1.3176\n",
      "Epoch 4/100\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.9247 - msle: 0.9247 - val_loss: 0.5591 - val_msle: 0.5591\n",
      "Epoch 5/100\n",
      "53/53 [==============================] - 3s 50ms/step - loss: 0.4319 - msle: 0.4319 - val_loss: 0.3007 - val_msle: 0.3007\n",
      "Epoch 6/100\n",
      "53/53 [==============================] - 2s 42ms/step - loss: 0.2587 - msle: 0.2587 - val_loss: 0.2069 - val_msle: 0.2069\n",
      "Epoch 7/100\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.1901 - msle: 0.1901 - val_loss: 0.1682 - val_msle: 0.1682\n",
      "Epoch 8/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.1589 - msle: 0.1589 - val_loss: 0.1488 - val_msle: 0.1488\n",
      "Epoch 9/100\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.1416 - msle: 0.1416 - val_loss: 0.1367 - val_msle: 0.1367\n",
      "Epoch 10/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.1297 - msle: 0.1297 - val_loss: 0.1275 - val_msle: 0.1275\n",
      "Epoch 11/100\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 0.1206 - msle: 0.1206 - val_loss: 0.1199 - val_msle: 0.1199\n",
      "Epoch 12/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.1131 - msle: 0.1131 - val_loss: 0.1137 - val_msle: 0.1137\n",
      "Epoch 13/100\n",
      "53/53 [==============================] - 2s 33ms/step - loss: 0.1067 - msle: 0.1067 - val_loss: 0.1081 - val_msle: 0.1081\n",
      "Epoch 14/100\n",
      "53/53 [==============================] - 2s 40ms/step - loss: 0.1012 - msle: 0.1012 - val_loss: 0.1034 - val_msle: 0.1034\n",
      "Epoch 15/100\n",
      "53/53 [==============================] - 2s 39ms/step - loss: 0.0962 - msle: 0.0962 - val_loss: 0.0989 - val_msle: 0.0989\n",
      "Epoch 16/100\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.0917 - msle: 0.0917 - val_loss: 0.0951 - val_msle: 0.0951\n",
      "Epoch 17/100\n",
      "53/53 [==============================] - 2s 37ms/step - loss: 0.0877 - msle: 0.0877 - val_loss: 0.0915 - val_msle: 0.0915\n",
      "Epoch 18/100\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.0840 - msle: 0.0840 - val_loss: 0.0885 - val_msle: 0.0885\n",
      "Epoch 19/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.0807 - msle: 0.0807 - val_loss: 0.0856 - val_msle: 0.0856\n",
      "Epoch 20/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0775 - msle: 0.0775 - val_loss: 0.0828 - val_msle: 0.0828\n",
      "Epoch 21/100\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.0748 - msle: 0.0748 - val_loss: 0.0804 - val_msle: 0.0804\n",
      "Epoch 22/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0723 - msle: 0.0723 - val_loss: 0.0781 - val_msle: 0.0781\n",
      "Epoch 23/100\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.0699 - msle: 0.0699 - val_loss: 0.0759 - val_msle: 0.0759\n",
      "Epoch 24/100\n",
      "53/53 [==============================] - 2s 41ms/step - loss: 0.0676 - msle: 0.0676 - val_loss: 0.0740 - val_msle: 0.0740\n",
      "Epoch 25/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0656 - msle: 0.0656 - val_loss: 0.0724 - val_msle: 0.0724\n",
      "Epoch 26/100\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.0637 - msle: 0.0637 - val_loss: 0.0707 - val_msle: 0.0707\n",
      "Epoch 27/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.0619 - msle: 0.0619 - val_loss: 0.0689 - val_msle: 0.0689\n",
      "Epoch 28/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0602 - msle: 0.0602 - val_loss: 0.0675 - val_msle: 0.0675\n",
      "Epoch 29/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0587 - msle: 0.0587 - val_loss: 0.0659 - val_msle: 0.0659\n",
      "Epoch 30/100\n",
      "53/53 [==============================] - 2s 31ms/step - loss: 0.0572 - msle: 0.0572 - val_loss: 0.0646 - val_msle: 0.0646\n",
      "Epoch 31/100\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.0559 - msle: 0.0559 - val_loss: 0.0634 - val_msle: 0.0634\n",
      "Epoch 32/100\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.0545 - msle: 0.0545 - val_loss: 0.0619 - val_msle: 0.0619\n",
      "Epoch 33/100\n",
      "53/53 [==============================] - 3s 48ms/step - loss: 0.0532 - msle: 0.0532 - val_loss: 0.0607 - val_msle: 0.0607\n",
      "Epoch 34/100\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.0522 - msle: 0.0522 - val_loss: 0.0595 - val_msle: 0.0595\n",
      "Epoch 35/100\n",
      "53/53 [==============================] - 1s 24ms/step - loss: 0.0509 - msle: 0.0509 - val_loss: 0.0586 - val_msle: 0.0586\n",
      "Epoch 36/100\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.0499 - msle: 0.0499 - val_loss: 0.0575 - val_msle: 0.0575\n",
      "Epoch 37/100\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.0488 - msle: 0.0488 - val_loss: 0.0562 - val_msle: 0.0562\n",
      "Epoch 38/100\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.0478 - msle: 0.0478 - val_loss: 0.0553 - val_msle: 0.0553\n",
      "Epoch 39/100\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 0.0468 - msle: 0.0468 - val_loss: 0.0543 - val_msle: 0.0543\n",
      "Epoch 40/100\n",
      "53/53 [==============================] - 1s 24ms/step - loss: 0.0459 - msle: 0.0459 - val_loss: 0.0534 - val_msle: 0.0534\n",
      "Epoch 41/100\n",
      "53/53 [==============================] - 3s 57ms/step - loss: 0.0449 - msle: 0.0449 - val_loss: 0.0525 - val_msle: 0.0525\n",
      "Epoch 42/100\n",
      "53/53 [==============================] - 2s 44ms/step - loss: 0.0440 - msle: 0.0440 - val_loss: 0.0518 - val_msle: 0.0518\n",
      "Epoch 43/100\n",
      "53/53 [==============================] - 2s 46ms/step - loss: 0.0432 - msle: 0.0432 - val_loss: 0.0508 - val_msle: 0.0508\n",
      "Epoch 44/100\n",
      "53/53 [==============================] - 3s 47ms/step - loss: 0.0423 - msle: 0.0423 - val_loss: 0.0498 - val_msle: 0.0498\n",
      "Epoch 45/100\n",
      "53/53 [==============================] - 2s 34ms/step - loss: 0.0414 - msle: 0.0414 - val_loss: 0.0493 - val_msle: 0.0493\n",
      "Epoch 46/100\n",
      "53/53 [==============================] - 2s 37ms/step - loss: 0.0408 - msle: 0.0408 - val_loss: 0.0486 - val_msle: 0.0486\n",
      "Epoch 47/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0398 - msle: 0.0398 - val_loss: 0.0479 - val_msle: 0.0479\n",
      "Epoch 48/100\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.0392 - msle: 0.0392 - val_loss: 0.0470 - val_msle: 0.0470\n",
      "Epoch 49/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.0383 - msle: 0.0383 - val_loss: 0.0467 - val_msle: 0.0467\n",
      "Epoch 50/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0378 - msle: 0.0378 - val_loss: 0.0457 - val_msle: 0.0457\n",
      "Epoch 51/100\n",
      "53/53 [==============================] - 2s 28ms/step - loss: 0.0370 - msle: 0.0370 - val_loss: 0.0451 - val_msle: 0.0451\n",
      "Epoch 52/100\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.0365 - msle: 0.0365 - val_loss: 0.0444 - val_msle: 0.0444\n",
      "Epoch 53/100\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.0357 - msle: 0.0357 - val_loss: 0.0439 - val_msle: 0.0439\n",
      "Epoch 54/100\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.0350 - msle: 0.0350 - val_loss: 0.0434 - val_msle: 0.0434\n",
      "Epoch 55/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0344 - msle: 0.0344 - val_loss: 0.0426 - val_msle: 0.0426\n",
      "Epoch 56/100\n",
      "53/53 [==============================] - 2s 45ms/step - loss: 0.0340 - msle: 0.0340 - val_loss: 0.0420 - val_msle: 0.0420\n",
      "Epoch 57/100\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.0334 - msle: 0.0334 - val_loss: 0.0417 - val_msle: 0.0417\n",
      "Epoch 58/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0328 - msle: 0.0328 - val_loss: 0.0415 - val_msle: 0.0415\n",
      "Epoch 59/100\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.0324 - msle: 0.0324 - val_loss: 0.0406 - val_msle: 0.0406\n",
      "Epoch 60/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0318 - msle: 0.0318 - val_loss: 0.0400 - val_msle: 0.0400\n",
      "Epoch 61/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0315 - msle: 0.0315 - val_loss: 0.0396 - val_msle: 0.0396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62/100\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.0309 - msle: 0.0309 - val_loss: 0.0388 - val_msle: 0.0388\n",
      "Epoch 63/100\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.0304 - msle: 0.0304 - val_loss: 0.0391 - val_msle: 0.0391\n",
      "Epoch 64/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0299 - msle: 0.0299 - val_loss: 0.0380 - val_msle: 0.0380\n",
      "Epoch 65/100\n",
      "53/53 [==============================] - 2s 35ms/step - loss: 0.0294 - msle: 0.0294 - val_loss: 0.0380 - val_msle: 0.0380\n",
      "Epoch 66/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0289 - msle: 0.0289 - val_loss: 0.0373 - val_msle: 0.0373\n",
      "Epoch 67/100\n",
      "53/53 [==============================] - 1s 24ms/step - loss: 0.0286 - msle: 0.0286 - val_loss: 0.0368 - val_msle: 0.0368\n",
      "Epoch 68/100\n",
      "53/53 [==============================] - 1s 23ms/step - loss: 0.0282 - msle: 0.0282 - val_loss: 0.0366 - val_msle: 0.0366\n",
      "Epoch 69/100\n",
      "53/53 [==============================] - 1s 24ms/step - loss: 0.0278 - msle: 0.0278 - val_loss: 0.0361 - val_msle: 0.0361\n",
      "Epoch 70/100\n",
      "53/53 [==============================] - 1s 23ms/step - loss: 0.0275 - msle: 0.0275 - val_loss: 0.0358 - val_msle: 0.0358\n",
      "Epoch 71/100\n",
      "53/53 [==============================] - 1s 25ms/step - loss: 0.0270 - msle: 0.0270 - val_loss: 0.0354 - val_msle: 0.0354\n",
      "Epoch 72/100\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.0266 - msle: 0.0266 - val_loss: 0.0351 - val_msle: 0.0351\n",
      "Epoch 73/100\n",
      "53/53 [==============================] - 1s 23ms/step - loss: 0.0263 - msle: 0.0263 - val_loss: 0.0343 - val_msle: 0.0343\n",
      "Epoch 74/100\n",
      "53/53 [==============================] - 1s 26ms/step - loss: 0.0260 - msle: 0.0260 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 75/100\n",
      "53/53 [==============================] - 1s 28ms/step - loss: 0.0258 - msle: 0.0258 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 76/100\n",
      "53/53 [==============================] - 1s 27ms/step - loss: 0.0254 - msle: 0.0254 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 77/100\n",
      "53/53 [==============================] - 1s 23ms/step - loss: 0.0250 - msle: 0.0250 - val_loss: 0.0328 - val_msle: 0.0328\n",
      "Epoch 78/100\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.0247 - msle: 0.0247 - val_loss: 0.0335 - val_msle: 0.0335\n",
      "Epoch 79/100\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.0244 - msle: 0.0244 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 80/100\n",
      "53/53 [==============================] - 2s 30ms/step - loss: 0.0242 - msle: 0.0242 - val_loss: 0.0327 - val_msle: 0.0327\n",
      "Epoch 81/100\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.0239 - msle: 0.0239 - val_loss: 0.0319 - val_msle: 0.0319\n",
      "Epoch 82/100\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0236 - msle: 0.0236 - val_loss: 0.0315 - val_msle: 0.0315\n",
      "Epoch 83/100\n",
      "53/53 [==============================] - 1s 12ms/step - loss: 0.0235 - msle: 0.0235 - val_loss: 0.0317 - val_msle: 0.0317\n",
      "Epoch 84/100\n",
      "53/53 [==============================] - 1s 10ms/step - loss: 0.0232 - msle: 0.0232 - val_loss: 0.0318 - val_msle: 0.0318\n",
      "Epoch 85/100\n",
      "53/53 [==============================] - 1s 21ms/step - loss: 0.0230 - msle: 0.0230 - val_loss: 0.0312 - val_msle: 0.0312\n",
      "Epoch 86/100\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0227 - msle: 0.0227 - val_loss: 0.0307 - val_msle: 0.0307\n",
      "Epoch 87/100\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0226 - msle: 0.0226 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 88/100\n",
      "53/53 [==============================] - 2s 36ms/step - loss: 0.0224 - msle: 0.0224 - val_loss: 0.0307 - val_msle: 0.0307\n",
      "Epoch 89/100\n",
      "53/53 [==============================] - 1s 22ms/step - loss: 0.0222 - msle: 0.0222 - val_loss: 0.0305 - val_msle: 0.0305\n",
      "Epoch 90/100\n",
      "53/53 [==============================] - 1s 15ms/step - loss: 0.0220 - msle: 0.0220 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 91/100\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0217 - msle: 0.0217 - val_loss: 0.0302 - val_msle: 0.0302\n",
      "Epoch 92/100\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0216 - msle: 0.0216 - val_loss: 0.0299 - val_msle: 0.0299\n",
      "Epoch 93/100\n",
      "53/53 [==============================] - 1s 13ms/step - loss: 0.0214 - msle: 0.0214 - val_loss: 0.0298 - val_msle: 0.0298\n",
      "Epoch 94/100\n",
      "53/53 [==============================] - 1s 16ms/step - loss: 0.0211 - msle: 0.0211 - val_loss: 0.0297 - val_msle: 0.0297\n",
      "Epoch 95/100\n",
      "53/53 [==============================] - 1s 17ms/step - loss: 0.0210 - msle: 0.0210 - val_loss: 0.0295 - val_msle: 0.0295\n",
      "Epoch 96/100\n",
      "53/53 [==============================] - 1s 20ms/step - loss: 0.0208 - msle: 0.0208 - val_loss: 0.0295 - val_msle: 0.0295\n",
      "Epoch 97/100\n",
      "53/53 [==============================] - 2s 32ms/step - loss: 0.0206 - msle: 0.0206 - val_loss: 0.0293 - val_msle: 0.0293\n",
      "Epoch 98/100\n",
      "53/53 [==============================] - 1s 20ms/step - loss: 0.0205 - msle: 0.0205 - val_loss: 0.0287 - val_msle: 0.0287\n",
      "Epoch 99/100\n",
      "53/53 [==============================] - 2s 29ms/step - loss: 0.0204 - msle: 0.0204 - val_loss: 0.0296 - val_msle: 0.0296\n",
      "Epoch 100/100\n",
      "53/53 [==============================] - 2s 42ms/step - loss: 0.0203 - msle: 0.0203 - val_loss: 0.0291 - val_msle: 0.0291\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6762b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8473461210171332\n",
      "Max Error: 24.898300170898438\n",
      "Mean absolute error: 5.875638134074661\n",
      "Mean squared error: 60.81557534441183\n",
      "Root Mean squared error: 7.798434159779246\n",
      "R2: 0.8448933359187755\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c009e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(100))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='train_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05ae7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
