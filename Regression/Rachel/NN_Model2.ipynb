{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e707cbf8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f70621",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e79a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3ad42",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe01f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba71637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       1.754401                1.293658   \n",
      "1                       1.417516                1.506790   \n",
      "2                       2.060537                1.356490   \n",
      "3                       4.321472                1.316557   \n",
      "4                       3.432616                1.645476   \n",
      "...                          ...                     ...   \n",
      "4221                    3.508241                2.349695   \n",
      "4222                    4.445945               -2.409495   \n",
      "4223                    8.016491                2.326577   \n",
      "4224                   -0.596625                2.547033   \n",
      "4225                   -2.307309               -3.481649   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_pericalcarine_thickness  SurfaceHoles  \\\n",
      "0                   1.400042                   -1.478966     -1.848772   \n",
      "1                   1.654019                   -0.614001     -1.386121   \n",
      "2                   1.489074                   -1.055623     -1.176158   \n",
      "3                   1.100188                   -0.240118     -1.374965   \n",
      "4                   2.126696                   -0.804164     -1.324106   \n",
      "...                      ...                         ...           ...   \n",
      "4221                0.438951                    0.939479      0.642613   \n",
      "4222                1.632490                   -3.114717      1.459527   \n",
      "4223                3.899386                   -2.704619      1.086624   \n",
      "4224                6.998379                   -0.534857      2.483849   \n",
      "4225                2.207858                   -2.810635      1.635403   \n",
      "\n",
      "      CC_Posterior  rh_caudalanteriorcingulate_thickness  CC_Posterior  \\\n",
      "0         2.539626                             -1.143975     -0.401122   \n",
      "1         2.361501                             -1.447363     -0.844003   \n",
      "2         2.161189                             -2.168337     -0.805680   \n",
      "3         2.411123                             -1.143665     -1.558788   \n",
      "4         3.070594                             -1.211187     -0.811768   \n",
      "...            ...                                   ...           ...   \n",
      "4221     -0.406872                             -1.102426     -0.922743   \n",
      "4222     -0.233831                             -0.562018     -0.909443   \n",
      "4223      2.365611                              1.473648      2.351771   \n",
      "4224      4.645213                              2.403529      0.988284   \n",
      "4225      1.921893                              1.090244     -0.159457   \n",
      "\n",
      "      Right-Caudate  lh_parahippocampal_thickness  MaskVol-to-eTIV  \\\n",
      "0         -0.358963                      1.582873        -0.187048   \n",
      "1         -0.757418                      1.738769         0.214804   \n",
      "2         -1.120962                      1.480457         0.099160   \n",
      "3         -0.835127                      1.332337         0.311221   \n",
      "4         -0.528026                      1.545616        -0.073550   \n",
      "...             ...                           ...              ...   \n",
      "4221      -0.654735                     -0.834200        -0.194721   \n",
      "4222       0.521745                     -0.177853        -0.682310   \n",
      "4223       1.212131                      0.533024         1.469945   \n",
      "4224       4.397097                      1.473408         0.704190   \n",
      "4225       2.164417                      0.204689        -1.082658   \n",
      "\n",
      "      Brain-Stem  Left-vessel  Right-vessel  non-WM-hypointensities  \\\n",
      "0       0.209257    -1.568421      0.575948               -0.396241   \n",
      "1       0.415871    -1.436860      0.206113               -0.794917   \n",
      "2       0.376384    -0.886978      0.688299               -0.881096   \n",
      "3      -0.305927    -1.329661      0.539914               -0.262982   \n",
      "4       0.434925    -1.436719      1.267570               -0.848368   \n",
      "...          ...          ...           ...                     ...   \n",
      "4221   -0.238110     1.758533      0.052343                0.007779   \n",
      "4222   -0.987577     0.120706     -0.488267               -0.816884   \n",
      "4223   -0.640863    -0.433954     -1.227835               -0.222189   \n",
      "4224    2.025961     0.834684     -1.558114               -1.264430   \n",
      "4225   -0.211648    -0.356349     -1.270502               -0.708634   \n",
      "\n",
      "      rh_isthmuscingulate_thickness  5th-Ventricle  5th-Ventricle  \\\n",
      "0                          0.533530       0.052257      -0.504310   \n",
      "1                          0.176967      -0.410607      -0.512574   \n",
      "2                          0.397028      -0.605281      -0.206022   \n",
      "3                          0.167378      -0.243812      -0.233506   \n",
      "4                         -0.023117       0.101645      -0.462372   \n",
      "...                             ...            ...            ...   \n",
      "4221                       0.093490      -0.804716      -0.474969   \n",
      "4222                       0.101665       0.490317      -0.597087   \n",
      "4223                       0.535755       1.400179       1.011579   \n",
      "4224                       1.035599       0.548394       1.591074   \n",
      "4225                       1.882211       1.327482       1.265542   \n",
      "\n",
      "      lh_rostralanteriorcingulate_thickness  lh_entorhinal_thickness  \n",
      "0                                  0.757531                -0.422710  \n",
      "1                                  1.251018                -0.393669  \n",
      "2                                  0.871157                -0.859068  \n",
      "3                                  0.802075                -0.969425  \n",
      "4                                  1.381600                -0.422679  \n",
      "...                                     ...                      ...  \n",
      "4221                              -0.547420                 1.599059  \n",
      "4222                               1.444307                 1.524585  \n",
      "4223                               0.056188                 1.537667  \n",
      "4224                              -0.322787                 0.609829  \n",
      "4225                              -0.209826                 1.872825  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_pericalcarine_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>lh_parahippocampal_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Brain-Stem</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>lh_rostralanteriorcingulate_thickness</th>\n",
       "      <th>lh_entorhinal_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.754401</td>\n",
       "      <td>1.293658</td>\n",
       "      <td>1.400042</td>\n",
       "      <td>-1.478966</td>\n",
       "      <td>-1.848772</td>\n",
       "      <td>2.539626</td>\n",
       "      <td>-1.143975</td>\n",
       "      <td>-0.401122</td>\n",
       "      <td>-0.358963</td>\n",
       "      <td>1.582873</td>\n",
       "      <td>-0.187048</td>\n",
       "      <td>0.209257</td>\n",
       "      <td>-1.568421</td>\n",
       "      <td>0.575948</td>\n",
       "      <td>-0.396241</td>\n",
       "      <td>0.533530</td>\n",
       "      <td>0.052257</td>\n",
       "      <td>-0.504310</td>\n",
       "      <td>0.757531</td>\n",
       "      <td>-0.422710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.417516</td>\n",
       "      <td>1.506790</td>\n",
       "      <td>1.654019</td>\n",
       "      <td>-0.614001</td>\n",
       "      <td>-1.386121</td>\n",
       "      <td>2.361501</td>\n",
       "      <td>-1.447363</td>\n",
       "      <td>-0.844003</td>\n",
       "      <td>-0.757418</td>\n",
       "      <td>1.738769</td>\n",
       "      <td>0.214804</td>\n",
       "      <td>0.415871</td>\n",
       "      <td>-1.436860</td>\n",
       "      <td>0.206113</td>\n",
       "      <td>-0.794917</td>\n",
       "      <td>0.176967</td>\n",
       "      <td>-0.410607</td>\n",
       "      <td>-0.512574</td>\n",
       "      <td>1.251018</td>\n",
       "      <td>-0.393669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.060537</td>\n",
       "      <td>1.356490</td>\n",
       "      <td>1.489074</td>\n",
       "      <td>-1.055623</td>\n",
       "      <td>-1.176158</td>\n",
       "      <td>2.161189</td>\n",
       "      <td>-2.168337</td>\n",
       "      <td>-0.805680</td>\n",
       "      <td>-1.120962</td>\n",
       "      <td>1.480457</td>\n",
       "      <td>0.099160</td>\n",
       "      <td>0.376384</td>\n",
       "      <td>-0.886978</td>\n",
       "      <td>0.688299</td>\n",
       "      <td>-0.881096</td>\n",
       "      <td>0.397028</td>\n",
       "      <td>-0.605281</td>\n",
       "      <td>-0.206022</td>\n",
       "      <td>0.871157</td>\n",
       "      <td>-0.859068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.321472</td>\n",
       "      <td>1.316557</td>\n",
       "      <td>1.100188</td>\n",
       "      <td>-0.240118</td>\n",
       "      <td>-1.374965</td>\n",
       "      <td>2.411123</td>\n",
       "      <td>-1.143665</td>\n",
       "      <td>-1.558788</td>\n",
       "      <td>-0.835127</td>\n",
       "      <td>1.332337</td>\n",
       "      <td>0.311221</td>\n",
       "      <td>-0.305927</td>\n",
       "      <td>-1.329661</td>\n",
       "      <td>0.539914</td>\n",
       "      <td>-0.262982</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>-0.243812</td>\n",
       "      <td>-0.233506</td>\n",
       "      <td>0.802075</td>\n",
       "      <td>-0.969425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.432616</td>\n",
       "      <td>1.645476</td>\n",
       "      <td>2.126696</td>\n",
       "      <td>-0.804164</td>\n",
       "      <td>-1.324106</td>\n",
       "      <td>3.070594</td>\n",
       "      <td>-1.211187</td>\n",
       "      <td>-0.811768</td>\n",
       "      <td>-0.528026</td>\n",
       "      <td>1.545616</td>\n",
       "      <td>-0.073550</td>\n",
       "      <td>0.434925</td>\n",
       "      <td>-1.436719</td>\n",
       "      <td>1.267570</td>\n",
       "      <td>-0.848368</td>\n",
       "      <td>-0.023117</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>-0.462372</td>\n",
       "      <td>1.381600</td>\n",
       "      <td>-0.422679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    1.754401                1.293658                1.400042   \n",
       "1                    1.417516                1.506790                1.654019   \n",
       "2                    2.060537                1.356490                1.489074   \n",
       "3                    4.321472                1.316557                1.100188   \n",
       "4                    3.432616                1.645476                2.126696   \n",
       "\n",
       "   lh_pericalcarine_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                   -1.478966     -1.848772      2.539626   \n",
       "1                   -0.614001     -1.386121      2.361501   \n",
       "2                   -1.055623     -1.176158      2.161189   \n",
       "3                   -0.240118     -1.374965      2.411123   \n",
       "4                   -0.804164     -1.324106      3.070594   \n",
       "\n",
       "   rh_caudalanteriorcingulate_thickness  CC_Posterior  Right-Caudate  \\\n",
       "0                             -1.143975     -0.401122      -0.358963   \n",
       "1                             -1.447363     -0.844003      -0.757418   \n",
       "2                             -2.168337     -0.805680      -1.120962   \n",
       "3                             -1.143665     -1.558788      -0.835127   \n",
       "4                             -1.211187     -0.811768      -0.528026   \n",
       "\n",
       "   lh_parahippocampal_thickness  MaskVol-to-eTIV  Brain-Stem  Left-vessel  \\\n",
       "0                      1.582873        -0.187048    0.209257    -1.568421   \n",
       "1                      1.738769         0.214804    0.415871    -1.436860   \n",
       "2                      1.480457         0.099160    0.376384    -0.886978   \n",
       "3                      1.332337         0.311221   -0.305927    -1.329661   \n",
       "4                      1.545616        -0.073550    0.434925    -1.436719   \n",
       "\n",
       "   Right-vessel  non-WM-hypointensities  rh_isthmuscingulate_thickness  \\\n",
       "0      0.575948               -0.396241                       0.533530   \n",
       "1      0.206113               -0.794917                       0.176967   \n",
       "2      0.688299               -0.881096                       0.397028   \n",
       "3      0.539914               -0.262982                       0.167378   \n",
       "4      1.267570               -0.848368                      -0.023117   \n",
       "\n",
       "   5th-Ventricle  5th-Ventricle  lh_rostralanteriorcingulate_thickness  \\\n",
       "0       0.052257      -0.504310                               0.757531   \n",
       "1      -0.410607      -0.512574                               1.251018   \n",
       "2      -0.605281      -0.206022                               0.871157   \n",
       "3      -0.243812      -0.233506                               0.802075   \n",
       "4       0.101645      -0.462372                               1.381600   \n",
       "\n",
       "   lh_entorhinal_thickness  \n",
       "0                -0.422710  \n",
       "1                -0.393669  \n",
       "2                -0.859068  \n",
       "3                -0.969425  \n",
       "4                -0.422679  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data_feat)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data_feat.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755d5dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863ba7f",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16899a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_79 (Dense)            (None, 128)               2688      \n",
      "                                                                 \n",
      " batch_normalization_71 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_72 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_80 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_81 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_82 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_83 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,769\n",
      "Trainable params: 334,209\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0025)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a541f",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f959a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 62ms/step - loss: 57.9875 - mean_absolute_error: 57.9875 - val_loss: 68.2866 - val_mean_absolute_error: 68.2866\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 55.0752 - mean_absolute_error: 55.0752 - val_loss: 61.2783 - val_mean_absolute_error: 61.2783\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 52.6577 - mean_absolute_error: 52.6577 - val_loss: 52.3720 - val_mean_absolute_error: 52.3720\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 50.1742 - mean_absolute_error: 50.1742 - val_loss: 47.7643 - val_mean_absolute_error: 47.7643\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 47.2877 - mean_absolute_error: 47.2877 - val_loss: 45.4951 - val_mean_absolute_error: 45.4951\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 43.9716 - mean_absolute_error: 43.9716 - val_loss: 42.2484 - val_mean_absolute_error: 42.2484\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 40.0319 - mean_absolute_error: 40.0319 - val_loss: 39.0528 - val_mean_absolute_error: 39.0528\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 35.3999 - mean_absolute_error: 35.3999 - val_loss: 36.1360 - val_mean_absolute_error: 36.1360\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 30.3388 - mean_absolute_error: 30.3388 - val_loss: 32.1395 - val_mean_absolute_error: 32.1395\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 24.6718 - mean_absolute_error: 24.6718 - val_loss: 25.9971 - val_mean_absolute_error: 25.9971\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 18.6099 - mean_absolute_error: 18.6099 - val_loss: 21.0968 - val_mean_absolute_error: 21.0968\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 12.5026 - mean_absolute_error: 12.5026 - val_loss: 13.5715 - val_mean_absolute_error: 13.5715\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 8.3808 - mean_absolute_error: 8.3808 - val_loss: 17.3887 - val_mean_absolute_error: 17.3887\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 7.0759 - mean_absolute_error: 7.0759 - val_loss: 12.8641 - val_mean_absolute_error: 12.8641\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.8969 - mean_absolute_error: 6.8969 - val_loss: 11.8585 - val_mean_absolute_error: 11.8585\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 6.7982 - mean_absolute_error: 6.7982 - val_loss: 9.8304 - val_mean_absolute_error: 9.8304\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.6795 - mean_absolute_error: 6.6795 - val_loss: 7.7535 - val_mean_absolute_error: 7.7535\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.4511 - mean_absolute_error: 6.4511 - val_loss: 7.7836 - val_mean_absolute_error: 7.7836\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.3195 - mean_absolute_error: 6.3195 - val_loss: 7.5270 - val_mean_absolute_error: 7.5270\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 6.4039 - mean_absolute_error: 6.4039 - val_loss: 8.2798 - val_mean_absolute_error: 8.2798\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.6426 - mean_absolute_error: 6.6426 - val_loss: 7.0464 - val_mean_absolute_error: 7.0464\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.2681 - mean_absolute_error: 6.2681 - val_loss: 6.8116 - val_mean_absolute_error: 6.8116\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 6.3150 - mean_absolute_error: 6.3150 - val_loss: 7.5610 - val_mean_absolute_error: 7.5610\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 6.0893 - mean_absolute_error: 6.0893 - val_loss: 7.2338 - val_mean_absolute_error: 7.2338\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 6.0990 - mean_absolute_error: 6.0990 - val_loss: 6.5179 - val_mean_absolute_error: 6.5179\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 6.0034 - mean_absolute_error: 6.0034 - val_loss: 6.1299 - val_mean_absolute_error: 6.1299\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.1226 - mean_absolute_error: 6.1226 - val_loss: 5.8366 - val_mean_absolute_error: 5.8366\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.9200 - mean_absolute_error: 5.9200 - val_loss: 5.7173 - val_mean_absolute_error: 5.7173\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.8608 - mean_absolute_error: 5.8608 - val_loss: 6.3053 - val_mean_absolute_error: 6.3053\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.8486 - mean_absolute_error: 5.8486 - val_loss: 5.7954 - val_mean_absolute_error: 5.7954\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 5.8802 - mean_absolute_error: 5.8802 - val_loss: 5.6895 - val_mean_absolute_error: 5.6895\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.8214 - mean_absolute_error: 5.8214 - val_loss: 5.8953 - val_mean_absolute_error: 5.8953\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.7360 - mean_absolute_error: 5.7360 - val_loss: 6.0529 - val_mean_absolute_error: 6.0529\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.8348 - mean_absolute_error: 5.8348 - val_loss: 5.8100 - val_mean_absolute_error: 5.8100\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.7800 - mean_absolute_error: 5.7800 - val_loss: 5.7466 - val_mean_absolute_error: 5.7466\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.6746 - mean_absolute_error: 5.6746 - val_loss: 6.5097 - val_mean_absolute_error: 6.5097\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.8032 - mean_absolute_error: 5.8032 - val_loss: 5.5079 - val_mean_absolute_error: 5.5079\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.7502 - mean_absolute_error: 5.7502 - val_loss: 5.4506 - val_mean_absolute_error: 5.4506\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.5333 - mean_absolute_error: 5.5333 - val_loss: 5.6075 - val_mean_absolute_error: 5.6075\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.4541 - mean_absolute_error: 5.4541 - val_loss: 5.4498 - val_mean_absolute_error: 5.4498\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.5401 - mean_absolute_error: 5.5401 - val_loss: 5.6452 - val_mean_absolute_error: 5.6452\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.3751 - mean_absolute_error: 5.3751 - val_loss: 5.4657 - val_mean_absolute_error: 5.4657\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.3039 - mean_absolute_error: 5.3039 - val_loss: 5.3067 - val_mean_absolute_error: 5.3067\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.2802 - mean_absolute_error: 5.2802 - val_loss: 5.4609 - val_mean_absolute_error: 5.4609\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.1772 - mean_absolute_error: 5.1772 - val_loss: 5.3169 - val_mean_absolute_error: 5.3169\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.4192 - mean_absolute_error: 5.4192 - val_loss: 5.4812 - val_mean_absolute_error: 5.4812\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.5042 - mean_absolute_error: 5.5042 - val_loss: 6.5203 - val_mean_absolute_error: 6.5203\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.2789 - mean_absolute_error: 5.2789 - val_loss: 5.4597 - val_mean_absolute_error: 5.4597\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.1636 - mean_absolute_error: 5.1636 - val_loss: 5.5532 - val_mean_absolute_error: 5.5532\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 14ms/step - loss: 5.3187 - mean_absolute_error: 5.3187 - val_loss: 5.6506 - val_mean_absolute_error: 5.6506\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.2257 - mean_absolute_error: 5.2257 - val_loss: 5.3419 - val_mean_absolute_error: 5.3419\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.1196 - mean_absolute_error: 5.1196 - val_loss: 5.6424 - val_mean_absolute_error: 5.6424\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.2264 - mean_absolute_error: 5.2264 - val_loss: 5.6571 - val_mean_absolute_error: 5.6571\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.1613 - mean_absolute_error: 5.1613 - val_loss: 5.8148 - val_mean_absolute_error: 5.8148\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.0761 - mean_absolute_error: 5.0761 - val_loss: 6.3250 - val_mean_absolute_error: 6.3250\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.0980 - mean_absolute_error: 5.0980 - val_loss: 5.5754 - val_mean_absolute_error: 5.5754\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.2433 - mean_absolute_error: 5.2433 - val_loss: 5.5225 - val_mean_absolute_error: 5.5225\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 4.9901 - mean_absolute_error: 4.9901 - val_loss: 6.0621 - val_mean_absolute_error: 6.0621\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=15, restore_best_weights = True)\n",
    "\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    batch_size=256,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b1a71",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a26a1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8831901333618151\n",
      "Max Error: 25.606857299804688\n",
      "Mean absolute error: 4.917904655888395\n",
      "Mean squared error: 45.80640497681754\n",
      "Root Mean squared error: 6.768042920728084\n",
      "R2: 0.8831733708137876\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4771e649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(58))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='Val Mean Average Error')\n",
    "plt.plot( x, ty, label='Train Mean Average Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba9397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
