{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e707cbf8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f70621",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8e79a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3ad42",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe01f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba71637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       1.754401                1.293658   \n",
      "1                       1.417516                1.506790   \n",
      "2                       2.060537                1.356490   \n",
      "3                       4.321472                1.316557   \n",
      "4                       3.432616                1.645476   \n",
      "...                          ...                     ...   \n",
      "4221                    3.508241                2.349695   \n",
      "4222                    4.445945               -2.409495   \n",
      "4223                    8.016491                2.326577   \n",
      "4224                   -0.596625                2.547033   \n",
      "4225                   -2.307309               -3.481649   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_pericalcarine_thickness  SurfaceHoles  \\\n",
      "0                   1.400042                   -1.478966     -1.848772   \n",
      "1                   1.654019                   -0.614001     -1.386121   \n",
      "2                   1.489074                   -1.055623     -1.176158   \n",
      "3                   1.100188                   -0.240118     -1.374965   \n",
      "4                   2.126696                   -0.804164     -1.324106   \n",
      "...                      ...                         ...           ...   \n",
      "4221                0.438951                    0.939479      0.642613   \n",
      "4222                1.632490                   -3.114717      1.459527   \n",
      "4223                3.899386                   -2.704619      1.086624   \n",
      "4224                6.998379                   -0.534857      2.483849   \n",
      "4225                2.207858                   -2.810635      1.635403   \n",
      "\n",
      "      CC_Posterior  rh_caudalanteriorcingulate_thickness  CC_Posterior  \\\n",
      "0         2.539626                             -1.143975     -0.401122   \n",
      "1         2.361501                             -1.447363     -0.844003   \n",
      "2         2.161189                             -2.168337     -0.805680   \n",
      "3         2.411123                             -1.143665     -1.558788   \n",
      "4         3.070594                             -1.211187     -0.811768   \n",
      "...            ...                                   ...           ...   \n",
      "4221     -0.406872                             -1.102426     -0.922743   \n",
      "4222     -0.233831                             -0.562018     -0.909443   \n",
      "4223      2.365611                              1.473648      2.351771   \n",
      "4224      4.645213                              2.403529      0.988284   \n",
      "4225      1.921893                              1.090244     -0.159457   \n",
      "\n",
      "      Right-Caudate  lh_parahippocampal_thickness  MaskVol-to-eTIV  \\\n",
      "0         -0.358963                      1.582873        -0.187048   \n",
      "1         -0.757418                      1.738769         0.214804   \n",
      "2         -1.120962                      1.480457         0.099160   \n",
      "3         -0.835127                      1.332337         0.311221   \n",
      "4         -0.528026                      1.545616        -0.073550   \n",
      "...             ...                           ...              ...   \n",
      "4221      -0.654735                     -0.834200        -0.194721   \n",
      "4222       0.521745                     -0.177853        -0.682310   \n",
      "4223       1.212131                      0.533024         1.469945   \n",
      "4224       4.397097                      1.473408         0.704190   \n",
      "4225       2.164417                      0.204689        -1.082658   \n",
      "\n",
      "      Brain-Stem  Left-vessel  Right-vessel  non-WM-hypointensities  \\\n",
      "0       0.209257    -1.568421      0.575948               -0.396241   \n",
      "1       0.415871    -1.436860      0.206113               -0.794917   \n",
      "2       0.376384    -0.886978      0.688299               -0.881096   \n",
      "3      -0.305927    -1.329661      0.539914               -0.262982   \n",
      "4       0.434925    -1.436719      1.267570               -0.848368   \n",
      "...          ...          ...           ...                     ...   \n",
      "4221   -0.238110     1.758533      0.052343                0.007779   \n",
      "4222   -0.987577     0.120706     -0.488267               -0.816884   \n",
      "4223   -0.640863    -0.433954     -1.227835               -0.222189   \n",
      "4224    2.025961     0.834684     -1.558114               -1.264430   \n",
      "4225   -0.211648    -0.356349     -1.270502               -0.708634   \n",
      "\n",
      "      rh_isthmuscingulate_thickness  5th-Ventricle  5th-Ventricle  \\\n",
      "0                          0.533530       0.052257      -0.504310   \n",
      "1                          0.176967      -0.410607      -0.512574   \n",
      "2                          0.397028      -0.605281      -0.206022   \n",
      "3                          0.167378      -0.243812      -0.233506   \n",
      "4                         -0.023117       0.101645      -0.462372   \n",
      "...                             ...            ...            ...   \n",
      "4221                       0.093490      -0.804716      -0.474969   \n",
      "4222                       0.101665       0.490317      -0.597087   \n",
      "4223                       0.535755       1.400179       1.011579   \n",
      "4224                       1.035599       0.548394       1.591074   \n",
      "4225                       1.882211       1.327482       1.265542   \n",
      "\n",
      "      lh_rostralanteriorcingulate_thickness  lh_entorhinal_thickness  \n",
      "0                                  0.757531                -0.422710  \n",
      "1                                  1.251018                -0.393669  \n",
      "2                                  0.871157                -0.859068  \n",
      "3                                  0.802075                -0.969425  \n",
      "4                                  1.381600                -0.422679  \n",
      "...                                     ...                      ...  \n",
      "4221                              -0.547420                 1.599059  \n",
      "4222                               1.444307                 1.524585  \n",
      "4223                               0.056188                 1.537667  \n",
      "4224                              -0.322787                 0.609829  \n",
      "4225                              -0.209826                 1.872825  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_pericalcarine_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>lh_parahippocampal_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Brain-Stem</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>lh_rostralanteriorcingulate_thickness</th>\n",
       "      <th>lh_entorhinal_thickness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.754401</td>\n",
       "      <td>1.293658</td>\n",
       "      <td>1.400042</td>\n",
       "      <td>-1.478966</td>\n",
       "      <td>-1.848772</td>\n",
       "      <td>2.539626</td>\n",
       "      <td>-1.143975</td>\n",
       "      <td>-0.401122</td>\n",
       "      <td>-0.358963</td>\n",
       "      <td>1.582873</td>\n",
       "      <td>-0.187048</td>\n",
       "      <td>0.209257</td>\n",
       "      <td>-1.568421</td>\n",
       "      <td>0.575948</td>\n",
       "      <td>-0.396241</td>\n",
       "      <td>0.533530</td>\n",
       "      <td>0.052257</td>\n",
       "      <td>-0.504310</td>\n",
       "      <td>0.757531</td>\n",
       "      <td>-0.422710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.417516</td>\n",
       "      <td>1.506790</td>\n",
       "      <td>1.654019</td>\n",
       "      <td>-0.614001</td>\n",
       "      <td>-1.386121</td>\n",
       "      <td>2.361501</td>\n",
       "      <td>-1.447363</td>\n",
       "      <td>-0.844003</td>\n",
       "      <td>-0.757418</td>\n",
       "      <td>1.738769</td>\n",
       "      <td>0.214804</td>\n",
       "      <td>0.415871</td>\n",
       "      <td>-1.436860</td>\n",
       "      <td>0.206113</td>\n",
       "      <td>-0.794917</td>\n",
       "      <td>0.176967</td>\n",
       "      <td>-0.410607</td>\n",
       "      <td>-0.512574</td>\n",
       "      <td>1.251018</td>\n",
       "      <td>-0.393669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.060537</td>\n",
       "      <td>1.356490</td>\n",
       "      <td>1.489074</td>\n",
       "      <td>-1.055623</td>\n",
       "      <td>-1.176158</td>\n",
       "      <td>2.161189</td>\n",
       "      <td>-2.168337</td>\n",
       "      <td>-0.805680</td>\n",
       "      <td>-1.120962</td>\n",
       "      <td>1.480457</td>\n",
       "      <td>0.099160</td>\n",
       "      <td>0.376384</td>\n",
       "      <td>-0.886978</td>\n",
       "      <td>0.688299</td>\n",
       "      <td>-0.881096</td>\n",
       "      <td>0.397028</td>\n",
       "      <td>-0.605281</td>\n",
       "      <td>-0.206022</td>\n",
       "      <td>0.871157</td>\n",
       "      <td>-0.859068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.321472</td>\n",
       "      <td>1.316557</td>\n",
       "      <td>1.100188</td>\n",
       "      <td>-0.240118</td>\n",
       "      <td>-1.374965</td>\n",
       "      <td>2.411123</td>\n",
       "      <td>-1.143665</td>\n",
       "      <td>-1.558788</td>\n",
       "      <td>-0.835127</td>\n",
       "      <td>1.332337</td>\n",
       "      <td>0.311221</td>\n",
       "      <td>-0.305927</td>\n",
       "      <td>-1.329661</td>\n",
       "      <td>0.539914</td>\n",
       "      <td>-0.262982</td>\n",
       "      <td>0.167378</td>\n",
       "      <td>-0.243812</td>\n",
       "      <td>-0.233506</td>\n",
       "      <td>0.802075</td>\n",
       "      <td>-0.969425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.432616</td>\n",
       "      <td>1.645476</td>\n",
       "      <td>2.126696</td>\n",
       "      <td>-0.804164</td>\n",
       "      <td>-1.324106</td>\n",
       "      <td>3.070594</td>\n",
       "      <td>-1.211187</td>\n",
       "      <td>-0.811768</td>\n",
       "      <td>-0.528026</td>\n",
       "      <td>1.545616</td>\n",
       "      <td>-0.073550</td>\n",
       "      <td>0.434925</td>\n",
       "      <td>-1.436719</td>\n",
       "      <td>1.267570</td>\n",
       "      <td>-0.848368</td>\n",
       "      <td>-0.023117</td>\n",
       "      <td>0.101645</td>\n",
       "      <td>-0.462372</td>\n",
       "      <td>1.381600</td>\n",
       "      <td>-0.422679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    1.754401                1.293658                1.400042   \n",
       "1                    1.417516                1.506790                1.654019   \n",
       "2                    2.060537                1.356490                1.489074   \n",
       "3                    4.321472                1.316557                1.100188   \n",
       "4                    3.432616                1.645476                2.126696   \n",
       "\n",
       "   lh_pericalcarine_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                   -1.478966     -1.848772      2.539626   \n",
       "1                   -0.614001     -1.386121      2.361501   \n",
       "2                   -1.055623     -1.176158      2.161189   \n",
       "3                   -0.240118     -1.374965      2.411123   \n",
       "4                   -0.804164     -1.324106      3.070594   \n",
       "\n",
       "   rh_caudalanteriorcingulate_thickness  CC_Posterior  Right-Caudate  \\\n",
       "0                             -1.143975     -0.401122      -0.358963   \n",
       "1                             -1.447363     -0.844003      -0.757418   \n",
       "2                             -2.168337     -0.805680      -1.120962   \n",
       "3                             -1.143665     -1.558788      -0.835127   \n",
       "4                             -1.211187     -0.811768      -0.528026   \n",
       "\n",
       "   lh_parahippocampal_thickness  MaskVol-to-eTIV  Brain-Stem  Left-vessel  \\\n",
       "0                      1.582873        -0.187048    0.209257    -1.568421   \n",
       "1                      1.738769         0.214804    0.415871    -1.436860   \n",
       "2                      1.480457         0.099160    0.376384    -0.886978   \n",
       "3                      1.332337         0.311221   -0.305927    -1.329661   \n",
       "4                      1.545616        -0.073550    0.434925    -1.436719   \n",
       "\n",
       "   Right-vessel  non-WM-hypointensities  rh_isthmuscingulate_thickness  \\\n",
       "0      0.575948               -0.396241                       0.533530   \n",
       "1      0.206113               -0.794917                       0.176967   \n",
       "2      0.688299               -0.881096                       0.397028   \n",
       "3      0.539914               -0.262982                       0.167378   \n",
       "4      1.267570               -0.848368                      -0.023117   \n",
       "\n",
       "   5th-Ventricle  5th-Ventricle  lh_rostralanteriorcingulate_thickness  \\\n",
       "0       0.052257      -0.504310                               0.757531   \n",
       "1      -0.410607      -0.512574                               1.251018   \n",
       "2      -0.605281      -0.206022                               0.871157   \n",
       "3      -0.243812      -0.233506                               0.802075   \n",
       "4       0.101645      -0.462372                               1.381600   \n",
       "\n",
       "   lh_entorhinal_thickness  \n",
       "0                -0.422710  \n",
       "1                -0.393669  \n",
       "2                -0.859068  \n",
       "3                -0.969425  \n",
       "4                -0.422679  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data_feat)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data_feat.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "755d5dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863ba7f",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "16899a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_79 (Dense)            (None, 128)               2688      \n",
      "                                                                 \n",
      " batch_normalization_71 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_79 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_26 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " batch_normalization_72 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_80 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 512)               131584    \n",
      "                                                                 \n",
      " batch_normalization_73 (Bat  (None, 512)              2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_81 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 256)               131328    \n",
      "                                                                 \n",
      " batch_normalization_74 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_82 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_75 (Bat  (None, 128)              512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_83 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_27 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 336,769\n",
      "Trainable params: 334,209\n",
      "Non-trainable params: 2,560\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(512))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "opt = Adam(learning_rate=0.0025)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a541f",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f959a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "14/14 [==============================] - 2s 62ms/step - loss: 57.9875 - mean_absolute_error: 57.9875 - val_loss: 68.2866 - val_mean_absolute_error: 68.2866\n",
      "Epoch 2/200\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 55.0752 - mean_absolute_error: 55.0752 - val_loss: 61.2783 - val_mean_absolute_error: 61.2783\n",
      "Epoch 3/200\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 52.6577 - mean_absolute_error: 52.6577 - val_loss: 52.3720 - val_mean_absolute_error: 52.3720\n",
      "Epoch 4/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 50.1742 - mean_absolute_error: 50.1742 - val_loss: 47.7643 - val_mean_absolute_error: 47.7643\n",
      "Epoch 5/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 47.2877 - mean_absolute_error: 47.2877 - val_loss: 45.4951 - val_mean_absolute_error: 45.4951\n",
      "Epoch 6/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 43.9716 - mean_absolute_error: 43.9716 - val_loss: 42.2484 - val_mean_absolute_error: 42.2484\n",
      "Epoch 7/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 40.0319 - mean_absolute_error: 40.0319 - val_loss: 39.0528 - val_mean_absolute_error: 39.0528\n",
      "Epoch 8/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 35.3999 - mean_absolute_error: 35.3999 - val_loss: 36.1360 - val_mean_absolute_error: 36.1360\n",
      "Epoch 9/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 30.3388 - mean_absolute_error: 30.3388 - val_loss: 32.1395 - val_mean_absolute_error: 32.1395\n",
      "Epoch 10/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 24.6718 - mean_absolute_error: 24.6718 - val_loss: 25.9971 - val_mean_absolute_error: 25.9971\n",
      "Epoch 11/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 18.6099 - mean_absolute_error: 18.6099 - val_loss: 21.0968 - val_mean_absolute_error: 21.0968\n",
      "Epoch 12/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 12.5026 - mean_absolute_error: 12.5026 - val_loss: 13.5715 - val_mean_absolute_error: 13.5715\n",
      "Epoch 13/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 8.3808 - mean_absolute_error: 8.3808 - val_loss: 17.3887 - val_mean_absolute_error: 17.3887\n",
      "Epoch 14/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 7.0759 - mean_absolute_error: 7.0759 - val_loss: 12.8641 - val_mean_absolute_error: 12.8641\n",
      "Epoch 15/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.8969 - mean_absolute_error: 6.8969 - val_loss: 11.8585 - val_mean_absolute_error: 11.8585\n",
      "Epoch 16/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 6.7982 - mean_absolute_error: 6.7982 - val_loss: 9.8304 - val_mean_absolute_error: 9.8304\n",
      "Epoch 17/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.6795 - mean_absolute_error: 6.6795 - val_loss: 7.7535 - val_mean_absolute_error: 7.7535\n",
      "Epoch 18/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.4511 - mean_absolute_error: 6.4511 - val_loss: 7.7836 - val_mean_absolute_error: 7.7836\n",
      "Epoch 19/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.3195 - mean_absolute_error: 6.3195 - val_loss: 7.5270 - val_mean_absolute_error: 7.5270\n",
      "Epoch 20/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 6.4039 - mean_absolute_error: 6.4039 - val_loss: 8.2798 - val_mean_absolute_error: 8.2798\n",
      "Epoch 21/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.6426 - mean_absolute_error: 6.6426 - val_loss: 7.0464 - val_mean_absolute_error: 7.0464\n",
      "Epoch 22/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.2681 - mean_absolute_error: 6.2681 - val_loss: 6.8116 - val_mean_absolute_error: 6.8116\n",
      "Epoch 23/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 6.3150 - mean_absolute_error: 6.3150 - val_loss: 7.5610 - val_mean_absolute_error: 7.5610\n",
      "Epoch 24/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 6.0893 - mean_absolute_error: 6.0893 - val_loss: 7.2338 - val_mean_absolute_error: 7.2338\n",
      "Epoch 25/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 6.0990 - mean_absolute_error: 6.0990 - val_loss: 6.5179 - val_mean_absolute_error: 6.5179\n",
      "Epoch 26/200\n",
      "14/14 [==============================] - 0s 17ms/step - loss: 6.0034 - mean_absolute_error: 6.0034 - val_loss: 6.1299 - val_mean_absolute_error: 6.1299\n",
      "Epoch 27/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 6.1226 - mean_absolute_error: 6.1226 - val_loss: 5.8366 - val_mean_absolute_error: 5.8366\n",
      "Epoch 28/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.9200 - mean_absolute_error: 5.9200 - val_loss: 5.7173 - val_mean_absolute_error: 5.7173\n",
      "Epoch 29/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.8608 - mean_absolute_error: 5.8608 - val_loss: 6.3053 - val_mean_absolute_error: 6.3053\n",
      "Epoch 30/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.8486 - mean_absolute_error: 5.8486 - val_loss: 5.7954 - val_mean_absolute_error: 5.7954\n",
      "Epoch 31/200\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 5.8802 - mean_absolute_error: 5.8802 - val_loss: 5.6895 - val_mean_absolute_error: 5.6895\n",
      "Epoch 32/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.8214 - mean_absolute_error: 5.8214 - val_loss: 5.8953 - val_mean_absolute_error: 5.8953\n",
      "Epoch 33/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.7360 - mean_absolute_error: 5.7360 - val_loss: 6.0529 - val_mean_absolute_error: 6.0529\n",
      "Epoch 34/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.8348 - mean_absolute_error: 5.8348 - val_loss: 5.8100 - val_mean_absolute_error: 5.8100\n",
      "Epoch 35/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.7800 - mean_absolute_error: 5.7800 - val_loss: 5.7466 - val_mean_absolute_error: 5.7466\n",
      "Epoch 36/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.6746 - mean_absolute_error: 5.6746 - val_loss: 6.5097 - val_mean_absolute_error: 6.5097\n",
      "Epoch 37/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.8032 - mean_absolute_error: 5.8032 - val_loss: 5.5079 - val_mean_absolute_error: 5.5079\n",
      "Epoch 38/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.7502 - mean_absolute_error: 5.7502 - val_loss: 5.4506 - val_mean_absolute_error: 5.4506\n",
      "Epoch 39/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.5333 - mean_absolute_error: 5.5333 - val_loss: 5.6075 - val_mean_absolute_error: 5.6075\n",
      "Epoch 40/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.4541 - mean_absolute_error: 5.4541 - val_loss: 5.4498 - val_mean_absolute_error: 5.4498\n",
      "Epoch 41/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.5401 - mean_absolute_error: 5.5401 - val_loss: 5.6452 - val_mean_absolute_error: 5.6452\n",
      "Epoch 42/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.3751 - mean_absolute_error: 5.3751 - val_loss: 5.4657 - val_mean_absolute_error: 5.4657\n",
      "Epoch 43/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.3039 - mean_absolute_error: 5.3039 - val_loss: 5.3067 - val_mean_absolute_error: 5.3067\n",
      "Epoch 44/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.2802 - mean_absolute_error: 5.2802 - val_loss: 5.4609 - val_mean_absolute_error: 5.4609\n",
      "Epoch 45/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.1772 - mean_absolute_error: 5.1772 - val_loss: 5.3169 - val_mean_absolute_error: 5.3169\n",
      "Epoch 46/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.4192 - mean_absolute_error: 5.4192 - val_loss: 5.4812 - val_mean_absolute_error: 5.4812\n",
      "Epoch 47/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.5042 - mean_absolute_error: 5.5042 - val_loss: 6.5203 - val_mean_absolute_error: 6.5203\n",
      "Epoch 48/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.2789 - mean_absolute_error: 5.2789 - val_loss: 5.4597 - val_mean_absolute_error: 5.4597\n",
      "Epoch 49/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.1636 - mean_absolute_error: 5.1636 - val_loss: 5.5532 - val_mean_absolute_error: 5.5532\n",
      "Epoch 50/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 14ms/step - loss: 5.3187 - mean_absolute_error: 5.3187 - val_loss: 5.6506 - val_mean_absolute_error: 5.6506\n",
      "Epoch 51/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.2257 - mean_absolute_error: 5.2257 - val_loss: 5.3419 - val_mean_absolute_error: 5.3419\n",
      "Epoch 52/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.1196 - mean_absolute_error: 5.1196 - val_loss: 5.6424 - val_mean_absolute_error: 5.6424\n",
      "Epoch 53/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.2264 - mean_absolute_error: 5.2264 - val_loss: 5.6571 - val_mean_absolute_error: 5.6571\n",
      "Epoch 54/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.1613 - mean_absolute_error: 5.1613 - val_loss: 5.8148 - val_mean_absolute_error: 5.8148\n",
      "Epoch 55/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 5.0761 - mean_absolute_error: 5.0761 - val_loss: 6.3250 - val_mean_absolute_error: 6.3250\n",
      "Epoch 56/200\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 5.0980 - mean_absolute_error: 5.0980 - val_loss: 5.5754 - val_mean_absolute_error: 5.5754\n",
      "Epoch 57/200\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 5.2433 - mean_absolute_error: 5.2433 - val_loss: 5.5225 - val_mean_absolute_error: 5.5225\n",
      "Epoch 58/200\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 4.9901 - mean_absolute_error: 4.9901 - val_loss: 6.0621 - val_mean_absolute_error: 6.0621\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=15, restore_best_weights = True)\n",
    "\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    batch_size=256,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b1a71",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a26a1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8831901333618151\n",
      "Max Error: 25.606857299804688\n",
      "Mean absolute error: 4.917904655888395\n",
      "Mean squared error: 45.80640497681754\n",
      "Root Mean squared error: 6.768042920728084\n",
      "R2: 0.8831733708137876\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4771e649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(58))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='train_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
