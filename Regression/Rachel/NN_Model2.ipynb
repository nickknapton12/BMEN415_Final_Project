{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e707cbf8",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f70621",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e79a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e3ad42",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fe01f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ba71637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       1.754401                1.293660   \n",
      "1                       1.417516                1.506793   \n",
      "2                       2.060537                1.356492   \n",
      "3                       4.321472                1.316559   \n",
      "4                       3.432616                1.645477   \n",
      "...                          ...                     ...   \n",
      "4221                    3.508241                2.349696   \n",
      "4222                    4.445945               -2.409496   \n",
      "4223                    8.016491                2.326575   \n",
      "4224                   -0.596625                2.547033   \n",
      "4225                   -2.307309               -3.481650   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_pericalcarine_thickness  SurfaceHoles  \\\n",
      "0                   1.400115                   -1.478937     -1.847383   \n",
      "1                   1.654055                   -0.614039     -1.384412   \n",
      "2                   1.489056                   -1.055725     -1.174763   \n",
      "3                   1.100276                   -0.240485     -1.374596   \n",
      "4                   2.126718                   -0.804346     -1.322060   \n",
      "...                      ...                         ...           ...   \n",
      "4221                0.438899                    0.939370      0.642702   \n",
      "4222                1.632408                   -3.114586      1.459535   \n",
      "4223                3.899407                   -2.705141      1.087964   \n",
      "4224                6.998411                   -0.534463      2.483705   \n",
      "4225                2.207893                   -2.810567      1.635759   \n",
      "\n",
      "      CC_Posterior  rh_caudalanteriorcingulate_thickness  CC_Posterior  \\\n",
      "0         2.537129                             -1.131811     -0.411876   \n",
      "1         2.360426                             -1.435565     -0.852958   \n",
      "2         2.158388                             -2.160262     -0.816436   \n",
      "3         2.403613                             -1.136448     -1.573823   \n",
      "4         3.067034                             -1.201233     -0.821464   \n",
      "...            ...                                   ...           ...   \n",
      "4221     -0.406527                             -1.099513     -0.929022   \n",
      "4222     -0.231345                             -0.572511     -0.881495   \n",
      "4223      2.355939                              1.469039      2.368361   \n",
      "4224      4.655117                              2.402933      0.996348   \n",
      "4225      1.921839                              1.092998     -0.158999   \n",
      "\n",
      "      Right-Caudate  lh_parahippocampal_thickness  MaskVol-to-eTIV  \\\n",
      "0         -0.365695                      1.570148        -0.211262   \n",
      "1         -0.769807                      1.730512         0.200279   \n",
      "2         -1.128342                      1.477538         0.085750   \n",
      "3         -0.828518                      1.327238         0.307597   \n",
      "4         -0.529591                      1.538776        -0.084633   \n",
      "...             ...                           ...              ...   \n",
      "4221      -0.682520                     -0.828171        -0.170015   \n",
      "4222       0.530202                     -0.171666        -0.695339   \n",
      "4223       1.258791                      0.546837         1.443235   \n",
      "4224       4.387611                      1.485116         0.710323   \n",
      "4225       2.174795                      0.183873        -1.104877   \n",
      "\n",
      "      Brain-Stem  Left-vessel  Right-vessel  non-WM-hypointensities  \\\n",
      "0       0.181798    -1.580504      0.608887               -0.451223   \n",
      "1       0.374897    -1.464860      0.247169               -0.836050   \n",
      "2       0.332070    -0.931302      0.722037               -0.862200   \n",
      "3      -0.370367    -1.351926      0.578170               -0.393221   \n",
      "4       0.413713    -1.465500      1.276663               -0.818454   \n",
      "...          ...          ...           ...                     ...   \n",
      "4221   -0.333545     1.744204      0.126221                0.028070   \n",
      "4222   -0.967471     0.161050     -0.569571               -0.752712   \n",
      "4223   -0.613412    -0.388707     -1.387285               -0.038585   \n",
      "4224    2.105984     0.830718     -1.547625               -1.172294   \n",
      "4225   -0.135859    -0.313153     -1.259574               -0.668938   \n",
      "\n",
      "      rh_isthmuscingulate_thickness  5th-Ventricle  non-WM-hypointensities  \\\n",
      "0                          0.448570       0.272656               -0.428067   \n",
      "1                          0.136242      -0.184904               -0.430852   \n",
      "2                          0.412208      -0.445485               -0.149722   \n",
      "3                          0.097569      -0.217631               -0.288023   \n",
      "4                         -0.046595       0.336643               -0.417106   \n",
      "...                             ...            ...                     ...   \n",
      "4221                       0.158418      -0.741218               -0.277713   \n",
      "4222                       0.162564       0.611995               -0.614344   \n",
      "4223                       0.515308       1.257061                1.022207   \n",
      "4224                       0.929878       0.665619                1.396297   \n",
      "4225                       1.808543       1.350960                1.453354   \n",
      "\n",
      "      lh_rostralanteriorcingulate_thickness  5th-Ventricle  \n",
      "0                                  0.644490      -0.308911  \n",
      "1                                  1.096066       0.046217  \n",
      "2                                  0.721634       0.376761  \n",
      "3                                  0.759698       0.241880  \n",
      "4                                  1.173832      -0.102437  \n",
      "...                                     ...            ...  \n",
      "4221                              -0.473395      -0.636532  \n",
      "4222                               1.249251      -1.954051  \n",
      "4223                              -0.026117      -1.011247  \n",
      "4224                              -0.531671      -1.472874  \n",
      "4225                              -0.364949      -1.092798  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_pericalcarine_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>lh_parahippocampal_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Brain-Stem</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>lh_rostralanteriorcingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.754401</td>\n",
       "      <td>1.293660</td>\n",
       "      <td>1.400115</td>\n",
       "      <td>-1.478937</td>\n",
       "      <td>-1.847383</td>\n",
       "      <td>2.537129</td>\n",
       "      <td>-1.131811</td>\n",
       "      <td>-0.411876</td>\n",
       "      <td>-0.365695</td>\n",
       "      <td>1.570148</td>\n",
       "      <td>-0.211262</td>\n",
       "      <td>0.181798</td>\n",
       "      <td>-1.580504</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>-0.451223</td>\n",
       "      <td>0.448570</td>\n",
       "      <td>0.272656</td>\n",
       "      <td>-0.428067</td>\n",
       "      <td>0.644490</td>\n",
       "      <td>-0.308911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.417516</td>\n",
       "      <td>1.506793</td>\n",
       "      <td>1.654055</td>\n",
       "      <td>-0.614039</td>\n",
       "      <td>-1.384412</td>\n",
       "      <td>2.360426</td>\n",
       "      <td>-1.435565</td>\n",
       "      <td>-0.852958</td>\n",
       "      <td>-0.769807</td>\n",
       "      <td>1.730512</td>\n",
       "      <td>0.200279</td>\n",
       "      <td>0.374897</td>\n",
       "      <td>-1.464860</td>\n",
       "      <td>0.247169</td>\n",
       "      <td>-0.836050</td>\n",
       "      <td>0.136242</td>\n",
       "      <td>-0.184904</td>\n",
       "      <td>-0.430852</td>\n",
       "      <td>1.096066</td>\n",
       "      <td>0.046217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.060537</td>\n",
       "      <td>1.356492</td>\n",
       "      <td>1.489056</td>\n",
       "      <td>-1.055725</td>\n",
       "      <td>-1.174763</td>\n",
       "      <td>2.158388</td>\n",
       "      <td>-2.160262</td>\n",
       "      <td>-0.816436</td>\n",
       "      <td>-1.128342</td>\n",
       "      <td>1.477538</td>\n",
       "      <td>0.085750</td>\n",
       "      <td>0.332070</td>\n",
       "      <td>-0.931302</td>\n",
       "      <td>0.722037</td>\n",
       "      <td>-0.862200</td>\n",
       "      <td>0.412208</td>\n",
       "      <td>-0.445485</td>\n",
       "      <td>-0.149722</td>\n",
       "      <td>0.721634</td>\n",
       "      <td>0.376761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.321472</td>\n",
       "      <td>1.316559</td>\n",
       "      <td>1.100276</td>\n",
       "      <td>-0.240485</td>\n",
       "      <td>-1.374596</td>\n",
       "      <td>2.403613</td>\n",
       "      <td>-1.136448</td>\n",
       "      <td>-1.573823</td>\n",
       "      <td>-0.828518</td>\n",
       "      <td>1.327238</td>\n",
       "      <td>0.307597</td>\n",
       "      <td>-0.370367</td>\n",
       "      <td>-1.351926</td>\n",
       "      <td>0.578170</td>\n",
       "      <td>-0.393221</td>\n",
       "      <td>0.097569</td>\n",
       "      <td>-0.217631</td>\n",
       "      <td>-0.288023</td>\n",
       "      <td>0.759698</td>\n",
       "      <td>0.241880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.432616</td>\n",
       "      <td>1.645477</td>\n",
       "      <td>2.126718</td>\n",
       "      <td>-0.804346</td>\n",
       "      <td>-1.322060</td>\n",
       "      <td>3.067034</td>\n",
       "      <td>-1.201233</td>\n",
       "      <td>-0.821464</td>\n",
       "      <td>-0.529591</td>\n",
       "      <td>1.538776</td>\n",
       "      <td>-0.084633</td>\n",
       "      <td>0.413713</td>\n",
       "      <td>-1.465500</td>\n",
       "      <td>1.276663</td>\n",
       "      <td>-0.818454</td>\n",
       "      <td>-0.046595</td>\n",
       "      <td>0.336643</td>\n",
       "      <td>-0.417106</td>\n",
       "      <td>1.173832</td>\n",
       "      <td>-0.102437</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    1.754401                1.293660                1.400115   \n",
       "1                    1.417516                1.506793                1.654055   \n",
       "2                    2.060537                1.356492                1.489056   \n",
       "3                    4.321472                1.316559                1.100276   \n",
       "4                    3.432616                1.645477                2.126718   \n",
       "\n",
       "   lh_pericalcarine_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                   -1.478937     -1.847383      2.537129   \n",
       "1                   -0.614039     -1.384412      2.360426   \n",
       "2                   -1.055725     -1.174763      2.158388   \n",
       "3                   -0.240485     -1.374596      2.403613   \n",
       "4                   -0.804346     -1.322060      3.067034   \n",
       "\n",
       "   rh_caudalanteriorcingulate_thickness  CC_Posterior  Right-Caudate  \\\n",
       "0                             -1.131811     -0.411876      -0.365695   \n",
       "1                             -1.435565     -0.852958      -0.769807   \n",
       "2                             -2.160262     -0.816436      -1.128342   \n",
       "3                             -1.136448     -1.573823      -0.828518   \n",
       "4                             -1.201233     -0.821464      -0.529591   \n",
       "\n",
       "   lh_parahippocampal_thickness  MaskVol-to-eTIV  Brain-Stem  Left-vessel  \\\n",
       "0                      1.570148        -0.211262    0.181798    -1.580504   \n",
       "1                      1.730512         0.200279    0.374897    -1.464860   \n",
       "2                      1.477538         0.085750    0.332070    -0.931302   \n",
       "3                      1.327238         0.307597   -0.370367    -1.351926   \n",
       "4                      1.538776        -0.084633    0.413713    -1.465500   \n",
       "\n",
       "   Right-vessel  non-WM-hypointensities  rh_isthmuscingulate_thickness  \\\n",
       "0      0.608887               -0.451223                       0.448570   \n",
       "1      0.247169               -0.836050                       0.136242   \n",
       "2      0.722037               -0.862200                       0.412208   \n",
       "3      0.578170               -0.393221                       0.097569   \n",
       "4      1.276663               -0.818454                      -0.046595   \n",
       "\n",
       "   5th-Ventricle  non-WM-hypointensities  \\\n",
       "0       0.272656               -0.428067   \n",
       "1      -0.184904               -0.430852   \n",
       "2      -0.445485               -0.149722   \n",
       "3      -0.217631               -0.288023   \n",
       "4       0.336643               -0.417106   \n",
       "\n",
       "   lh_rostralanteriorcingulate_thickness  5th-Ventricle  \n",
       "0                               0.644490      -0.308911  \n",
       "1                               1.096066       0.046217  \n",
       "2                               0.721634       0.376761  \n",
       "3                               0.759698       0.241880  \n",
       "4                               1.173832      -0.102437  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data_feat)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data_feat.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "755d5dad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8863ba7f",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16899a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_46 (Dense)            (None, 32)                672       \n",
      "                                                                 \n",
      " activation_46 (Activation)  (None, 32)                0         \n",
      "                                                                 \n",
      " dense_47 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " activation_47 (Activation)  (None, 16)                0         \n",
      "                                                                 \n",
      " dense_48 (Dense)            (None, 8)                 136       \n",
      "                                                                 \n",
      " activation_48 (Activation)  (None, 8)                 0         \n",
      "                                                                 \n",
      " dense_49 (Dense)            (None, 4)                 36        \n",
      "                                                                 \n",
      " activation_49 (Activation)  (None, 4)                 0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 5         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,377\n",
      "Trainable params: 1,377\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "model.add(tf.keras.layers.Dense(32))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(16))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(4))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a541f",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f959a1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 6.5762 - msle: 6.5762 - val_loss: 2.4876 - val_msle: 2.4876\n",
      "Epoch 2/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 1.2193 - msle: 1.2193 - val_loss: 0.4532 - val_msle: 0.4532\n",
      "Epoch 3/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.3143 - msle: 0.3143 - val_loss: 0.2128 - val_msle: 0.2128\n",
      "Epoch 4/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.1826 - msle: 0.1826 - val_loss: 0.1570 - val_msle: 0.1570\n",
      "Epoch 5/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.1418 - msle: 0.1418 - val_loss: 0.1348 - val_msle: 0.1348\n",
      "Epoch 6/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.1234 - msle: 0.1234 - val_loss: 0.1227 - val_msle: 0.1227\n",
      "Epoch 7/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.1117 - msle: 0.1117 - val_loss: 0.1138 - val_msle: 0.1138\n",
      "Epoch 8/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.1026 - msle: 0.1026 - val_loss: 0.1060 - val_msle: 0.1060\n",
      "Epoch 9/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0950 - msle: 0.0950 - val_loss: 0.0994 - val_msle: 0.0994\n",
      "Epoch 10/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0885 - msle: 0.0885 - val_loss: 0.0935 - val_msle: 0.0935\n",
      "Epoch 11/200\n",
      "113/113 [==============================] - 1s 10ms/step - loss: 0.0829 - msle: 0.0829 - val_loss: 0.0884 - val_msle: 0.0884\n",
      "Epoch 12/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0781 - msle: 0.0781 - val_loss: 0.0840 - val_msle: 0.0840\n",
      "Epoch 13/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0741 - msle: 0.0741 - val_loss: 0.0798 - val_msle: 0.0798\n",
      "Epoch 14/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0705 - msle: 0.0705 - val_loss: 0.0766 - val_msle: 0.0766\n",
      "Epoch 15/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0673 - msle: 0.0673 - val_loss: 0.0737 - val_msle: 0.0737\n",
      "Epoch 16/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0643 - msle: 0.0643 - val_loss: 0.0710 - val_msle: 0.0710\n",
      "Epoch 17/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0617 - msle: 0.0617 - val_loss: 0.0687 - val_msle: 0.0687\n",
      "Epoch 18/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0595 - msle: 0.0595 - val_loss: 0.0667 - val_msle: 0.0667\n",
      "Epoch 19/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0573 - msle: 0.0573 - val_loss: 0.0645 - val_msle: 0.0645\n",
      "Epoch 20/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0554 - msle: 0.0554 - val_loss: 0.0626 - val_msle: 0.0626\n",
      "Epoch 21/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0536 - msle: 0.0536 - val_loss: 0.0615 - val_msle: 0.0615\n",
      "Epoch 22/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0520 - msle: 0.0520 - val_loss: 0.0599 - val_msle: 0.0599\n",
      "Epoch 23/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0504 - msle: 0.0504 - val_loss: 0.0579 - val_msle: 0.0579\n",
      "Epoch 24/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0489 - msle: 0.0489 - val_loss: 0.0569 - val_msle: 0.0569\n",
      "Epoch 25/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0475 - msle: 0.0475 - val_loss: 0.0548 - val_msle: 0.0548\n",
      "Epoch 26/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0462 - msle: 0.0462 - val_loss: 0.0537 - val_msle: 0.0537\n",
      "Epoch 27/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0447 - msle: 0.0447 - val_loss: 0.0522 - val_msle: 0.0522\n",
      "Epoch 28/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0436 - msle: 0.0436 - val_loss: 0.0513 - val_msle: 0.0513\n",
      "Epoch 29/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0421 - msle: 0.0421 - val_loss: 0.0502 - val_msle: 0.0502\n",
      "Epoch 30/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0411 - msle: 0.0411 - val_loss: 0.0493 - val_msle: 0.0493\n",
      "Epoch 31/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0399 - msle: 0.0399 - val_loss: 0.0480 - val_msle: 0.0480\n",
      "Epoch 32/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0390 - msle: 0.0390 - val_loss: 0.0469 - val_msle: 0.0469\n",
      "Epoch 33/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0379 - msle: 0.0379 - val_loss: 0.0464 - val_msle: 0.0464\n",
      "Epoch 34/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0370 - msle: 0.0370 - val_loss: 0.0456 - val_msle: 0.0456\n",
      "Epoch 35/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0361 - msle: 0.0361 - val_loss: 0.0451 - val_msle: 0.0451\n",
      "Epoch 36/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0349 - msle: 0.0349 - val_loss: 0.0442 - val_msle: 0.0442\n",
      "Epoch 37/200\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 0.0343 - msle: 0.0343 - val_loss: 0.0430 - val_msle: 0.0430\n",
      "Epoch 38/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0333 - msle: 0.0333 - val_loss: 0.0422 - val_msle: 0.0422\n",
      "Epoch 39/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0326 - msle: 0.0326 - val_loss: 0.0415 - val_msle: 0.0415\n",
      "Epoch 40/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0320 - msle: 0.0320 - val_loss: 0.0405 - val_msle: 0.0405\n",
      "Epoch 41/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0313 - msle: 0.0313 - val_loss: 0.0408 - val_msle: 0.0408\n",
      "Epoch 42/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0310 - msle: 0.0310 - val_loss: 0.0398 - val_msle: 0.0398\n",
      "Epoch 43/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0301 - msle: 0.0301 - val_loss: 0.0388 - val_msle: 0.0388\n",
      "Epoch 44/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0299 - msle: 0.0299 - val_loss: 0.0403 - val_msle: 0.0403\n",
      "Epoch 45/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0296 - msle: 0.0296 - val_loss: 0.0387 - val_msle: 0.0387\n",
      "Epoch 46/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0289 - msle: 0.0289 - val_loss: 0.0382 - val_msle: 0.0382\n",
      "Epoch 47/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0284 - msle: 0.0284 - val_loss: 0.0377 - val_msle: 0.0377\n",
      "Epoch 48/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0281 - msle: 0.0281 - val_loss: 0.0369 - val_msle: 0.0369\n",
      "Epoch 49/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0280 - msle: 0.0280 - val_loss: 0.0362 - val_msle: 0.0362\n",
      "Epoch 50/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0276 - msle: 0.0276 - val_loss: 0.0360 - val_msle: 0.0360\n",
      "Epoch 51/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0271 - msle: 0.0271 - val_loss: 0.0353 - val_msle: 0.0353\n",
      "Epoch 52/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0271 - msle: 0.0271 - val_loss: 0.0360 - val_msle: 0.0360\n",
      "Epoch 53/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0267 - msle: 0.0267 - val_loss: 0.0359 - val_msle: 0.0359\n",
      "Epoch 54/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0263 - msle: 0.0263 - val_loss: 0.0349 - val_msle: 0.0349\n",
      "Epoch 55/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0261 - msle: 0.0261 - val_loss: 0.0358 - val_msle: 0.0358\n",
      "Epoch 56/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0258 - msle: 0.0258 - val_loss: 0.0354 - val_msle: 0.0354\n",
      "Epoch 57/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0257 - msle: 0.0257 - val_loss: 0.0348 - val_msle: 0.0348\n",
      "Epoch 58/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0254 - msle: 0.0254 - val_loss: 0.0338 - val_msle: 0.0338\n",
      "Epoch 59/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0254 - msle: 0.0254 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 60/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0249 - msle: 0.0249 - val_loss: 0.0337 - val_msle: 0.0337\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0252 - msle: 0.0252 - val_loss: 0.0338 - val_msle: 0.0338\n",
      "Epoch 62/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0248 - msle: 0.0248 - val_loss: 0.0349 - val_msle: 0.0349\n",
      "Epoch 63/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0246 - msle: 0.0246 - val_loss: 0.0335 - val_msle: 0.0335\n",
      "Epoch 64/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0245 - msle: 0.0245 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 65/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0244 - msle: 0.0244 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 66/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0241 - msle: 0.0241 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 67/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0241 - msle: 0.0241 - val_loss: 0.0332 - val_msle: 0.0332\n",
      "Epoch 68/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0241 - msle: 0.0241 - val_loss: 0.0325 - val_msle: 0.0325\n",
      "Epoch 69/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0237 - msle: 0.0237 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 70/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0236 - msle: 0.0236 - val_loss: 0.0329 - val_msle: 0.0329\n",
      "Epoch 71/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0234 - msle: 0.0234 - val_loss: 0.0323 - val_msle: 0.0323\n",
      "Epoch 72/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0232 - msle: 0.0232 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 73/200\n",
      "113/113 [==============================] - 1s 9ms/step - loss: 0.0230 - msle: 0.0230 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 74/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0230 - msle: 0.0230 - val_loss: 0.0329 - val_msle: 0.0329\n",
      "Epoch 75/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0228 - msle: 0.0228 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 76/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0225 - msle: 0.0225 - val_loss: 0.0330 - val_msle: 0.0330\n",
      "Epoch 77/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0226 - msle: 0.0226 - val_loss: 0.0335 - val_msle: 0.0335\n",
      "Epoch 78/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0224 - msle: 0.0224 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 79/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0221 - msle: 0.0221 - val_loss: 0.0327 - val_msle: 0.0327\n",
      "Epoch 80/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0223 - msle: 0.0223 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 81/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0219 - msle: 0.0219 - val_loss: 0.0331 - val_msle: 0.0331\n",
      "Epoch 82/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0216 - msle: 0.0216 - val_loss: 0.0320 - val_msle: 0.0320\n",
      "Epoch 83/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0216 - msle: 0.0216 - val_loss: 0.0318 - val_msle: 0.0318\n",
      "Epoch 84/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0214 - msle: 0.0214 - val_loss: 0.0315 - val_msle: 0.0315\n",
      "Epoch 85/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0216 - msle: 0.0216 - val_loss: 0.0322 - val_msle: 0.0322\n",
      "Epoch 86/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0213 - msle: 0.0213 - val_loss: 0.0325 - val_msle: 0.0325\n",
      "Epoch 87/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0211 - msle: 0.0211 - val_loss: 0.0317 - val_msle: 0.0317\n",
      "Epoch 88/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0207 - msle: 0.0207 - val_loss: 0.0314 - val_msle: 0.0314\n",
      "Epoch 89/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0208 - msle: 0.0208 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 90/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0209 - msle: 0.0209 - val_loss: 0.0328 - val_msle: 0.0328\n",
      "Epoch 91/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0205 - msle: 0.0205 - val_loss: 0.0326 - val_msle: 0.0326\n",
      "Epoch 92/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0202 - msle: 0.0202 - val_loss: 0.0320 - val_msle: 0.0320\n",
      "Epoch 93/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0206 - msle: 0.0206 - val_loss: 0.0331 - val_msle: 0.0331\n",
      "Epoch 94/200\n",
      "113/113 [==============================] - 1s 7ms/step - loss: 0.0204 - msle: 0.0204 - val_loss: 0.0315 - val_msle: 0.0315\n",
      "Epoch 95/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0200 - msle: 0.0200 - val_loss: 0.0329 - val_msle: 0.0329\n",
      "Epoch 96/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0201 - msle: 0.0201 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 97/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0199 - msle: 0.0199 - val_loss: 0.0325 - val_msle: 0.0325\n",
      "Epoch 98/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0198 - msle: 0.0198 - val_loss: 0.0331 - val_msle: 0.0331\n",
      "Epoch 99/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0199 - msle: 0.0199 - val_loss: 0.0326 - val_msle: 0.0326\n",
      "Epoch 100/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0198 - msle: 0.0198 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 101/200\n",
      "113/113 [==============================] - 1s 6ms/step - loss: 0.0194 - msle: 0.0194 - val_loss: 0.0331 - val_msle: 0.0331\n",
      "Epoch 102/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0197 - msle: 0.0197 - val_loss: 0.0326 - val_msle: 0.0326\n",
      "Epoch 103/200\n",
      "113/113 [==============================] - 1s 5ms/step - loss: 0.0195 - msle: 0.0195 - val_loss: 0.0323 - val_msle: 0.0323\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=15, restore_best_weights = True)\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=200, \n",
    "    batch_size=30,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9b1a71",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a26a1d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8717352232991527\n",
      "Max Error: 21.55675506591797\n",
      "Mean absolute error: 5.381495745676868\n",
      "Mean squared error: 51.230245823366204\n",
      "Root Mean squared error: 7.157530707120033\n",
      "R2: 0.8693401733894216\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4771e649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(102))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='train_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
