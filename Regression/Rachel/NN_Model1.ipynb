{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1934156d",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "1. Introduction\n",
    "2. Import\n",
    "3. Analysis & Preprocessing\n",
    "4. Model\n",
    "5. Training\n",
    "6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e831b",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "References:\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4bdb66",
   "metadata": {},
   "source": [
    "# 2. Import\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "977f6ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d0fd1a",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ac8e7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de215e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       2.116693                1.364192   \n",
      "1                       1.781763                1.577277   \n",
      "2                       2.423064                1.424486   \n",
      "3                       4.657487                1.366377   \n",
      "4                       3.795704                1.701514   \n",
      "...                          ...                     ...   \n",
      "4221                    3.332053                2.220375   \n",
      "4222                    4.258130               -2.535943   \n",
      "4223                    7.826457                2.169780   \n",
      "4224                   -0.702317                2.439428   \n",
      "4225                   -2.373678               -3.566133   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_lateralorbitofrontal_thickness  SurfaceHoles  \\\n",
      "0                   1.509690                          -2.003048     -1.881062   \n",
      "1                   1.751915                          -1.118768     -1.487368   \n",
      "2                   1.583744                          -1.542772     -1.247103   \n",
      "3                   1.174566                          -0.638117     -1.459437   \n",
      "4                   2.226951                          -1.243184     -1.390031   \n",
      "...                      ...                                ...           ...   \n",
      "4221                0.517985                           1.504561      0.766545   \n",
      "4222                1.742539                          -2.390150      1.916746   \n",
      "4223                3.996040                          -1.862993      1.557271   \n",
      "4224                7.148797                           0.141173      2.689027   \n",
      "4225                2.435460                          -2.369722      1.958758   \n",
      "\n",
      "      CC_Posterior  rh_entorhinal_thickness  CC_Posterior  Right-Caudate  \\\n",
      "0         2.275291                -1.646929     -0.014603      -0.455127   \n",
      "1         2.076278                -1.813623     -0.372086      -0.911972   \n",
      "2         1.776939                -2.457333     -0.636613      -1.255602   \n",
      "3         2.255052                -1.235504     -0.942565      -1.100492   \n",
      "4         2.823588                -1.682617     -0.284929      -0.686542   \n",
      "...            ...                      ...           ...            ...   \n",
      "4221     -0.083866                 0.282199     -2.290109      -0.592740   \n",
      "4222      0.215448                 0.949526     -2.436899       0.593719   \n",
      "4223      2.809830                 1.455756      0.581448       1.829448   \n",
      "4224      5.142553                 1.877965      0.907111       4.585551   \n",
      "4225      2.429117                 1.591531     -0.542403       2.209285   \n",
      "\n",
      "      MaskVol-to-eTIV  rh_frontalpole_thickness  MaskVol-to-eTIV  \\\n",
      "0            1.796729                 -0.679393         0.104424   \n",
      "1            1.974108                 -0.611399         0.555653   \n",
      "2            1.762072                 -0.733668         0.541905   \n",
      "3            1.570616                 -0.537640         0.040204   \n",
      "4            1.793880                 -0.838911         0.428843   \n",
      "...               ...                       ...              ...   \n",
      "4221        -0.868444                  0.572612        -0.481707   \n",
      "4222        -0.287017                  0.677901        -1.580592   \n",
      "4223        -0.114406                  3.722192        -0.564620   \n",
      "4224         1.198714                  0.803463         1.881741   \n",
      "4225         0.196188                 -0.231549        -0.990961   \n",
      "\n",
      "      Right-Cerebellum-White-Matter  MaskVol-to-eTIV  Left-vessel  \\\n",
      "0                         -1.141063         1.573431    -1.159093   \n",
      "1                         -0.934538         1.449886    -1.570098   \n",
      "2                         -0.497774         1.726920    -1.110241   \n",
      "3                         -0.533427         1.805882    -1.553970   \n",
      "4                         -1.060138         2.051861    -0.761959   \n",
      "...                             ...              ...          ...   \n",
      "4221                       0.860651        -1.178114     2.152890   \n",
      "4222                      -0.367443        -1.315944     1.122815   \n",
      "4223                      -0.829976        -2.448380     0.975432   \n",
      "4224                      -0.138087        -2.636348     1.171807   \n",
      "4225                      -0.856451        -1.601113     0.298548   \n",
      "\n",
      "      non-WM-hypointensities  rh_isthmuscingulate_thickness  5th-Ventricle  \\\n",
      "0                  -0.417015                       0.090793      -0.937496   \n",
      "1                  -0.800062                      -0.422113      -0.518682   \n",
      "2                  -0.868901                      -0.211182      -0.183662   \n",
      "3                  -0.246886                      -0.408339      -0.493771   \n",
      "4                  -0.630384                      -0.543936      -0.964464   \n",
      "...                      ...                            ...            ...   \n",
      "4221               -0.263818                       0.517526       1.783929   \n",
      "4222               -1.055350                       0.738712      -0.542306   \n",
      "4223               -0.028346                       1.308332      -0.182910   \n",
      "4224               -1.473929                       1.492655      -0.086108   \n",
      "4225               -1.024181                       2.461982      -0.867859   \n",
      "\n",
      "      non-WM-hypointensities  Left-vessel  \n",
      "0                   0.261800     0.078680  \n",
      "1                   0.396438     0.258847  \n",
      "2                   0.325125    -0.164866  \n",
      "3                   0.399181    -0.184321  \n",
      "4                   0.468170     0.363352  \n",
      "...                      ...          ...  \n",
      "4221               -0.689697     1.077454  \n",
      "4222                0.421468     2.162383  \n",
      "4223                0.692507     0.689188  \n",
      "4224                0.731891     0.979252  \n",
      "4225                1.186197     0.376996  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_lateralorbitofrontal_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_entorhinal_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-Cerebellum-White-Matter</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>Left-vessel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.116693</td>\n",
       "      <td>1.364192</td>\n",
       "      <td>1.509690</td>\n",
       "      <td>-2.003048</td>\n",
       "      <td>-1.881062</td>\n",
       "      <td>2.275291</td>\n",
       "      <td>-1.646929</td>\n",
       "      <td>-0.014603</td>\n",
       "      <td>-0.455127</td>\n",
       "      <td>1.796729</td>\n",
       "      <td>-0.679393</td>\n",
       "      <td>0.104424</td>\n",
       "      <td>-1.141063</td>\n",
       "      <td>1.573431</td>\n",
       "      <td>-1.159093</td>\n",
       "      <td>-0.417015</td>\n",
       "      <td>0.090793</td>\n",
       "      <td>-0.937496</td>\n",
       "      <td>0.261800</td>\n",
       "      <td>0.078680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.781763</td>\n",
       "      <td>1.577277</td>\n",
       "      <td>1.751915</td>\n",
       "      <td>-1.118768</td>\n",
       "      <td>-1.487368</td>\n",
       "      <td>2.076278</td>\n",
       "      <td>-1.813623</td>\n",
       "      <td>-0.372086</td>\n",
       "      <td>-0.911972</td>\n",
       "      <td>1.974108</td>\n",
       "      <td>-0.611399</td>\n",
       "      <td>0.555653</td>\n",
       "      <td>-0.934538</td>\n",
       "      <td>1.449886</td>\n",
       "      <td>-1.570098</td>\n",
       "      <td>-0.800062</td>\n",
       "      <td>-0.422113</td>\n",
       "      <td>-0.518682</td>\n",
       "      <td>0.396438</td>\n",
       "      <td>0.258847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.423064</td>\n",
       "      <td>1.424486</td>\n",
       "      <td>1.583744</td>\n",
       "      <td>-1.542772</td>\n",
       "      <td>-1.247103</td>\n",
       "      <td>1.776939</td>\n",
       "      <td>-2.457333</td>\n",
       "      <td>-0.636613</td>\n",
       "      <td>-1.255602</td>\n",
       "      <td>1.762072</td>\n",
       "      <td>-0.733668</td>\n",
       "      <td>0.541905</td>\n",
       "      <td>-0.497774</td>\n",
       "      <td>1.726920</td>\n",
       "      <td>-1.110241</td>\n",
       "      <td>-0.868901</td>\n",
       "      <td>-0.211182</td>\n",
       "      <td>-0.183662</td>\n",
       "      <td>0.325125</td>\n",
       "      <td>-0.164866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.657487</td>\n",
       "      <td>1.366377</td>\n",
       "      <td>1.174566</td>\n",
       "      <td>-0.638117</td>\n",
       "      <td>-1.459437</td>\n",
       "      <td>2.255052</td>\n",
       "      <td>-1.235504</td>\n",
       "      <td>-0.942565</td>\n",
       "      <td>-1.100492</td>\n",
       "      <td>1.570616</td>\n",
       "      <td>-0.537640</td>\n",
       "      <td>0.040204</td>\n",
       "      <td>-0.533427</td>\n",
       "      <td>1.805882</td>\n",
       "      <td>-1.553970</td>\n",
       "      <td>-0.246886</td>\n",
       "      <td>-0.408339</td>\n",
       "      <td>-0.493771</td>\n",
       "      <td>0.399181</td>\n",
       "      <td>-0.184321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.795704</td>\n",
       "      <td>1.701514</td>\n",
       "      <td>2.226951</td>\n",
       "      <td>-1.243184</td>\n",
       "      <td>-1.390031</td>\n",
       "      <td>2.823588</td>\n",
       "      <td>-1.682617</td>\n",
       "      <td>-0.284929</td>\n",
       "      <td>-0.686542</td>\n",
       "      <td>1.793880</td>\n",
       "      <td>-0.838911</td>\n",
       "      <td>0.428843</td>\n",
       "      <td>-1.060138</td>\n",
       "      <td>2.051861</td>\n",
       "      <td>-0.761959</td>\n",
       "      <td>-0.630384</td>\n",
       "      <td>-0.543936</td>\n",
       "      <td>-0.964464</td>\n",
       "      <td>0.468170</td>\n",
       "      <td>0.363352</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    2.116693                1.364192                1.509690   \n",
       "1                    1.781763                1.577277                1.751915   \n",
       "2                    2.423064                1.424486                1.583744   \n",
       "3                    4.657487                1.366377                1.174566   \n",
       "4                    3.795704                1.701514                2.226951   \n",
       "\n",
       "   lh_lateralorbitofrontal_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                          -2.003048     -1.881062      2.275291   \n",
       "1                          -1.118768     -1.487368      2.076278   \n",
       "2                          -1.542772     -1.247103      1.776939   \n",
       "3                          -0.638117     -1.459437      2.255052   \n",
       "4                          -1.243184     -1.390031      2.823588   \n",
       "\n",
       "   rh_entorhinal_thickness  CC_Posterior  Right-Caudate  MaskVol-to-eTIV  \\\n",
       "0                -1.646929     -0.014603      -0.455127         1.796729   \n",
       "1                -1.813623     -0.372086      -0.911972         1.974108   \n",
       "2                -2.457333     -0.636613      -1.255602         1.762072   \n",
       "3                -1.235504     -0.942565      -1.100492         1.570616   \n",
       "4                -1.682617     -0.284929      -0.686542         1.793880   \n",
       "\n",
       "   rh_frontalpole_thickness  MaskVol-to-eTIV  Right-Cerebellum-White-Matter  \\\n",
       "0                 -0.679393         0.104424                      -1.141063   \n",
       "1                 -0.611399         0.555653                      -0.934538   \n",
       "2                 -0.733668         0.541905                      -0.497774   \n",
       "3                 -0.537640         0.040204                      -0.533427   \n",
       "4                 -0.838911         0.428843                      -1.060138   \n",
       "\n",
       "   MaskVol-to-eTIV  Left-vessel  non-WM-hypointensities  \\\n",
       "0         1.573431    -1.159093               -0.417015   \n",
       "1         1.449886    -1.570098               -0.800062   \n",
       "2         1.726920    -1.110241               -0.868901   \n",
       "3         1.805882    -1.553970               -0.246886   \n",
       "4         2.051861    -0.761959               -0.630384   \n",
       "\n",
       "   rh_isthmuscingulate_thickness  5th-Ventricle  non-WM-hypointensities  \\\n",
       "0                       0.090793      -0.937496                0.261800   \n",
       "1                      -0.422113      -0.518682                0.396438   \n",
       "2                      -0.211182      -0.183662                0.325125   \n",
       "3                      -0.408339      -0.493771                0.399181   \n",
       "4                      -0.543936      -0.964464                0.468170   \n",
       "\n",
       "   Left-vessel  \n",
       "0     0.078680  \n",
       "1     0.258847  \n",
       "2    -0.164866  \n",
       "3    -0.184321  \n",
       "4     0.363352  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fae3fe00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65e88fe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 15:24:29.373455: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-04-09 15:24:29.373633: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  (None, 20)               80        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                336       \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 16)               64        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 8)                 136       \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 8)                32        \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 8)                 0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 9         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657\n",
      "Trainable params: 569\n",
      "Non-trainable params: 88\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dense(16))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(8))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "186eaa56",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2959908b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 15:24:30.044180: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-04-09 15:24:30.830345: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - ETA: 0s - loss: 13.9909 - msle: 13.9909"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 15:24:35.221492: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 6s 42ms/step - loss: 13.9909 - msle: 13.9909 - val_loss: 13.9816 - val_msle: 13.9816\n",
      "Epoch 2/100\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 11.6265 - msle: 11.6265 - val_loss: 11.8713 - val_msle: 11.8713\n",
      "Epoch 3/100\n",
      "106/106 [==============================] - 4s 39ms/step - loss: 10.0943 - msle: 10.0943 - val_loss: 9.7664 - val_msle: 9.7664\n",
      "Epoch 4/100\n",
      "106/106 [==============================] - 5s 43ms/step - loss: 8.7186 - msle: 8.7186 - val_loss: 7.8928 - val_msle: 7.8928\n",
      "Epoch 5/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 7.3151 - msle: 7.3151 - val_loss: 6.3580 - val_msle: 6.3580\n",
      "Epoch 6/100\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 6.2327 - msle: 6.2327 - val_loss: 5.7294 - val_msle: 5.7294\n",
      "Epoch 7/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 5.5081 - msle: 5.5081 - val_loss: 5.1801 - val_msle: 5.1801\n",
      "Epoch 8/100\n",
      "106/106 [==============================] - 5s 45ms/step - loss: 4.9268 - msle: 4.9268 - val_loss: 4.7204 - val_msle: 4.7204\n",
      "Epoch 9/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 4.4392 - msle: 4.4392 - val_loss: 4.2689 - val_msle: 4.2689\n",
      "Epoch 10/100\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 4.0098 - msle: 4.0098 - val_loss: 3.8377 - val_msle: 3.8377\n",
      "Epoch 11/100\n",
      "106/106 [==============================] - 4s 38ms/step - loss: 3.6286 - msle: 3.6286 - val_loss: 3.4544 - val_msle: 3.4544\n",
      "Epoch 12/100\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 3.2837 - msle: 3.2837 - val_loss: 3.1342 - val_msle: 3.1342\n",
      "Epoch 13/100\n",
      "106/106 [==============================] - 4s 40ms/step - loss: 2.9798 - msle: 2.9798 - val_loss: 2.8245 - val_msle: 2.8245\n",
      "Epoch 14/100\n",
      "106/106 [==============================] - 4s 41ms/step - loss: 2.7088 - msle: 2.7088 - val_loss: 2.5691 - val_msle: 2.5691\n",
      "Epoch 15/100\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 2.4599 - msle: 2.4599 - val_loss: 2.3213 - val_msle: 2.3213\n",
      "Epoch 16/100\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 2.2439 - msle: 2.2439 - val_loss: 2.1059 - val_msle: 2.1059\n",
      "Epoch 17/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 2.0438 - msle: 2.0438 - val_loss: 1.9272 - val_msle: 1.9272\n",
      "Epoch 18/100\n",
      "106/106 [==============================] - 5s 46ms/step - loss: 1.8655 - msle: 1.8655 - val_loss: 1.7553 - val_msle: 1.7553\n",
      "Epoch 19/100\n",
      "106/106 [==============================] - 4s 40ms/step - loss: 1.7001 - msle: 1.7001 - val_loss: 1.5980 - val_msle: 1.5980\n",
      "Epoch 20/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 1.5513 - msle: 1.5513 - val_loss: 1.4510 - val_msle: 1.4510\n",
      "Epoch 21/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 1.4156 - msle: 1.4156 - val_loss: 1.3190 - val_msle: 1.3190\n",
      "Epoch 22/100\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 1.2926 - msle: 1.2926 - val_loss: 1.2054 - val_msle: 1.2054\n",
      "Epoch 23/100\n",
      "106/106 [==============================] - 4s 33ms/step - loss: 1.1827 - msle: 1.1827 - val_loss: 1.1058 - val_msle: 1.1058\n",
      "Epoch 24/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 1.0801 - msle: 1.0801 - val_loss: 1.0135 - val_msle: 1.0135\n",
      "Epoch 25/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.9848 - msle: 0.9848 - val_loss: 0.9210 - val_msle: 0.9210\n",
      "Epoch 26/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.8997 - msle: 0.8997 - val_loss: 0.8447 - val_msle: 0.8447\n",
      "Epoch 27/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 0.8242 - msle: 0.8242 - val_loss: 0.7691 - val_msle: 0.7691\n",
      "Epoch 28/100\n",
      "106/106 [==============================] - 4s 35ms/step - loss: 0.7527 - msle: 0.7527 - val_loss: 0.6998 - val_msle: 0.6998\n",
      "Epoch 29/100\n",
      "106/106 [==============================] - 4s 36ms/step - loss: 0.6862 - msle: 0.6862 - val_loss: 0.6364 - val_msle: 0.6364\n",
      "Epoch 30/100\n",
      "106/106 [==============================] - 4s 37ms/step - loss: 0.6286 - msle: 0.6286 - val_loss: 0.5833 - val_msle: 0.5833\n",
      "Epoch 31/100\n",
      "106/106 [==============================] - 3s 33ms/step - loss: 0.5731 - msle: 0.5731 - val_loss: 0.5383 - val_msle: 0.5383\n",
      "Epoch 32/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.5237 - msle: 0.5237 - val_loss: 0.4873 - val_msle: 0.4873\n",
      "Epoch 33/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.4766 - msle: 0.4766 - val_loss: 0.4420 - val_msle: 0.4420\n",
      "Epoch 34/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.4351 - msle: 0.4351 - val_loss: 0.4003 - val_msle: 0.4003\n",
      "Epoch 35/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.3973 - msle: 0.3973 - val_loss: 0.3629 - val_msle: 0.3629\n",
      "Epoch 36/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.3625 - msle: 0.3625 - val_loss: 0.3304 - val_msle: 0.3304\n",
      "Epoch 37/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.3301 - msle: 0.3301 - val_loss: 0.2984 - val_msle: 0.2984\n",
      "Epoch 38/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.2968 - msle: 0.2968 - val_loss: 0.2723 - val_msle: 0.2723\n",
      "Epoch 39/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.2736 - msle: 0.2736 - val_loss: 0.2425 - val_msle: 0.2425\n",
      "Epoch 40/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.2473 - msle: 0.2473 - val_loss: 0.2212 - val_msle: 0.2212\n",
      "Epoch 41/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.2246 - msle: 0.2246 - val_loss: 0.2020 - val_msle: 0.2020\n",
      "Epoch 42/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.2032 - msle: 0.2032 - val_loss: 0.1822 - val_msle: 0.1822\n",
      "Epoch 43/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.1838 - msle: 0.1838 - val_loss: 0.1666 - val_msle: 0.1666\n",
      "Epoch 44/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.1678 - msle: 0.1678 - val_loss: 0.1467 - val_msle: 0.1467\n",
      "Epoch 45/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.1508 - msle: 0.1508 - val_loss: 0.1350 - val_msle: 0.1350\n",
      "Epoch 46/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.1369 - msle: 0.1369 - val_loss: 0.1206 - val_msle: 0.1206\n",
      "Epoch 47/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.1276 - msle: 0.1276 - val_loss: 0.1120 - val_msle: 0.1120\n",
      "Epoch 48/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.1147 - msle: 0.1147 - val_loss: 0.1014 - val_msle: 0.1014\n",
      "Epoch 49/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.1033 - msle: 0.1033 - val_loss: 0.0925 - val_msle: 0.0925\n",
      "Epoch 50/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.0946 - msle: 0.0946 - val_loss: 0.0838 - val_msle: 0.0838\n",
      "Epoch 51/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0877 - msle: 0.0877 - val_loss: 0.0770 - val_msle: 0.0770\n",
      "Epoch 52/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.0807 - msle: 0.0807 - val_loss: 0.0704 - val_msle: 0.0704\n",
      "Epoch 53/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0735 - msle: 0.0735 - val_loss: 0.0656 - val_msle: 0.0656\n",
      "Epoch 54/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0686 - msle: 0.0686 - val_loss: 0.0599 - val_msle: 0.0599\n",
      "Epoch 55/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0620 - msle: 0.0620 - val_loss: 0.0537 - val_msle: 0.0537\n",
      "Epoch 56/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.0586 - msle: 0.0586 - val_loss: 0.0500 - val_msle: 0.0500\n",
      "Epoch 57/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.0550 - msle: 0.0550 - val_loss: 0.0464 - val_msle: 0.0464\n",
      "Epoch 58/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0516 - msle: 0.0516 - val_loss: 0.0434 - val_msle: 0.0434\n",
      "Epoch 59/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0461 - msle: 0.0461 - val_loss: 0.0405 - val_msle: 0.0405\n",
      "Epoch 60/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0457 - msle: 0.0457 - val_loss: 0.0379 - val_msle: 0.0379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.0437 - msle: 0.0437 - val_loss: 0.0358 - val_msle: 0.0358\n",
      "Epoch 62/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.0391 - msle: 0.0391 - val_loss: 0.0346 - val_msle: 0.0346\n",
      "Epoch 63/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.0377 - msle: 0.0377 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 64/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.0374 - msle: 0.0374 - val_loss: 0.0327 - val_msle: 0.0327\n",
      "Epoch 65/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.0360 - msle: 0.0360 - val_loss: 0.0308 - val_msle: 0.0308\n",
      "Epoch 66/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0363 - msle: 0.0363 - val_loss: 0.0294 - val_msle: 0.0294\n",
      "Epoch 67/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0335 - msle: 0.0335 - val_loss: 0.0290 - val_msle: 0.0290\n",
      "Epoch 68/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.0318 - msle: 0.0318 - val_loss: 0.0282 - val_msle: 0.0282\n",
      "Epoch 69/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.0322 - msle: 0.0322 - val_loss: 0.0281 - val_msle: 0.0281\n",
      "Epoch 70/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0323 - msle: 0.0323 - val_loss: 0.0273 - val_msle: 0.0273\n",
      "Epoch 71/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.0311 - msle: 0.0311 - val_loss: 0.0271 - val_msle: 0.0271\n",
      "Epoch 72/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0321 - msle: 0.0321 - val_loss: 0.0276 - val_msle: 0.0276\n",
      "Epoch 73/100\n",
      "106/106 [==============================] - 3s 32ms/step - loss: 0.0319 - msle: 0.0319 - val_loss: 0.0266 - val_msle: 0.0266\n",
      "Epoch 74/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0280 - msle: 0.0280 - val_loss: 0.0269 - val_msle: 0.0269\n",
      "Epoch 75/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.0316 - msle: 0.0316 - val_loss: 0.0258 - val_msle: 0.0258\n",
      "Epoch 76/100\n",
      "106/106 [==============================] - 3s 26ms/step - loss: 0.0296 - msle: 0.0296 - val_loss: 0.0256 - val_msle: 0.0256\n",
      "Epoch 77/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.0280 - msle: 0.0280 - val_loss: 0.0259 - val_msle: 0.0259\n",
      "Epoch 78/100\n",
      "106/106 [==============================] - 3s 25ms/step - loss: 0.0299 - msle: 0.0299 - val_loss: 0.0261 - val_msle: 0.0261\n",
      "Epoch 79/100\n",
      "106/106 [==============================] - 3s 24ms/step - loss: 0.0296 - msle: 0.0296 - val_loss: 0.0248 - val_msle: 0.0248\n",
      "Epoch 80/100\n",
      "106/106 [==============================] - 3s 28ms/step - loss: 0.0291 - msle: 0.0291 - val_loss: 0.0245 - val_msle: 0.0245\n",
      "Epoch 81/100\n",
      "106/106 [==============================] - 3s 27ms/step - loss: 0.0283 - msle: 0.0283 - val_loss: 0.0258 - val_msle: 0.0258\n",
      "Epoch 82/100\n",
      "106/106 [==============================] - 3s 31ms/step - loss: 0.0279 - msle: 0.0279 - val_loss: 0.0252 - val_msle: 0.0252\n",
      "Epoch 83/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0277 - msle: 0.0277 - val_loss: 0.0248 - val_msle: 0.0248\n",
      "Epoch 84/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0277 - msle: 0.0277 - val_loss: 0.0245 - val_msle: 0.0245\n",
      "Epoch 85/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0265 - msle: 0.0265 - val_loss: 0.0247 - val_msle: 0.0247\n",
      "Epoch 86/100\n",
      "106/106 [==============================] - 4s 34ms/step - loss: 0.0273 - msle: 0.0273 - val_loss: 0.0247 - val_msle: 0.0247\n",
      "Epoch 87/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.0286 - msle: 0.0286 - val_loss: 0.0247 - val_msle: 0.0247\n",
      "Epoch 88/100\n",
      "106/106 [==============================] - 2s 23ms/step - loss: 0.0286 - msle: 0.0286 - val_loss: 0.0250 - val_msle: 0.0250\n",
      "Epoch 89/100\n",
      "106/106 [==============================] - 3s 30ms/step - loss: 0.0267 - msle: 0.0267 - val_loss: 0.0247 - val_msle: 0.0247\n",
      "Epoch 90/100\n",
      "106/106 [==============================] - 3s 29ms/step - loss: 0.0267 - msle: 0.0267 - val_loss: 0.0250 - val_msle: 0.0250\n"
     ]
    }
   ],
   "source": [
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=32,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3ce0627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8591236303319296\n",
      "Max Error: 21.710084915161133\n",
      "Mean absolute error: 5.96589498699836\n",
      "Mean squared error: 58.058176847947045\n",
      "Root Mean squared error: 7.619591645747628\n",
      "R2: 0.8519259238686079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-09 15:29:26.721073: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1005ef2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(90))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='train_loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc9930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
