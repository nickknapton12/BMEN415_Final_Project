{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea11364",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1823af",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75a2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069962",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05d81db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5353dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       2.116693                1.364191   \n",
      "1                       1.781763                1.577276   \n",
      "2                       2.423065                1.424486   \n",
      "3                       4.657487                1.366376   \n",
      "4                       3.795704                1.701513   \n",
      "...                          ...                     ...   \n",
      "4221                    3.332053                2.220377   \n",
      "4222                    4.258130               -2.535944   \n",
      "4223                    7.826457                2.169780   \n",
      "4224                   -0.702316                2.439424   \n",
      "4225                   -2.373678               -3.566135   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_lateralorbitofrontal_thickness  SurfaceHoles  \\\n",
      "0                   1.509738                          -2.002542     -1.880681   \n",
      "1                   1.751879                          -1.118448     -1.486195   \n",
      "2                   1.583671                          -1.542656     -1.246317   \n",
      "3                   1.174550                          -0.637088     -1.459873   \n",
      "4                   2.226951                          -1.242501     -1.388356   \n",
      "...                      ...                                ...           ...   \n",
      "4221                0.517971                           1.504663      0.767874   \n",
      "4222                1.742639                          -2.389918      1.916256   \n",
      "4223                3.996086                          -1.861737      1.557014   \n",
      "4224                7.148702                           0.139656      2.690266   \n",
      "4225                2.435552                          -2.369988      1.958036   \n",
      "\n",
      "      CC_Posterior  rh_entorhinal_thickness  CC_Posterior  Right-Caudate  \\\n",
      "0         2.278202                -1.648855     -0.011644      -0.458969   \n",
      "1         2.075799                -1.814979     -0.369271      -0.918797   \n",
      "2         1.772968                -2.457122     -0.634560      -1.260260   \n",
      "3         2.255091                -1.240476     -0.939504      -1.082391   \n",
      "4         2.820242                -1.689395     -0.277617      -0.695485   \n",
      "...            ...                      ...           ...            ...   \n",
      "4221     -0.090273                 0.276253     -2.287790      -0.620017   \n",
      "4222      0.224896                 0.949633     -2.436397       0.600692   \n",
      "4223      2.801791                 1.434244      0.585143       1.839752   \n",
      "4224      5.135757                 1.901716      0.901710       4.569846   \n",
      "4225      2.430738                 1.590827     -0.544596       2.189233   \n",
      "\n",
      "      MaskVol-to-eTIV  rh_frontalpole_thickness  MaskVol-to-eTIV  \\\n",
      "0            1.810743                 -0.668529         0.109944   \n",
      "1            1.998003                 -0.610768         0.505767   \n",
      "2            1.776340                 -0.738290         0.492492   \n",
      "3            1.561048                 -0.528174         0.028773   \n",
      "4            1.820614                 -0.828457         0.417255   \n",
      "...               ...                       ...              ...   \n",
      "4221        -0.851313                  0.574070        -0.409632   \n",
      "4222        -0.289168                  0.708990        -1.480759   \n",
      "4223        -0.119359                  3.734455        -0.349164   \n",
      "4224         1.201568                  0.776795         1.684760   \n",
      "4225         0.201932                 -0.210286        -0.948274   \n",
      "\n",
      "      Right-Cerebellum-White-Matter  MaskVol-to-eTIV  Left-vessel  \\\n",
      "0                         -1.141238         1.576612    -1.046274   \n",
      "1                         -0.918977         1.498072    -1.465700   \n",
      "2                         -0.476125         1.777970    -1.045709   \n",
      "3                         -0.528027         1.901717    -1.419796   \n",
      "4                         -0.992883         2.106391    -0.663778   \n",
      "...                             ...              ...          ...   \n",
      "4221                       0.924824        -1.315517     1.984509   \n",
      "4222                      -0.425657        -1.445257     1.225113   \n",
      "4223                      -0.763637        -2.322532     0.757355   \n",
      "4224                      -0.200260        -2.656968     1.115324   \n",
      "4225                      -0.911891        -1.615231     0.238375   \n",
      "\n",
      "      non-WM-hypointensities  lh_caudalanteriorcingulate_thickness  \\\n",
      "0                  -0.414457                              0.137179   \n",
      "1                  -0.787135                             -0.469268   \n",
      "2                  -0.873142                             -0.289058   \n",
      "3                  -0.270695                             -0.324507   \n",
      "4                  -0.666856                             -0.513133   \n",
      "...                      ...                                   ...   \n",
      "4221               -0.339618                              0.265219   \n",
      "4222               -1.080079                              0.538184   \n",
      "4223               -0.284824                              1.481699   \n",
      "4224               -1.443746                              1.398233   \n",
      "4225               -1.204476                              2.403907   \n",
      "\n",
      "      5th-Ventricle  non-WM-hypointensities  non-WM-hypointensities  \n",
      "0          0.668323               -0.278374                0.585567  \n",
      "1          0.405115               -0.237538                0.762405  \n",
      "2          0.083622                0.102540                0.199502  \n",
      "3          0.442483                0.018752                0.287582  \n",
      "4          0.942842               -0.201809                0.898185  \n",
      "...             ...                     ...                     ...  \n",
      "4221      -1.585345               -0.713712                0.463837  \n",
      "4222       0.112422               -0.560618                2.590270  \n",
      "4223       0.744069                0.673851                0.892396  \n",
      "4224      -0.292430                1.455330                0.728030  \n",
      "4225       0.655369                1.206156                0.828720  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_lateralorbitofrontal_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_entorhinal_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-Cerebellum-White-Matter</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>lh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.116693</td>\n",
       "      <td>1.364191</td>\n",
       "      <td>1.509738</td>\n",
       "      <td>-2.002542</td>\n",
       "      <td>-1.880681</td>\n",
       "      <td>2.278202</td>\n",
       "      <td>-1.648855</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>-0.458969</td>\n",
       "      <td>1.810743</td>\n",
       "      <td>-0.668529</td>\n",
       "      <td>0.109944</td>\n",
       "      <td>-1.141238</td>\n",
       "      <td>1.576612</td>\n",
       "      <td>-1.046274</td>\n",
       "      <td>-0.414457</td>\n",
       "      <td>0.137179</td>\n",
       "      <td>0.668323</td>\n",
       "      <td>-0.278374</td>\n",
       "      <td>0.585567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.781763</td>\n",
       "      <td>1.577276</td>\n",
       "      <td>1.751879</td>\n",
       "      <td>-1.118448</td>\n",
       "      <td>-1.486195</td>\n",
       "      <td>2.075799</td>\n",
       "      <td>-1.814979</td>\n",
       "      <td>-0.369271</td>\n",
       "      <td>-0.918797</td>\n",
       "      <td>1.998003</td>\n",
       "      <td>-0.610768</td>\n",
       "      <td>0.505767</td>\n",
       "      <td>-0.918977</td>\n",
       "      <td>1.498072</td>\n",
       "      <td>-1.465700</td>\n",
       "      <td>-0.787135</td>\n",
       "      <td>-0.469268</td>\n",
       "      <td>0.405115</td>\n",
       "      <td>-0.237538</td>\n",
       "      <td>0.762405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.423065</td>\n",
       "      <td>1.424486</td>\n",
       "      <td>1.583671</td>\n",
       "      <td>-1.542656</td>\n",
       "      <td>-1.246317</td>\n",
       "      <td>1.772968</td>\n",
       "      <td>-2.457122</td>\n",
       "      <td>-0.634560</td>\n",
       "      <td>-1.260260</td>\n",
       "      <td>1.776340</td>\n",
       "      <td>-0.738290</td>\n",
       "      <td>0.492492</td>\n",
       "      <td>-0.476125</td>\n",
       "      <td>1.777970</td>\n",
       "      <td>-1.045709</td>\n",
       "      <td>-0.873142</td>\n",
       "      <td>-0.289058</td>\n",
       "      <td>0.083622</td>\n",
       "      <td>0.102540</td>\n",
       "      <td>0.199502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.657487</td>\n",
       "      <td>1.366376</td>\n",
       "      <td>1.174550</td>\n",
       "      <td>-0.637088</td>\n",
       "      <td>-1.459873</td>\n",
       "      <td>2.255091</td>\n",
       "      <td>-1.240476</td>\n",
       "      <td>-0.939504</td>\n",
       "      <td>-1.082391</td>\n",
       "      <td>1.561048</td>\n",
       "      <td>-0.528174</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>-0.528027</td>\n",
       "      <td>1.901717</td>\n",
       "      <td>-1.419796</td>\n",
       "      <td>-0.270695</td>\n",
       "      <td>-0.324507</td>\n",
       "      <td>0.442483</td>\n",
       "      <td>0.018752</td>\n",
       "      <td>0.287582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.795704</td>\n",
       "      <td>1.701513</td>\n",
       "      <td>2.226951</td>\n",
       "      <td>-1.242501</td>\n",
       "      <td>-1.388356</td>\n",
       "      <td>2.820242</td>\n",
       "      <td>-1.689395</td>\n",
       "      <td>-0.277617</td>\n",
       "      <td>-0.695485</td>\n",
       "      <td>1.820614</td>\n",
       "      <td>-0.828457</td>\n",
       "      <td>0.417255</td>\n",
       "      <td>-0.992883</td>\n",
       "      <td>2.106391</td>\n",
       "      <td>-0.663778</td>\n",
       "      <td>-0.666856</td>\n",
       "      <td>-0.513133</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>-0.201809</td>\n",
       "      <td>0.898185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    2.116693                1.364191                1.509738   \n",
       "1                    1.781763                1.577276                1.751879   \n",
       "2                    2.423065                1.424486                1.583671   \n",
       "3                    4.657487                1.366376                1.174550   \n",
       "4                    3.795704                1.701513                2.226951   \n",
       "\n",
       "   lh_lateralorbitofrontal_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                          -2.002542     -1.880681      2.278202   \n",
       "1                          -1.118448     -1.486195      2.075799   \n",
       "2                          -1.542656     -1.246317      1.772968   \n",
       "3                          -0.637088     -1.459873      2.255091   \n",
       "4                          -1.242501     -1.388356      2.820242   \n",
       "\n",
       "   rh_entorhinal_thickness  CC_Posterior  Right-Caudate  MaskVol-to-eTIV  \\\n",
       "0                -1.648855     -0.011644      -0.458969         1.810743   \n",
       "1                -1.814979     -0.369271      -0.918797         1.998003   \n",
       "2                -2.457122     -0.634560      -1.260260         1.776340   \n",
       "3                -1.240476     -0.939504      -1.082391         1.561048   \n",
       "4                -1.689395     -0.277617      -0.695485         1.820614   \n",
       "\n",
       "   rh_frontalpole_thickness  MaskVol-to-eTIV  Right-Cerebellum-White-Matter  \\\n",
       "0                 -0.668529         0.109944                      -1.141238   \n",
       "1                 -0.610768         0.505767                      -0.918977   \n",
       "2                 -0.738290         0.492492                      -0.476125   \n",
       "3                 -0.528174         0.028773                      -0.528027   \n",
       "4                 -0.828457         0.417255                      -0.992883   \n",
       "\n",
       "   MaskVol-to-eTIV  Left-vessel  non-WM-hypointensities  \\\n",
       "0         1.576612    -1.046274               -0.414457   \n",
       "1         1.498072    -1.465700               -0.787135   \n",
       "2         1.777970    -1.045709               -0.873142   \n",
       "3         1.901717    -1.419796               -0.270695   \n",
       "4         2.106391    -0.663778               -0.666856   \n",
       "\n",
       "   lh_caudalanteriorcingulate_thickness  5th-Ventricle  \\\n",
       "0                              0.137179       0.668323   \n",
       "1                             -0.469268       0.405115   \n",
       "2                             -0.289058       0.083622   \n",
       "3                             -0.324507       0.442483   \n",
       "4                             -0.513133       0.942842   \n",
       "\n",
       "   non-WM-hypointensities  non-WM-hypointensities  \n",
       "0               -0.278374                0.585567  \n",
       "1               -0.237538                0.762405  \n",
       "2                0.102540                0.199502  \n",
       "3                0.018752                0.287582  \n",
       "4               -0.201809                0.898185  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a32919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f198fb3",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5959ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,905\n",
      "Trainable params: 83,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6be966",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3abb1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 22ms/step - loss: 8.2075 - msle: 8.2075 - val_loss: 1.6454 - val_msle: 1.6454\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6331 - msle: 0.6331 - val_loss: 0.3557 - val_msle: 0.3557\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.3920 - msle: 0.3920 - val_loss: 0.3242 - val_msle: 0.3242\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.2450 - msle: 0.2450 - val_loss: 0.1901 - val_msle: 0.1901\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1906 - msle: 0.1906 - val_loss: 0.1701 - val_msle: 0.1701\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1669 - msle: 0.1669 - val_loss: 0.1510 - val_msle: 0.1510\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1471 - msle: 0.1471 - val_loss: 0.1353 - val_msle: 0.1353\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1301 - msle: 0.1301 - val_loss: 0.1206 - val_msle: 0.1206\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1150 - msle: 0.1150 - val_loss: 0.1081 - val_msle: 0.1081\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1009 - msle: 0.1009 - val_loss: 0.0968 - val_msle: 0.0968\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0889 - msle: 0.0889 - val_loss: 0.0869 - val_msle: 0.0869\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0795 - msle: 0.0795 - val_loss: 0.0811 - val_msle: 0.0811\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0722 - msle: 0.0722 - val_loss: 0.0741 - val_msle: 0.0741\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0665 - msle: 0.0665 - val_loss: 0.0696 - val_msle: 0.0696\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0616 - msle: 0.0616 - val_loss: 0.0643 - val_msle: 0.0643\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0576 - msle: 0.0576 - val_loss: 0.0617 - val_msle: 0.0617\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0542 - msle: 0.0542 - val_loss: 0.0584 - val_msle: 0.0584\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0512 - msle: 0.0512 - val_loss: 0.0557 - val_msle: 0.0557\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0485 - msle: 0.0485 - val_loss: 0.0534 - val_msle: 0.0534\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0464 - msle: 0.0464 - val_loss: 0.0518 - val_msle: 0.0518\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0443 - msle: 0.0443 - val_loss: 0.0501 - val_msle: 0.0501\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0424 - msle: 0.0424 - val_loss: 0.0489 - val_msle: 0.0489\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0404 - msle: 0.0404 - val_loss: 0.0473 - val_msle: 0.0473\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0385 - msle: 0.0385 - val_loss: 0.0464 - val_msle: 0.0464\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0370 - msle: 0.0370 - val_loss: 0.0454 - val_msle: 0.0454\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0357 - msle: 0.0357 - val_loss: 0.0435 - val_msle: 0.0435\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0342 - msle: 0.0342 - val_loss: 0.0426 - val_msle: 0.0426\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0325 - msle: 0.0325 - val_loss: 0.0419 - val_msle: 0.0419\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0312 - msle: 0.0312 - val_loss: 0.0408 - val_msle: 0.0408\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0302 - msle: 0.0302 - val_loss: 0.0397 - val_msle: 0.0397\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0293 - msle: 0.0293 - val_loss: 0.0397 - val_msle: 0.0397\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0284 - msle: 0.0284 - val_loss: 0.0391 - val_msle: 0.0391\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0270 - msle: 0.0270 - val_loss: 0.0376 - val_msle: 0.0376\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0265 - msle: 0.0265 - val_loss: 0.0378 - val_msle: 0.0378\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0250 - msle: 0.0250 - val_loss: 0.0367 - val_msle: 0.0367\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0242 - msle: 0.0242 - val_loss: 0.0362 - val_msle: 0.0362\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0236 - msle: 0.0236 - val_loss: 0.0381 - val_msle: 0.0381\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0242 - msle: 0.0242 - val_loss: 0.0360 - val_msle: 0.0360\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0224 - msle: 0.0224 - val_loss: 0.0350 - val_msle: 0.0350\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0215 - msle: 0.0215 - val_loss: 0.0343 - val_msle: 0.0343\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0208 - msle: 0.0208 - val_loss: 0.0357 - val_msle: 0.0357\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0213 - msle: 0.0213 - val_loss: 0.0345 - val_msle: 0.0345\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0202 - msle: 0.0202 - val_loss: 0.0343 - val_msle: 0.0343\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0194 - msle: 0.0194 - val_loss: 0.0330 - val_msle: 0.0330\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0185 - msle: 0.0185 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0181 - msle: 0.0181 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0183 - msle: 0.0183 - val_loss: 0.0328 - val_msle: 0.0328\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0182 - msle: 0.0182 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0167 - msle: 0.0167 - val_loss: 0.0316 - val_msle: 0.0316\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0163 - msle: 0.0163 - val_loss: 0.0317 - val_msle: 0.0317\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0160 - msle: 0.0160 - val_loss: 0.0316 - val_msle: 0.0316\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0156 - msle: 0.0156 - val_loss: 0.0315 - val_msle: 0.0315\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0153 - msle: 0.0153 - val_loss: 0.0311 - val_msle: 0.0311\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0149 - msle: 0.0149 - val_loss: 0.0308 - val_msle: 0.0308\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0146 - msle: 0.0146 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0142 - msle: 0.0142 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0140 - msle: 0.0140 - val_loss: 0.0311 - val_msle: 0.0311\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0137 - msle: 0.0137 - val_loss: 0.0305 - val_msle: 0.0305\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0132 - msle: 0.0132 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0135 - msle: 0.0135 - val_loss: 0.0304 - val_msle: 0.0304\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0130 - msle: 0.0130 - val_loss: 0.0305 - val_msle: 0.0305\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0126 - msle: 0.0126 - val_loss: 0.0304 - val_msle: 0.0304\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0126 - msle: 0.0126 - val_loss: 0.0316 - val_msle: 0.0316\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0125 - msle: 0.0125 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0129 - msle: 0.0129 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0121 - msle: 0.0121 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0116 - msle: 0.0116 - val_loss: 0.0297 - val_msle: 0.0297\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0114 - msle: 0.0114 - val_loss: 0.0302 - val_msle: 0.0302\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0110 - msle: 0.0110 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0112 - msle: 0.0112 - val_loss: 0.0300 - val_msle: 0.0300\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0107 - msle: 0.0107 - val_loss: 0.0296 - val_msle: 0.0296\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0104 - msle: 0.0104 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.0103 - msle: 0.0103 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0109 - msle: 0.0109 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0106 - msle: 0.0106 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0100 - msle: 0.0100 - val_loss: 0.0320 - val_msle: 0.0320\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0100 - msle: 0.0100 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0098 - msle: 0.0098 - val_loss: 0.0307 - val_msle: 0.0307\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0093 - msle: 0.0093 - val_loss: 0.0313 - val_msle: 0.0313\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0093 - msle: 0.0093 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0090 - msle: 0.0090 - val_loss: 0.0307 - val_msle: 0.0307\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d408819",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e48fb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8359023201758367\n",
      "Max Error: 31.26587677001953\n",
      "Mean absolute error: 6.089218112657655\n",
      "Mean squared error: 64.42383750549024\n",
      "Root Mean squared error: 8.026446131725438\n",
      "R2: 0.8356906686124831\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4866cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(81))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be40c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
