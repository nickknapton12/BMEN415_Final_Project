{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea11364",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1823af",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75a2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069962",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05d81db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5353dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       1.754402                1.293661   \n",
      "1                       1.417516                1.506793   \n",
      "2                       2.060537                1.356492   \n",
      "3                       4.321472                1.316560   \n",
      "4                       3.432616                1.645478   \n",
      "...                          ...                     ...   \n",
      "4221                    3.508241                2.349696   \n",
      "4222                    4.445945               -2.409496   \n",
      "4223                    8.016491                2.326577   \n",
      "4224                   -0.596625                2.547033   \n",
      "4225                   -2.307309               -3.481649   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_pericalcarine_thickness  SurfaceHoles  \\\n",
      "0                   1.400035                   -1.478872     -1.847496   \n",
      "1                   1.653987                   -0.614048     -1.384830   \n",
      "2                   1.488951                   -1.055340     -1.174998   \n",
      "3                   1.100195                   -0.240310     -1.374512   \n",
      "4                   2.126633                   -0.804793     -1.322204   \n",
      "...                      ...                         ...           ...   \n",
      "4221                0.438968                    0.939801      0.641966   \n",
      "4222                1.632551                   -3.114777      1.459036   \n",
      "4223                3.899470                   -2.705463      1.087679   \n",
      "4224                6.998604                   -0.535334      2.483747   \n",
      "4225                2.207923                   -2.811137      1.635708   \n",
      "\n",
      "      CC_Posterior  rh_caudalanteriorcingulate_thickness  CC_Posterior  \\\n",
      "0         2.543808                             -1.131069     -0.411168   \n",
      "1         2.367862                             -1.435384     -0.854679   \n",
      "2         2.164486                             -2.159995     -0.820318   \n",
      "3         2.415121                             -1.136740     -1.573520   \n",
      "4         3.079020                             -1.200164     -0.815543   \n",
      "...            ...                                   ...           ...   \n",
      "4221     -0.407802                             -1.097621     -0.941874   \n",
      "4222     -0.235889                             -0.572814     -0.904172   \n",
      "4223      2.369678                              1.469668      2.351290   \n",
      "4224      4.642015                              2.408468      1.002992   \n",
      "4225      1.923662                              1.102109     -0.154756   \n",
      "\n",
      "      Right-Caudate  lh_parahippocampal_thickness  MaskVol-to-eTIV  \\\n",
      "0         -0.375874                      1.575706        -0.215381   \n",
      "1         -0.772121                      1.730385         0.194947   \n",
      "2         -1.129607                      1.481564         0.072066   \n",
      "3         -0.824431                      1.332201         0.288752   \n",
      "4         -0.544645                      1.522294        -0.082370   \n",
      "...             ...                           ...              ...   \n",
      "4221      -0.635432                     -0.819785        -0.181214   \n",
      "4222       0.550401                     -0.168608        -0.685791   \n",
      "4223       1.246512                      0.535511         1.479203   \n",
      "4224       4.405416                      1.482941         0.689009   \n",
      "4225       2.158918                      0.192730        -1.116467   \n",
      "\n",
      "      Brain-Stem  Left-vessel  Right-vessel  non-WM-hypointensities  \\\n",
      "0       0.175385    -1.594659      0.590112               -0.419819   \n",
      "1       0.374460    -1.466169      0.229012               -0.858767   \n",
      "2       0.312901    -0.851789      0.768033               -0.818180   \n",
      "3      -0.333657    -1.369837      0.511955               -0.327659   \n",
      "4       0.416392    -1.480764      1.228200               -0.870006   \n",
      "...          ...          ...           ...                     ...   \n",
      "4221   -0.280696     1.676497      0.170626               -0.078760   \n",
      "4222   -0.963481     0.055946     -0.672095               -0.778529   \n",
      "4223   -0.578519    -0.610596     -1.582753               -0.047811   \n",
      "4224    2.014027     0.789826     -1.505699               -1.233256   \n",
      "4225   -0.183326    -0.386018     -1.283849               -0.609903   \n",
      "\n",
      "      rh_isthmuscingulate_thickness  5th-Ventricle  non-WM-hypointensities  \\\n",
      "0                          0.490388       0.210059               -0.255816   \n",
      "1                          0.179173      -0.294345               -0.241843   \n",
      "2                          0.456272      -0.468695               -0.019350   \n",
      "3                          0.142232      -0.358880               -0.033206   \n",
      "4                          0.017986       0.276737               -0.136212   \n",
      "...                             ...            ...                     ...   \n",
      "4221                       0.042539      -0.917933               -0.526400   \n",
      "4222                       0.211769       0.361659               -0.563364   \n",
      "4223                       0.781419       1.184685                1.157254   \n",
      "4224                       1.053152       0.682845                1.401773   \n",
      "4225                       1.884649       1.380549                1.316358   \n",
      "\n",
      "      5th-Ventricle  5th-Ventricle  \n",
      "0          0.745396      -0.548675  \n",
      "1          1.224005      -0.287636  \n",
      "2          0.983764       0.133955  \n",
      "3          0.886275      -0.056838  \n",
      "4          1.185145      -0.515933  \n",
      "...             ...            ...  \n",
      "4221      -0.588001       0.172580  \n",
      "4222       0.975863      -2.166769  \n",
      "4223      -0.658737      -0.717793  \n",
      "4224      -1.132188      -1.078134  \n",
      "4225      -0.489069      -0.597589  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_pericalcarine_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>lh_parahippocampal_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Brain-Stem</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.754402</td>\n",
       "      <td>1.293661</td>\n",
       "      <td>1.400035</td>\n",
       "      <td>-1.478872</td>\n",
       "      <td>-1.847496</td>\n",
       "      <td>2.543808</td>\n",
       "      <td>-1.131069</td>\n",
       "      <td>-0.411168</td>\n",
       "      <td>-0.375874</td>\n",
       "      <td>1.575706</td>\n",
       "      <td>-0.215381</td>\n",
       "      <td>0.175385</td>\n",
       "      <td>-1.594659</td>\n",
       "      <td>0.590112</td>\n",
       "      <td>-0.419819</td>\n",
       "      <td>0.490388</td>\n",
       "      <td>0.210059</td>\n",
       "      <td>-0.255816</td>\n",
       "      <td>0.745396</td>\n",
       "      <td>-0.548675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.417516</td>\n",
       "      <td>1.506793</td>\n",
       "      <td>1.653987</td>\n",
       "      <td>-0.614048</td>\n",
       "      <td>-1.384830</td>\n",
       "      <td>2.367862</td>\n",
       "      <td>-1.435384</td>\n",
       "      <td>-0.854679</td>\n",
       "      <td>-0.772121</td>\n",
       "      <td>1.730385</td>\n",
       "      <td>0.194947</td>\n",
       "      <td>0.374460</td>\n",
       "      <td>-1.466169</td>\n",
       "      <td>0.229012</td>\n",
       "      <td>-0.858767</td>\n",
       "      <td>0.179173</td>\n",
       "      <td>-0.294345</td>\n",
       "      <td>-0.241843</td>\n",
       "      <td>1.224005</td>\n",
       "      <td>-0.287636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.060537</td>\n",
       "      <td>1.356492</td>\n",
       "      <td>1.488951</td>\n",
       "      <td>-1.055340</td>\n",
       "      <td>-1.174998</td>\n",
       "      <td>2.164486</td>\n",
       "      <td>-2.159995</td>\n",
       "      <td>-0.820318</td>\n",
       "      <td>-1.129607</td>\n",
       "      <td>1.481564</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.312901</td>\n",
       "      <td>-0.851789</td>\n",
       "      <td>0.768033</td>\n",
       "      <td>-0.818180</td>\n",
       "      <td>0.456272</td>\n",
       "      <td>-0.468695</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>0.983764</td>\n",
       "      <td>0.133955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.321472</td>\n",
       "      <td>1.316560</td>\n",
       "      <td>1.100195</td>\n",
       "      <td>-0.240310</td>\n",
       "      <td>-1.374512</td>\n",
       "      <td>2.415121</td>\n",
       "      <td>-1.136740</td>\n",
       "      <td>-1.573520</td>\n",
       "      <td>-0.824431</td>\n",
       "      <td>1.332201</td>\n",
       "      <td>0.288752</td>\n",
       "      <td>-0.333657</td>\n",
       "      <td>-1.369837</td>\n",
       "      <td>0.511955</td>\n",
       "      <td>-0.327659</td>\n",
       "      <td>0.142232</td>\n",
       "      <td>-0.358880</td>\n",
       "      <td>-0.033206</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>-0.056838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.432616</td>\n",
       "      <td>1.645478</td>\n",
       "      <td>2.126633</td>\n",
       "      <td>-0.804793</td>\n",
       "      <td>-1.322204</td>\n",
       "      <td>3.079020</td>\n",
       "      <td>-1.200164</td>\n",
       "      <td>-0.815543</td>\n",
       "      <td>-0.544645</td>\n",
       "      <td>1.522294</td>\n",
       "      <td>-0.082370</td>\n",
       "      <td>0.416392</td>\n",
       "      <td>-1.480764</td>\n",
       "      <td>1.228200</td>\n",
       "      <td>-0.870006</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.276737</td>\n",
       "      <td>-0.136212</td>\n",
       "      <td>1.185145</td>\n",
       "      <td>-0.515933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    1.754402                1.293661                1.400035   \n",
       "1                    1.417516                1.506793                1.653987   \n",
       "2                    2.060537                1.356492                1.488951   \n",
       "3                    4.321472                1.316560                1.100195   \n",
       "4                    3.432616                1.645478                2.126633   \n",
       "\n",
       "   lh_pericalcarine_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                   -1.478872     -1.847496      2.543808   \n",
       "1                   -0.614048     -1.384830      2.367862   \n",
       "2                   -1.055340     -1.174998      2.164486   \n",
       "3                   -0.240310     -1.374512      2.415121   \n",
       "4                   -0.804793     -1.322204      3.079020   \n",
       "\n",
       "   rh_caudalanteriorcingulate_thickness  CC_Posterior  Right-Caudate  \\\n",
       "0                             -1.131069     -0.411168      -0.375874   \n",
       "1                             -1.435384     -0.854679      -0.772121   \n",
       "2                             -2.159995     -0.820318      -1.129607   \n",
       "3                             -1.136740     -1.573520      -0.824431   \n",
       "4                             -1.200164     -0.815543      -0.544645   \n",
       "\n",
       "   lh_parahippocampal_thickness  MaskVol-to-eTIV  Brain-Stem  Left-vessel  \\\n",
       "0                      1.575706        -0.215381    0.175385    -1.594659   \n",
       "1                      1.730385         0.194947    0.374460    -1.466169   \n",
       "2                      1.481564         0.072066    0.312901    -0.851789   \n",
       "3                      1.332201         0.288752   -0.333657    -1.369837   \n",
       "4                      1.522294        -0.082370    0.416392    -1.480764   \n",
       "\n",
       "   Right-vessel  non-WM-hypointensities  rh_isthmuscingulate_thickness  \\\n",
       "0      0.590112               -0.419819                       0.490388   \n",
       "1      0.229012               -0.858767                       0.179173   \n",
       "2      0.768033               -0.818180                       0.456272   \n",
       "3      0.511955               -0.327659                       0.142232   \n",
       "4      1.228200               -0.870006                       0.017986   \n",
       "\n",
       "   5th-Ventricle  non-WM-hypointensities  5th-Ventricle  5th-Ventricle  \n",
       "0       0.210059               -0.255816       0.745396      -0.548675  \n",
       "1      -0.294345               -0.241843       1.224005      -0.287636  \n",
       "2      -0.468695               -0.019350       0.983764       0.133955  \n",
       "3      -0.358880               -0.033206       0.886275      -0.056838  \n",
       "4       0.276737               -0.136212       1.185145      -0.515933  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data_feat)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data_feat.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a32919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f198fb3",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5959ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_25 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,905\n",
      "Trainable params: 83,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6be966",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3abb1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 1s 12ms/step - loss: 3.7670 - msle: 3.7670 - val_loss: 0.3715 - val_msle: 0.3715\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3261 - msle: 0.3261 - val_loss: 0.2001 - val_msle: 0.2001\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1860 - msle: 0.1860 - val_loss: 0.1632 - val_msle: 0.1632\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1463 - msle: 0.1463 - val_loss: 0.1355 - val_msle: 0.1355\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1208 - msle: 0.1208 - val_loss: 0.1158 - val_msle: 0.1158\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1029 - msle: 0.1029 - val_loss: 0.1012 - val_msle: 0.1012\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0908 - msle: 0.0908 - val_loss: 0.0920 - val_msle: 0.0920\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0825 - msle: 0.0825 - val_loss: 0.0836 - val_msle: 0.0836\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0757 - msle: 0.0757 - val_loss: 0.0776 - val_msle: 0.0776\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0701 - msle: 0.0701 - val_loss: 0.0710 - val_msle: 0.0710\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0652 - msle: 0.0652 - val_loss: 0.0667 - val_msle: 0.0667\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0603 - msle: 0.0603 - val_loss: 0.0628 - val_msle: 0.0628\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0567 - msle: 0.0567 - val_loss: 0.0594 - val_msle: 0.0594\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0533 - msle: 0.0533 - val_loss: 0.0553 - val_msle: 0.0553\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0490 - msle: 0.0490 - val_loss: 0.0526 - val_msle: 0.0526\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0462 - msle: 0.0462 - val_loss: 0.0509 - val_msle: 0.0509\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0433 - msle: 0.0433 - val_loss: 0.0471 - val_msle: 0.0471\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0410 - msle: 0.0410 - val_loss: 0.0450 - val_msle: 0.0450\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0384 - msle: 0.0384 - val_loss: 0.0445 - val_msle: 0.0445\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0359 - msle: 0.0359 - val_loss: 0.0417 - val_msle: 0.0417\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0338 - msle: 0.0338 - val_loss: 0.0402 - val_msle: 0.0402\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0319 - msle: 0.0319 - val_loss: 0.0393 - val_msle: 0.0393\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0302 - msle: 0.0302 - val_loss: 0.0375 - val_msle: 0.0375\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0292 - msle: 0.0292 - val_loss: 0.0386 - val_msle: 0.0386\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0276 - msle: 0.0276 - val_loss: 0.0367 - val_msle: 0.0367\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0261 - msle: 0.0261 - val_loss: 0.0356 - val_msle: 0.0356\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0251 - msle: 0.0251 - val_loss: 0.0349 - val_msle: 0.0349\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0241 - msle: 0.0241 - val_loss: 0.0352 - val_msle: 0.0352\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0232 - msle: 0.0232 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0223 - msle: 0.0223 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0221 - msle: 0.0221 - val_loss: 0.0352 - val_msle: 0.0352\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0212 - msle: 0.0212 - val_loss: 0.0346 - val_msle: 0.0346\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0201 - msle: 0.0201 - val_loss: 0.0341 - val_msle: 0.0341\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0197 - msle: 0.0197 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0188 - msle: 0.0188 - val_loss: 0.0354 - val_msle: 0.0354\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0183 - msle: 0.0183 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0180 - msle: 0.0180 - val_loss: 0.0337 - val_msle: 0.0337\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0174 - msle: 0.0174 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0169 - msle: 0.0169 - val_loss: 0.0337 - val_msle: 0.0337\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0163 - msle: 0.0163 - val_loss: 0.0345 - val_msle: 0.0345\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0162 - msle: 0.0162 - val_loss: 0.0356 - val_msle: 0.0356\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0158 - msle: 0.0158 - val_loss: 0.0332 - val_msle: 0.0332\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0152 - msle: 0.0152 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0152 - msle: 0.0152 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0146 - msle: 0.0146 - val_loss: 0.0340 - val_msle: 0.0340\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0143 - msle: 0.0143 - val_loss: 0.0359 - val_msle: 0.0359\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0139 - msle: 0.0139 - val_loss: 0.0351 - val_msle: 0.0351\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0136 - msle: 0.0136 - val_loss: 0.0344 - val_msle: 0.0344\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0133 - msle: 0.0133 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0129 - msle: 0.0129 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0124 - msle: 0.0124 - val_loss: 0.0346 - val_msle: 0.0346\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0125 - msle: 0.0125 - val_loss: 0.0345 - val_msle: 0.0345\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d408819",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e48fb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.828164883556715\n",
      "Max Error: 33.99394226074219\n",
      "Mean absolute error: 6.014110430231634\n",
      "Mean squared error: 67.98508997972411\n",
      "Root Mean squared error: 8.245307148901373\n",
      "R2: 0.8266078968373053\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4866cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(52))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
