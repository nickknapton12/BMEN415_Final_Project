{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea11364",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1823af",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75a2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069962",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05d81db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5353dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       2.116693                1.364191   \n",
      "1                       1.781763                1.577276   \n",
      "2                       2.423065                1.424486   \n",
      "3                       4.657487                1.366376   \n",
      "4                       3.795704                1.701513   \n",
      "...                          ...                     ...   \n",
      "4221                    3.332053                2.220377   \n",
      "4222                    4.258130               -2.535944   \n",
      "4223                    7.826457                2.169780   \n",
      "4224                   -0.702316                2.439424   \n",
      "4225                   -2.373678               -3.566135   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_lateralorbitofrontal_thickness  SurfaceHoles  \\\n",
      "0                   1.509738                          -2.002542     -1.880681   \n",
      "1                   1.751879                          -1.118448     -1.486195   \n",
      "2                   1.583671                          -1.542656     -1.246317   \n",
      "3                   1.174550                          -0.637088     -1.459873   \n",
      "4                   2.226951                          -1.242501     -1.388356   \n",
      "...                      ...                                ...           ...   \n",
      "4221                0.517971                           1.504663      0.767874   \n",
      "4222                1.742639                          -2.389918      1.916256   \n",
      "4223                3.996086                          -1.861737      1.557014   \n",
      "4224                7.148702                           0.139656      2.690266   \n",
      "4225                2.435552                          -2.369988      1.958036   \n",
      "\n",
      "      CC_Posterior  rh_entorhinal_thickness  CC_Posterior  Right-Caudate  \\\n",
      "0         2.278202                -1.648855     -0.011644      -0.458969   \n",
      "1         2.075799                -1.814979     -0.369271      -0.918797   \n",
      "2         1.772968                -2.457122     -0.634560      -1.260260   \n",
      "3         2.255091                -1.240476     -0.939504      -1.082391   \n",
      "4         2.820242                -1.689395     -0.277617      -0.695485   \n",
      "...            ...                      ...           ...            ...   \n",
      "4221     -0.090273                 0.276253     -2.287790      -0.620017   \n",
      "4222      0.224896                 0.949633     -2.436397       0.600692   \n",
      "4223      2.801791                 1.434244      0.585143       1.839752   \n",
      "4224      5.135757                 1.901716      0.901710       4.569846   \n",
      "4225      2.430738                 1.590827     -0.544596       2.189233   \n",
      "\n",
      "      MaskVol-to-eTIV  rh_frontalpole_thickness  MaskVol-to-eTIV  \\\n",
      "0            1.810743                 -0.668529         0.109944   \n",
      "1            1.998003                 -0.610768         0.505767   \n",
      "2            1.776340                 -0.738290         0.492492   \n",
      "3            1.561048                 -0.528174         0.028773   \n",
      "4            1.820614                 -0.828457         0.417255   \n",
      "...               ...                       ...              ...   \n",
      "4221        -0.851313                  0.574070        -0.409632   \n",
      "4222        -0.289168                  0.708990        -1.480759   \n",
      "4223        -0.119359                  3.734455        -0.349164   \n",
      "4224         1.201568                  0.776795         1.684760   \n",
      "4225         0.201932                 -0.210286        -0.948274   \n",
      "\n",
      "      Right-Cerebellum-White-Matter  MaskVol-to-eTIV  Left-vessel  \\\n",
      "0                         -1.141238         1.576612    -1.046274   \n",
      "1                         -0.918977         1.498072    -1.465700   \n",
      "2                         -0.476125         1.777970    -1.045709   \n",
      "3                         -0.528027         1.901717    -1.419796   \n",
      "4                         -0.992883         2.106391    -0.663778   \n",
      "...                             ...              ...          ...   \n",
      "4221                       0.924824        -1.315517     1.984509   \n",
      "4222                      -0.425657        -1.445257     1.225113   \n",
      "4223                      -0.763637        -2.322532     0.757355   \n",
      "4224                      -0.200260        -2.656968     1.115324   \n",
      "4225                      -0.911891        -1.615231     0.238375   \n",
      "\n",
      "      non-WM-hypointensities  lh_caudalanteriorcingulate_thickness  \\\n",
      "0                  -0.414457                              0.137179   \n",
      "1                  -0.787135                             -0.469268   \n",
      "2                  -0.873142                             -0.289058   \n",
      "3                  -0.270695                             -0.324507   \n",
      "4                  -0.666856                             -0.513133   \n",
      "...                      ...                                   ...   \n",
      "4221               -0.339618                              0.265219   \n",
      "4222               -1.080079                              0.538184   \n",
      "4223               -0.284824                              1.481699   \n",
      "4224               -1.443746                              1.398233   \n",
      "4225               -1.204476                              2.403907   \n",
      "\n",
      "      5th-Ventricle  non-WM-hypointensities  non-WM-hypointensities  \n",
      "0          0.668323               -0.278374                0.585567  \n",
      "1          0.405115               -0.237538                0.762405  \n",
      "2          0.083622                0.102540                0.199502  \n",
      "3          0.442483                0.018752                0.287582  \n",
      "4          0.942842               -0.201809                0.898185  \n",
      "...             ...                     ...                     ...  \n",
      "4221      -1.585345               -0.713712                0.463837  \n",
      "4222       0.112422               -0.560618                2.590270  \n",
      "4223       0.744069                0.673851                0.892396  \n",
      "4224      -0.292430                1.455330                0.728030  \n",
      "4225       0.655369                1.206156                0.828720  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_lateralorbitofrontal_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_entorhinal_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Right-Cerebellum-White-Matter</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>lh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.116693</td>\n",
       "      <td>1.364191</td>\n",
       "      <td>1.509738</td>\n",
       "      <td>-2.002542</td>\n",
       "      <td>-1.880681</td>\n",
       "      <td>2.278202</td>\n",
       "      <td>-1.648855</td>\n",
       "      <td>-0.011644</td>\n",
       "      <td>-0.458969</td>\n",
       "      <td>1.810743</td>\n",
       "      <td>-0.668529</td>\n",
       "      <td>0.109944</td>\n",
       "      <td>-1.141238</td>\n",
       "      <td>1.576612</td>\n",
       "      <td>-1.046274</td>\n",
       "      <td>-0.414457</td>\n",
       "      <td>0.137179</td>\n",
       "      <td>0.668323</td>\n",
       "      <td>-0.278374</td>\n",
       "      <td>0.585567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.781763</td>\n",
       "      <td>1.577276</td>\n",
       "      <td>1.751879</td>\n",
       "      <td>-1.118448</td>\n",
       "      <td>-1.486195</td>\n",
       "      <td>2.075799</td>\n",
       "      <td>-1.814979</td>\n",
       "      <td>-0.369271</td>\n",
       "      <td>-0.918797</td>\n",
       "      <td>1.998003</td>\n",
       "      <td>-0.610768</td>\n",
       "      <td>0.505767</td>\n",
       "      <td>-0.918977</td>\n",
       "      <td>1.498072</td>\n",
       "      <td>-1.465700</td>\n",
       "      <td>-0.787135</td>\n",
       "      <td>-0.469268</td>\n",
       "      <td>0.405115</td>\n",
       "      <td>-0.237538</td>\n",
       "      <td>0.762405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.423065</td>\n",
       "      <td>1.424486</td>\n",
       "      <td>1.583671</td>\n",
       "      <td>-1.542656</td>\n",
       "      <td>-1.246317</td>\n",
       "      <td>1.772968</td>\n",
       "      <td>-2.457122</td>\n",
       "      <td>-0.634560</td>\n",
       "      <td>-1.260260</td>\n",
       "      <td>1.776340</td>\n",
       "      <td>-0.738290</td>\n",
       "      <td>0.492492</td>\n",
       "      <td>-0.476125</td>\n",
       "      <td>1.777970</td>\n",
       "      <td>-1.045709</td>\n",
       "      <td>-0.873142</td>\n",
       "      <td>-0.289058</td>\n",
       "      <td>0.083622</td>\n",
       "      <td>0.102540</td>\n",
       "      <td>0.199502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.657487</td>\n",
       "      <td>1.366376</td>\n",
       "      <td>1.174550</td>\n",
       "      <td>-0.637088</td>\n",
       "      <td>-1.459873</td>\n",
       "      <td>2.255091</td>\n",
       "      <td>-1.240476</td>\n",
       "      <td>-0.939504</td>\n",
       "      <td>-1.082391</td>\n",
       "      <td>1.561048</td>\n",
       "      <td>-0.528174</td>\n",
       "      <td>0.028773</td>\n",
       "      <td>-0.528027</td>\n",
       "      <td>1.901717</td>\n",
       "      <td>-1.419796</td>\n",
       "      <td>-0.270695</td>\n",
       "      <td>-0.324507</td>\n",
       "      <td>0.442483</td>\n",
       "      <td>0.018752</td>\n",
       "      <td>0.287582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.795704</td>\n",
       "      <td>1.701513</td>\n",
       "      <td>2.226951</td>\n",
       "      <td>-1.242501</td>\n",
       "      <td>-1.388356</td>\n",
       "      <td>2.820242</td>\n",
       "      <td>-1.689395</td>\n",
       "      <td>-0.277617</td>\n",
       "      <td>-0.695485</td>\n",
       "      <td>1.820614</td>\n",
       "      <td>-0.828457</td>\n",
       "      <td>0.417255</td>\n",
       "      <td>-0.992883</td>\n",
       "      <td>2.106391</td>\n",
       "      <td>-0.663778</td>\n",
       "      <td>-0.666856</td>\n",
       "      <td>-0.513133</td>\n",
       "      <td>0.942842</td>\n",
       "      <td>-0.201809</td>\n",
       "      <td>0.898185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    2.116693                1.364191                1.509738   \n",
       "1                    1.781763                1.577276                1.751879   \n",
       "2                    2.423065                1.424486                1.583671   \n",
       "3                    4.657487                1.366376                1.174550   \n",
       "4                    3.795704                1.701513                2.226951   \n",
       "\n",
       "   lh_lateralorbitofrontal_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                          -2.002542     -1.880681      2.278202   \n",
       "1                          -1.118448     -1.486195      2.075799   \n",
       "2                          -1.542656     -1.246317      1.772968   \n",
       "3                          -0.637088     -1.459873      2.255091   \n",
       "4                          -1.242501     -1.388356      2.820242   \n",
       "\n",
       "   rh_entorhinal_thickness  CC_Posterior  Right-Caudate  MaskVol-to-eTIV  \\\n",
       "0                -1.648855     -0.011644      -0.458969         1.810743   \n",
       "1                -1.814979     -0.369271      -0.918797         1.998003   \n",
       "2                -2.457122     -0.634560      -1.260260         1.776340   \n",
       "3                -1.240476     -0.939504      -1.082391         1.561048   \n",
       "4                -1.689395     -0.277617      -0.695485         1.820614   \n",
       "\n",
       "   rh_frontalpole_thickness  MaskVol-to-eTIV  Right-Cerebellum-White-Matter  \\\n",
       "0                 -0.668529         0.109944                      -1.141238   \n",
       "1                 -0.610768         0.505767                      -0.918977   \n",
       "2                 -0.738290         0.492492                      -0.476125   \n",
       "3                 -0.528174         0.028773                      -0.528027   \n",
       "4                 -0.828457         0.417255                      -0.992883   \n",
       "\n",
       "   MaskVol-to-eTIV  Left-vessel  non-WM-hypointensities  \\\n",
       "0         1.576612    -1.046274               -0.414457   \n",
       "1         1.498072    -1.465700               -0.787135   \n",
       "2         1.777970    -1.045709               -0.873142   \n",
       "3         1.901717    -1.419796               -0.270695   \n",
       "4         2.106391    -0.663778               -0.666856   \n",
       "\n",
       "   lh_caudalanteriorcingulate_thickness  5th-Ventricle  \\\n",
       "0                              0.137179       0.668323   \n",
       "1                             -0.469268       0.405115   \n",
       "2                             -0.289058       0.083622   \n",
       "3                             -0.324507       0.442483   \n",
       "4                             -0.513133       0.942842   \n",
       "\n",
       "   non-WM-hypointensities  non-WM-hypointensities  \n",
       "0               -0.278374                0.585567  \n",
       "1               -0.237538                0.762405  \n",
       "2                0.102540                0.199502  \n",
       "3                0.018752                0.287582  \n",
       "4               -0.201809                0.898185  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a32919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f198fb3",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5959ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_30 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,905\n",
      "Trainable params: 83,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6be966",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3abb1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "14/14 [==============================] - 1s 22ms/step - loss: 8.2075 - msle: 8.2075 - val_loss: 1.6454 - val_msle: 1.6454\n",
      "Epoch 2/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.6331 - msle: 0.6331 - val_loss: 0.3557 - val_msle: 0.3557\n",
      "Epoch 3/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.3920 - msle: 0.3920 - val_loss: 0.3242 - val_msle: 0.3242\n",
      "Epoch 4/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.2450 - msle: 0.2450 - val_loss: 0.1901 - val_msle: 0.1901\n",
      "Epoch 5/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1906 - msle: 0.1906 - val_loss: 0.1701 - val_msle: 0.1701\n",
      "Epoch 6/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1669 - msle: 0.1669 - val_loss: 0.1510 - val_msle: 0.1510\n",
      "Epoch 7/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1471 - msle: 0.1471 - val_loss: 0.1353 - val_msle: 0.1353\n",
      "Epoch 8/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1301 - msle: 0.1301 - val_loss: 0.1206 - val_msle: 0.1206\n",
      "Epoch 9/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1150 - msle: 0.1150 - val_loss: 0.1081 - val_msle: 0.1081\n",
      "Epoch 10/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.1009 - msle: 0.1009 - val_loss: 0.0968 - val_msle: 0.0968\n",
      "Epoch 11/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0889 - msle: 0.0889 - val_loss: 0.0869 - val_msle: 0.0869\n",
      "Epoch 12/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0795 - msle: 0.0795 - val_loss: 0.0811 - val_msle: 0.0811\n",
      "Epoch 13/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0722 - msle: 0.0722 - val_loss: 0.0741 - val_msle: 0.0741\n",
      "Epoch 14/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0665 - msle: 0.0665 - val_loss: 0.0696 - val_msle: 0.0696\n",
      "Epoch 15/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0616 - msle: 0.0616 - val_loss: 0.0643 - val_msle: 0.0643\n",
      "Epoch 16/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0576 - msle: 0.0576 - val_loss: 0.0617 - val_msle: 0.0617\n",
      "Epoch 17/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0542 - msle: 0.0542 - val_loss: 0.0584 - val_msle: 0.0584\n",
      "Epoch 18/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0512 - msle: 0.0512 - val_loss: 0.0557 - val_msle: 0.0557\n",
      "Epoch 19/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0485 - msle: 0.0485 - val_loss: 0.0534 - val_msle: 0.0534\n",
      "Epoch 20/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0464 - msle: 0.0464 - val_loss: 0.0518 - val_msle: 0.0518\n",
      "Epoch 21/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0443 - msle: 0.0443 - val_loss: 0.0501 - val_msle: 0.0501\n",
      "Epoch 22/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0424 - msle: 0.0424 - val_loss: 0.0489 - val_msle: 0.0489\n",
      "Epoch 23/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0404 - msle: 0.0404 - val_loss: 0.0473 - val_msle: 0.0473\n",
      "Epoch 24/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0385 - msle: 0.0385 - val_loss: 0.0464 - val_msle: 0.0464\n",
      "Epoch 25/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0370 - msle: 0.0370 - val_loss: 0.0454 - val_msle: 0.0454\n",
      "Epoch 26/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0357 - msle: 0.0357 - val_loss: 0.0435 - val_msle: 0.0435\n",
      "Epoch 27/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0342 - msle: 0.0342 - val_loss: 0.0426 - val_msle: 0.0426\n",
      "Epoch 28/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0325 - msle: 0.0325 - val_loss: 0.0419 - val_msle: 0.0419\n",
      "Epoch 29/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0312 - msle: 0.0312 - val_loss: 0.0408 - val_msle: 0.0408\n",
      "Epoch 30/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0302 - msle: 0.0302 - val_loss: 0.0397 - val_msle: 0.0397\n",
      "Epoch 31/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0293 - msle: 0.0293 - val_loss: 0.0397 - val_msle: 0.0397\n",
      "Epoch 32/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0284 - msle: 0.0284 - val_loss: 0.0391 - val_msle: 0.0391\n",
      "Epoch 33/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0270 - msle: 0.0270 - val_loss: 0.0376 - val_msle: 0.0376\n",
      "Epoch 34/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0265 - msle: 0.0265 - val_loss: 0.0378 - val_msle: 0.0378\n",
      "Epoch 35/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0250 - msle: 0.0250 - val_loss: 0.0367 - val_msle: 0.0367\n",
      "Epoch 36/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0242 - msle: 0.0242 - val_loss: 0.0362 - val_msle: 0.0362\n",
      "Epoch 37/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0236 - msle: 0.0236 - val_loss: 0.0381 - val_msle: 0.0381\n",
      "Epoch 38/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0242 - msle: 0.0242 - val_loss: 0.0360 - val_msle: 0.0360\n",
      "Epoch 39/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0224 - msle: 0.0224 - val_loss: 0.0350 - val_msle: 0.0350\n",
      "Epoch 40/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0215 - msle: 0.0215 - val_loss: 0.0343 - val_msle: 0.0343\n",
      "Epoch 41/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0208 - msle: 0.0208 - val_loss: 0.0357 - val_msle: 0.0357\n",
      "Epoch 42/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0213 - msle: 0.0213 - val_loss: 0.0345 - val_msle: 0.0345\n",
      "Epoch 43/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0202 - msle: 0.0202 - val_loss: 0.0343 - val_msle: 0.0343\n",
      "Epoch 44/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0194 - msle: 0.0194 - val_loss: 0.0330 - val_msle: 0.0330\n",
      "Epoch 45/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0185 - msle: 0.0185 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 46/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0181 - msle: 0.0181 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 47/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0183 - msle: 0.0183 - val_loss: 0.0328 - val_msle: 0.0328\n",
      "Epoch 48/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0182 - msle: 0.0182 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 49/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0167 - msle: 0.0167 - val_loss: 0.0316 - val_msle: 0.0316\n",
      "Epoch 50/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0163 - msle: 0.0163 - val_loss: 0.0317 - val_msle: 0.0317\n",
      "Epoch 51/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0160 - msle: 0.0160 - val_loss: 0.0316 - val_msle: 0.0316\n",
      "Epoch 52/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0156 - msle: 0.0156 - val_loss: 0.0315 - val_msle: 0.0315\n",
      "Epoch 53/100\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0153 - msle: 0.0153 - val_loss: 0.0311 - val_msle: 0.0311\n",
      "Epoch 54/100\n",
      "14/14 [==============================] - 0s 14ms/step - loss: 0.0149 - msle: 0.0149 - val_loss: 0.0308 - val_msle: 0.0308\n",
      "Epoch 55/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0146 - msle: 0.0146 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 56/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0142 - msle: 0.0142 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 57/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0140 - msle: 0.0140 - val_loss: 0.0311 - val_msle: 0.0311\n",
      "Epoch 58/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0137 - msle: 0.0137 - val_loss: 0.0305 - val_msle: 0.0305\n",
      "Epoch 59/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0132 - msle: 0.0132 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 60/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0135 - msle: 0.0135 - val_loss: 0.0304 - val_msle: 0.0304\n",
      "Epoch 61/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0130 - msle: 0.0130 - val_loss: 0.0305 - val_msle: 0.0305\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0126 - msle: 0.0126 - val_loss: 0.0304 - val_msle: 0.0304\n",
      "Epoch 63/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0126 - msle: 0.0126 - val_loss: 0.0316 - val_msle: 0.0316\n",
      "Epoch 64/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0125 - msle: 0.0125 - val_loss: 0.0324 - val_msle: 0.0324\n",
      "Epoch 65/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0129 - msle: 0.0129 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 66/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0121 - msle: 0.0121 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 67/100\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0116 - msle: 0.0116 - val_loss: 0.0297 - val_msle: 0.0297\n",
      "Epoch 68/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0114 - msle: 0.0114 - val_loss: 0.0302 - val_msle: 0.0302\n",
      "Epoch 69/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0110 - msle: 0.0110 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 70/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0112 - msle: 0.0112 - val_loss: 0.0300 - val_msle: 0.0300\n",
      "Epoch 71/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0107 - msle: 0.0107 - val_loss: 0.0296 - val_msle: 0.0296\n",
      "Epoch 72/100\n",
      "14/14 [==============================] - 0s 15ms/step - loss: 0.0104 - msle: 0.0104 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 73/100\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.0103 - msle: 0.0103 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 74/100\n",
      "14/14 [==============================] - 0s 16ms/step - loss: 0.0109 - msle: 0.0109 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 75/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0106 - msle: 0.0106 - val_loss: 0.0303 - val_msle: 0.0303\n",
      "Epoch 76/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0100 - msle: 0.0100 - val_loss: 0.0320 - val_msle: 0.0320\n",
      "Epoch 77/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0100 - msle: 0.0100 - val_loss: 0.0321 - val_msle: 0.0321\n",
      "Epoch 78/100\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.0098 - msle: 0.0098 - val_loss: 0.0307 - val_msle: 0.0307\n",
      "Epoch 79/100\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.0093 - msle: 0.0093 - val_loss: 0.0313 - val_msle: 0.0313\n",
      "Epoch 80/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0093 - msle: 0.0093 - val_loss: 0.0310 - val_msle: 0.0310\n",
      "Epoch 81/100\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.0090 - msle: 0.0090 - val_loss: 0.0307 - val_msle: 0.0307\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=256,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d408819",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e48fb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.8359023201758367\n",
      "Max Error: 31.26587677001953\n",
      "Mean absolute error: 6.089218112657655\n",
      "Mean squared error: 64.42383750549024\n",
      "Root Mean squared error: 8.026446131725438\n",
      "R2: 0.8356906686124831\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4866cfdd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (100,) and (81,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [41]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m vy \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m ty \u001b[38;5;241m=\u001b[39m hist\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot( x, ty, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "File \u001b[0;32m~/miniforge3/envs/newBmen415/lib/python3.8/site-packages/matplotlib/pyplot.py:2757\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2755\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2759\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/newBmen415/lib/python3.8/site-packages/matplotlib/axes/_axes.py:1632\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1391\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1392\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1631\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1632\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1633\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1634\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/miniforge3/envs/newBmen415/lib/python3.8/site-packages/matplotlib/axes/_base.py:312\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    311\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 312\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/newBmen415/lib/python3.8/site-packages/matplotlib/axes/_base.py:498\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 498\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (100,) and (81,)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(100))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='val_loss')\n",
    "plt.plot( x, ty, label='loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be40c16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
