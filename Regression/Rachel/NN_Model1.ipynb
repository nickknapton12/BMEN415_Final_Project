{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ea11364",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "Introduction\n",
    "Import\n",
    "Analysis & Preprocessing\n",
    "Model\n",
    "Training\n",
    "Analysis & Conclusion\n",
    "\n",
    "# 1. Introduction\n",
    "References:\n",
    "\n",
    "- https://machinelearningmastery.com/feature-selection-for-regression-data/\n",
    "- https://www.analyticsvidhya.com/blog/2021/08/a-walk-through-of-regression-analysis-using-artificial-neural-networks-in-tensorflow/\n",
    "- https://machinelearningmastery.com/regression-tutorial-keras-deep-learning-library-python/\n",
    "- https://thinkingneuron.com/using-artificial-neural-networks-for-regression-in-python/\n",
    "- https://www.studytonight.com/post/what-is-mean-squared-error-mean-absolute-error-root-mean-squared-error-and-r-squared#:~:text=MAE%3A%20It%20is%20not%20very,the%20weighted%20individual%20differences%20equally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1823af",
   "metadata": {},
   "source": [
    "# 2. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b75a2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import utils, callbacks\n",
    "from tensorflow.keras.losses import MeanSquaredLogarithmicError\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61069962",
   "metadata": {},
   "source": [
    "# 3. Analysis & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b05d81db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.No</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>Left-Inf-Lat-Vent</th>\n",
       "      <th>Left-Cerebellum-White-Matter</th>\n",
       "      <th>Left-Cerebellum-Cortex</th>\n",
       "      <th>Left-Thalamus</th>\n",
       "      <th>Left-Caudate</th>\n",
       "      <th>Left-Putamen</th>\n",
       "      <th>Left-Pallidum</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>...</th>\n",
       "      <th>rh_supramarginal_thickness</th>\n",
       "      <th>rh_frontalpole_thickness</th>\n",
       "      <th>rh_temporalpole_thickness</th>\n",
       "      <th>rh_transversetemporal_thickness</th>\n",
       "      <th>rh_insula_thickness</th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>BrainSegVolNotVent.2</th>\n",
       "      <th>eTIV.1</th>\n",
       "      <th>Age</th>\n",
       "      <th>dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4.226000e+03</td>\n",
       "      <td>4226.000000</td>\n",
       "      <td>4226.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>13370.040795</td>\n",
       "      <td>574.849716</td>\n",
       "      <td>14646.696711</td>\n",
       "      <td>52002.811571</td>\n",
       "      <td>7164.947539</td>\n",
       "      <td>3337.653526</td>\n",
       "      <td>4505.158755</td>\n",
       "      <td>1958.214458</td>\n",
       "      <td>1418.947373</td>\n",
       "      <td>...</td>\n",
       "      <td>2.429779</td>\n",
       "      <td>2.684327</td>\n",
       "      <td>3.555803</td>\n",
       "      <td>2.288283</td>\n",
       "      <td>2.846123</td>\n",
       "      <td>2.372266</td>\n",
       "      <td>1.085468e+06</td>\n",
       "      <td>1.514925e+06</td>\n",
       "      <td>58.374586</td>\n",
       "      <td>4.533838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1220.085448</td>\n",
       "      <td>9194.928348</td>\n",
       "      <td>594.590387</td>\n",
       "      <td>2622.868798</td>\n",
       "      <td>6378.435917</td>\n",
       "      <td>1207.229615</td>\n",
       "      <td>502.352001</td>\n",
       "      <td>713.658580</td>\n",
       "      <td>287.139826</td>\n",
       "      <td>635.143286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.185543</td>\n",
       "      <td>0.275245</td>\n",
       "      <td>0.332094</td>\n",
       "      <td>0.269851</td>\n",
       "      <td>0.195038</td>\n",
       "      <td>0.146944</td>\n",
       "      <td>1.248881e+05</td>\n",
       "      <td>1.651798e+05</td>\n",
       "      <td>20.064099</td>\n",
       "      <td>3.057928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>2204.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6920.100000</td>\n",
       "      <td>29911.800000</td>\n",
       "      <td>4145.400000</td>\n",
       "      <td>1035.600000</td>\n",
       "      <td>2294.000000</td>\n",
       "      <td>851.900000</td>\n",
       "      <td>39.700000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.345000</td>\n",
       "      <td>1.655000</td>\n",
       "      <td>1.940000</td>\n",
       "      <td>1.176000</td>\n",
       "      <td>1.533000</td>\n",
       "      <td>1.483290</td>\n",
       "      <td>6.279600e+05</td>\n",
       "      <td>8.329815e+05</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1057.250000</td>\n",
       "      <td>7031.625000</td>\n",
       "      <td>243.200000</td>\n",
       "      <td>12909.875000</td>\n",
       "      <td>47359.675000</td>\n",
       "      <td>6239.425000</td>\n",
       "      <td>2984.500000</td>\n",
       "      <td>4008.125000</td>\n",
       "      <td>1764.700000</td>\n",
       "      <td>941.825000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.309000</td>\n",
       "      <td>2.510000</td>\n",
       "      <td>3.360000</td>\n",
       "      <td>2.105000</td>\n",
       "      <td>2.720000</td>\n",
       "      <td>2.274935</td>\n",
       "      <td>9.957585e+05</td>\n",
       "      <td>1.404471e+06</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2113.500000</td>\n",
       "      <td>10669.950000</td>\n",
       "      <td>385.800000</td>\n",
       "      <td>14277.000000</td>\n",
       "      <td>51333.650000</td>\n",
       "      <td>7032.150000</td>\n",
       "      <td>3294.050000</td>\n",
       "      <td>4438.100000</td>\n",
       "      <td>1940.100000</td>\n",
       "      <td>1225.450000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.440500</td>\n",
       "      <td>2.685000</td>\n",
       "      <td>3.586500</td>\n",
       "      <td>2.297000</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>2.383375</td>\n",
       "      <td>1.075919e+06</td>\n",
       "      <td>1.511767e+06</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3169.750000</td>\n",
       "      <td>17332.650000</td>\n",
       "      <td>720.825000</td>\n",
       "      <td>15959.725000</td>\n",
       "      <td>56287.775000</td>\n",
       "      <td>7977.400000</td>\n",
       "      <td>3655.125000</td>\n",
       "      <td>4963.025000</td>\n",
       "      <td>2128.000000</td>\n",
       "      <td>1780.225000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.562750</td>\n",
       "      <td>2.851000</td>\n",
       "      <td>3.790000</td>\n",
       "      <td>2.476000</td>\n",
       "      <td>2.975000</td>\n",
       "      <td>2.483142</td>\n",
       "      <td>1.168888e+06</td>\n",
       "      <td>1.625445e+06</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4226.000000</td>\n",
       "      <td>79812.500000</td>\n",
       "      <td>7533.800000</td>\n",
       "      <td>35042.500000</td>\n",
       "      <td>79948.200000</td>\n",
       "      <td>13008.300000</td>\n",
       "      <td>6018.000000</td>\n",
       "      <td>8446.100000</td>\n",
       "      <td>4357.700000</td>\n",
       "      <td>4461.600000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.996000</td>\n",
       "      <td>3.928000</td>\n",
       "      <td>4.487000</td>\n",
       "      <td>3.123000</td>\n",
       "      <td>3.482000</td>\n",
       "      <td>2.803730</td>\n",
       "      <td>1.545129e+06</td>\n",
       "      <td>2.075213e+06</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>9.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 141 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              S.No  Left-Lateral-Ventricle  Left-Inf-Lat-Vent  \\\n",
       "count  4226.000000             4226.000000        4226.000000   \n",
       "mean   2113.500000            13370.040795         574.849716   \n",
       "std    1220.085448             9194.928348         594.590387   \n",
       "min       1.000000             2204.100000           0.000000   \n",
       "25%    1057.250000             7031.625000         243.200000   \n",
       "50%    2113.500000            10669.950000         385.800000   \n",
       "75%    3169.750000            17332.650000         720.825000   \n",
       "max    4226.000000            79812.500000        7533.800000   \n",
       "\n",
       "       Left-Cerebellum-White-Matter  Left-Cerebellum-Cortex  Left-Thalamus  \\\n",
       "count                   4226.000000             4226.000000    4226.000000   \n",
       "mean                   14646.696711            52002.811571    7164.947539   \n",
       "std                     2622.868798             6378.435917    1207.229615   \n",
       "min                     6920.100000            29911.800000    4145.400000   \n",
       "25%                    12909.875000            47359.675000    6239.425000   \n",
       "50%                    14277.000000            51333.650000    7032.150000   \n",
       "75%                    15959.725000            56287.775000    7977.400000   \n",
       "max                    35042.500000            79948.200000   13008.300000   \n",
       "\n",
       "       Left-Caudate  Left-Putamen  Left-Pallidum  3rd-Ventricle  ...  \\\n",
       "count   4226.000000   4226.000000    4226.000000    4226.000000  ...   \n",
       "mean    3337.653526   4505.158755    1958.214458    1418.947373  ...   \n",
       "std      502.352001    713.658580     287.139826     635.143286  ...   \n",
       "min     1035.600000   2294.000000     851.900000      39.700000  ...   \n",
       "25%     2984.500000   4008.125000    1764.700000     941.825000  ...   \n",
       "50%     3294.050000   4438.100000    1940.100000    1225.450000  ...   \n",
       "75%     3655.125000   4963.025000    2128.000000    1780.225000  ...   \n",
       "max     6018.000000   8446.100000    4357.700000    4461.600000  ...   \n",
       "\n",
       "       rh_supramarginal_thickness  rh_frontalpole_thickness  \\\n",
       "count                 4226.000000               4226.000000   \n",
       "mean                     2.429779                  2.684327   \n",
       "std                      0.185543                  0.275245   \n",
       "min                      1.345000                  1.655000   \n",
       "25%                      2.309000                  2.510000   \n",
       "50%                      2.440500                  2.685000   \n",
       "75%                      2.562750                  2.851000   \n",
       "max                      2.996000                  3.928000   \n",
       "\n",
       "       rh_temporalpole_thickness  rh_transversetemporal_thickness  \\\n",
       "count                4226.000000                      4226.000000   \n",
       "mean                    3.555803                         2.288283   \n",
       "std                     0.332094                         0.269851   \n",
       "min                     1.940000                         1.176000   \n",
       "25%                     3.360000                         2.105000   \n",
       "50%                     3.586500                         2.297000   \n",
       "75%                     3.790000                         2.476000   \n",
       "max                     4.487000                         3.123000   \n",
       "\n",
       "       rh_insula_thickness  rh_MeanThickness_thickness  BrainSegVolNotVent.2  \\\n",
       "count          4226.000000                 4226.000000          4.226000e+03   \n",
       "mean              2.846123                    2.372266          1.085468e+06   \n",
       "std               0.195038                    0.146944          1.248881e+05   \n",
       "min               1.533000                    1.483290          6.279600e+05   \n",
       "25%               2.720000                    2.274935          9.957585e+05   \n",
       "50%               2.851000                    2.383375          1.075919e+06   \n",
       "75%               2.975000                    2.483142          1.168888e+06   \n",
       "max               3.482000                    2.803730          1.545129e+06   \n",
       "\n",
       "             eTIV.1          Age      dataset  \n",
       "count  4.226000e+03  4226.000000  4226.000000  \n",
       "mean   1.514925e+06    58.374586     4.533838  \n",
       "std    1.651798e+05    20.064099     3.057928  \n",
       "min    8.329815e+05    18.000000     1.000000  \n",
       "25%    1.404471e+06    43.000000     1.000000  \n",
       "50%    1.511767e+06    61.000000     4.000000  \n",
       "75%    1.625445e+06    76.000000     8.000000  \n",
       "max    2.075213e+06    96.000000     9.000000  \n",
       "\n",
       "[8 rows x 141 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel('../../data_sets/Volumetric_features.xlsx')\n",
    "data_feat = pd.DataFrame(data, columns = data.columns[:-1])\n",
    "data_feat = data_feat.drop(['S.No','Age'], axis=1)\n",
    "\n",
    "data.head(5)\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5353dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method NDFrame.head of       rh_MeanThickness_thickness  CerebralWhiteMatterVol  \\\n",
      "0                       1.754402                1.293661   \n",
      "1                       1.417516                1.506793   \n",
      "2                       2.060537                1.356492   \n",
      "3                       4.321472                1.316560   \n",
      "4                       3.432616                1.645478   \n",
      "...                          ...                     ...   \n",
      "4221                    3.508241                2.349696   \n",
      "4222                    4.445945               -2.409496   \n",
      "4223                    8.016491                2.326577   \n",
      "4224                   -0.596625                2.547033   \n",
      "4225                   -2.307309               -3.481649   \n",
      "\n",
      "      Left-Lateral-Ventricle  lh_pericalcarine_thickness  SurfaceHoles  \\\n",
      "0                   1.400035                   -1.478872     -1.847496   \n",
      "1                   1.653987                   -0.614048     -1.384830   \n",
      "2                   1.488951                   -1.055340     -1.174998   \n",
      "3                   1.100195                   -0.240310     -1.374512   \n",
      "4                   2.126633                   -0.804793     -1.322204   \n",
      "...                      ...                         ...           ...   \n",
      "4221                0.438968                    0.939801      0.641966   \n",
      "4222                1.632551                   -3.114777      1.459036   \n",
      "4223                3.899470                   -2.705463      1.087679   \n",
      "4224                6.998604                   -0.535334      2.483747   \n",
      "4225                2.207923                   -2.811137      1.635708   \n",
      "\n",
      "      CC_Posterior  rh_caudalanteriorcingulate_thickness  CC_Posterior  \\\n",
      "0         2.543808                             -1.131069     -0.411168   \n",
      "1         2.367862                             -1.435384     -0.854679   \n",
      "2         2.164486                             -2.159995     -0.820318   \n",
      "3         2.415121                             -1.136740     -1.573520   \n",
      "4         3.079020                             -1.200164     -0.815543   \n",
      "...            ...                                   ...           ...   \n",
      "4221     -0.407802                             -1.097621     -0.941874   \n",
      "4222     -0.235889                             -0.572814     -0.904172   \n",
      "4223      2.369678                              1.469668      2.351290   \n",
      "4224      4.642015                              2.408468      1.002992   \n",
      "4225      1.923662                              1.102109     -0.154756   \n",
      "\n",
      "      Right-Caudate  lh_parahippocampal_thickness  MaskVol-to-eTIV  \\\n",
      "0         -0.375874                      1.575706        -0.215381   \n",
      "1         -0.772121                      1.730385         0.194947   \n",
      "2         -1.129607                      1.481564         0.072066   \n",
      "3         -0.824431                      1.332201         0.288752   \n",
      "4         -0.544645                      1.522294        -0.082370   \n",
      "...             ...                           ...              ...   \n",
      "4221      -0.635432                     -0.819785        -0.181214   \n",
      "4222       0.550401                     -0.168608        -0.685791   \n",
      "4223       1.246512                      0.535511         1.479203   \n",
      "4224       4.405416                      1.482941         0.689009   \n",
      "4225       2.158918                      0.192730        -1.116467   \n",
      "\n",
      "      Brain-Stem  Left-vessel  Right-vessel  non-WM-hypointensities  \\\n",
      "0       0.175385    -1.594659      0.590112               -0.419819   \n",
      "1       0.374460    -1.466169      0.229012               -0.858767   \n",
      "2       0.312901    -0.851789      0.768033               -0.818180   \n",
      "3      -0.333657    -1.369837      0.511955               -0.327659   \n",
      "4       0.416392    -1.480764      1.228200               -0.870006   \n",
      "...          ...          ...           ...                     ...   \n",
      "4221   -0.280696     1.676497      0.170626               -0.078760   \n",
      "4222   -0.963481     0.055946     -0.672095               -0.778529   \n",
      "4223   -0.578519    -0.610596     -1.582753               -0.047811   \n",
      "4224    2.014027     0.789826     -1.505699               -1.233256   \n",
      "4225   -0.183326    -0.386018     -1.283849               -0.609903   \n",
      "\n",
      "      rh_isthmuscingulate_thickness  5th-Ventricle  non-WM-hypointensities  \\\n",
      "0                          0.490388       0.210059               -0.255816   \n",
      "1                          0.179173      -0.294345               -0.241843   \n",
      "2                          0.456272      -0.468695               -0.019350   \n",
      "3                          0.142232      -0.358880               -0.033206   \n",
      "4                          0.017986       0.276737               -0.136212   \n",
      "...                             ...            ...                     ...   \n",
      "4221                       0.042539      -0.917933               -0.526400   \n",
      "4222                       0.211769       0.361659               -0.563364   \n",
      "4223                       0.781419       1.184685                1.157254   \n",
      "4224                       1.053152       0.682845                1.401773   \n",
      "4225                       1.884649       1.380549                1.316358   \n",
      "\n",
      "      5th-Ventricle  5th-Ventricle  \n",
      "0          0.745396      -0.548675  \n",
      "1          1.224005      -0.287636  \n",
      "2          0.983764       0.133955  \n",
      "3          0.886275      -0.056838  \n",
      "4          1.185145      -0.515933  \n",
      "...             ...            ...  \n",
      "4221      -0.588001       0.172580  \n",
      "4222       0.975863      -2.166769  \n",
      "4223      -0.658737      -0.717793  \n",
      "4224      -1.132188      -1.078134  \n",
      "4225      -0.489069      -0.597589  \n",
      "\n",
      "[4226 rows x 20 columns]>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rh_MeanThickness_thickness</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>Left-Lateral-Ventricle</th>\n",
       "      <th>lh_pericalcarine_thickness</th>\n",
       "      <th>SurfaceHoles</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
       "      <th>CC_Posterior</th>\n",
       "      <th>Right-Caudate</th>\n",
       "      <th>lh_parahippocampal_thickness</th>\n",
       "      <th>MaskVol-to-eTIV</th>\n",
       "      <th>Brain-Stem</th>\n",
       "      <th>Left-vessel</th>\n",
       "      <th>Right-vessel</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>rh_isthmuscingulate_thickness</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>non-WM-hypointensities</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "      <th>5th-Ventricle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.754402</td>\n",
       "      <td>1.293661</td>\n",
       "      <td>1.400035</td>\n",
       "      <td>-1.478872</td>\n",
       "      <td>-1.847496</td>\n",
       "      <td>2.543808</td>\n",
       "      <td>-1.131069</td>\n",
       "      <td>-0.411168</td>\n",
       "      <td>-0.375874</td>\n",
       "      <td>1.575706</td>\n",
       "      <td>-0.215381</td>\n",
       "      <td>0.175385</td>\n",
       "      <td>-1.594659</td>\n",
       "      <td>0.590112</td>\n",
       "      <td>-0.419819</td>\n",
       "      <td>0.490388</td>\n",
       "      <td>0.210059</td>\n",
       "      <td>-0.255816</td>\n",
       "      <td>0.745396</td>\n",
       "      <td>-0.548675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.417516</td>\n",
       "      <td>1.506793</td>\n",
       "      <td>1.653987</td>\n",
       "      <td>-0.614048</td>\n",
       "      <td>-1.384830</td>\n",
       "      <td>2.367862</td>\n",
       "      <td>-1.435384</td>\n",
       "      <td>-0.854679</td>\n",
       "      <td>-0.772121</td>\n",
       "      <td>1.730385</td>\n",
       "      <td>0.194947</td>\n",
       "      <td>0.374460</td>\n",
       "      <td>-1.466169</td>\n",
       "      <td>0.229012</td>\n",
       "      <td>-0.858767</td>\n",
       "      <td>0.179173</td>\n",
       "      <td>-0.294345</td>\n",
       "      <td>-0.241843</td>\n",
       "      <td>1.224005</td>\n",
       "      <td>-0.287636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.060537</td>\n",
       "      <td>1.356492</td>\n",
       "      <td>1.488951</td>\n",
       "      <td>-1.055340</td>\n",
       "      <td>-1.174998</td>\n",
       "      <td>2.164486</td>\n",
       "      <td>-2.159995</td>\n",
       "      <td>-0.820318</td>\n",
       "      <td>-1.129607</td>\n",
       "      <td>1.481564</td>\n",
       "      <td>0.072066</td>\n",
       "      <td>0.312901</td>\n",
       "      <td>-0.851789</td>\n",
       "      <td>0.768033</td>\n",
       "      <td>-0.818180</td>\n",
       "      <td>0.456272</td>\n",
       "      <td>-0.468695</td>\n",
       "      <td>-0.019350</td>\n",
       "      <td>0.983764</td>\n",
       "      <td>0.133955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.321472</td>\n",
       "      <td>1.316560</td>\n",
       "      <td>1.100195</td>\n",
       "      <td>-0.240310</td>\n",
       "      <td>-1.374512</td>\n",
       "      <td>2.415121</td>\n",
       "      <td>-1.136740</td>\n",
       "      <td>-1.573520</td>\n",
       "      <td>-0.824431</td>\n",
       "      <td>1.332201</td>\n",
       "      <td>0.288752</td>\n",
       "      <td>-0.333657</td>\n",
       "      <td>-1.369837</td>\n",
       "      <td>0.511955</td>\n",
       "      <td>-0.327659</td>\n",
       "      <td>0.142232</td>\n",
       "      <td>-0.358880</td>\n",
       "      <td>-0.033206</td>\n",
       "      <td>0.886275</td>\n",
       "      <td>-0.056838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.432616</td>\n",
       "      <td>1.645478</td>\n",
       "      <td>2.126633</td>\n",
       "      <td>-0.804793</td>\n",
       "      <td>-1.322204</td>\n",
       "      <td>3.079020</td>\n",
       "      <td>-1.200164</td>\n",
       "      <td>-0.815543</td>\n",
       "      <td>-0.544645</td>\n",
       "      <td>1.522294</td>\n",
       "      <td>-0.082370</td>\n",
       "      <td>0.416392</td>\n",
       "      <td>-1.480764</td>\n",
       "      <td>1.228200</td>\n",
       "      <td>-0.870006</td>\n",
       "      <td>0.017986</td>\n",
       "      <td>0.276737</td>\n",
       "      <td>-0.136212</td>\n",
       "      <td>1.185145</td>\n",
       "      <td>-0.515933</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rh_MeanThickness_thickness  CerebralWhiteMatterVol  Left-Lateral-Ventricle  \\\n",
       "0                    1.754402                1.293661                1.400035   \n",
       "1                    1.417516                1.506793                1.653987   \n",
       "2                    2.060537                1.356492                1.488951   \n",
       "3                    4.321472                1.316560                1.100195   \n",
       "4                    3.432616                1.645478                2.126633   \n",
       "\n",
       "   lh_pericalcarine_thickness  SurfaceHoles  CC_Posterior  \\\n",
       "0                   -1.478872     -1.847496      2.543808   \n",
       "1                   -0.614048     -1.384830      2.367862   \n",
       "2                   -1.055340     -1.174998      2.164486   \n",
       "3                   -0.240310     -1.374512      2.415121   \n",
       "4                   -0.804793     -1.322204      3.079020   \n",
       "\n",
       "   rh_caudalanteriorcingulate_thickness  CC_Posterior  Right-Caudate  \\\n",
       "0                             -1.131069     -0.411168      -0.375874   \n",
       "1                             -1.435384     -0.854679      -0.772121   \n",
       "2                             -2.159995     -0.820318      -1.129607   \n",
       "3                             -1.136740     -1.573520      -0.824431   \n",
       "4                             -1.200164     -0.815543      -0.544645   \n",
       "\n",
       "   lh_parahippocampal_thickness  MaskVol-to-eTIV  Brain-Stem  Left-vessel  \\\n",
       "0                      1.575706        -0.215381    0.175385    -1.594659   \n",
       "1                      1.730385         0.194947    0.374460    -1.466169   \n",
       "2                      1.481564         0.072066    0.312901    -0.851789   \n",
       "3                      1.332201         0.288752   -0.333657    -1.369837   \n",
       "4                      1.522294        -0.082370    0.416392    -1.480764   \n",
       "\n",
       "   Right-vessel  non-WM-hypointensities  rh_isthmuscingulate_thickness  \\\n",
       "0      0.590112               -0.419819                       0.490388   \n",
       "1      0.229012               -0.858767                       0.179173   \n",
       "2      0.768033               -0.818180                       0.456272   \n",
       "3      0.511955               -0.327659                       0.142232   \n",
       "4      1.228200               -0.870006                       0.017986   \n",
       "\n",
       "   5th-Ventricle  non-WM-hypointensities  5th-Ventricle  5th-Ventricle  \n",
       "0       0.210059               -0.255816       0.745396      -0.548675  \n",
       "1      -0.294345               -0.241843       1.224005      -0.287636  \n",
       "2      -0.468695               -0.019350       0.983764       0.133955  \n",
       "3      -0.358880               -0.033206       0.886275      -0.056838  \n",
       "4       0.276737               -0.136212       1.185145      -0.515933  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "x = scaler.fit_transform(data_feat)\n",
    "n = 20\n",
    "pca = PCA(n_components=n)\n",
    "pca_data = pca.fit_transform(x)\n",
    "\n",
    "labels = data_feat.columns.values.tolist()\n",
    "label_index = [np.abs(pca.components_[i]).argmax() for i in range(n)]\n",
    "columns = [labels[label_index[i]] for i in range(n)]\n",
    "\n",
    "pca_df = pd.DataFrame(data=pca_data, columns=columns)\n",
    "print(pca_df.head)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a32919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape is: (3380, 20)\n",
      "y_train shape is: (3380,) \n",
      "\n",
      "x_val shape is: (634, 20)\n",
      "y_val shape is: (634,) \n",
      "\n",
      "x_test shape is: (212, 20)\n",
      "y_test shape is: (212,)\n"
     ]
    }
   ],
   "source": [
    "# Split for validation --> train, val, test = 80/15/5\n",
    "# train to test (val and test) --> include random shuffle\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(pca_df, data['Age'], test_size=0.20, random_state=33)\n",
    "\n",
    "# (20% of total dataset -> 75% validation = 15% total, 25% validation = 5% total\n",
    "# val and test --> include random shuffle\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_validation, y_validation, test_size=0.25, random_state=33)\n",
    "\n",
    "print(\"x_train shape is:\",x_train.shape)\n",
    "print(\"y_train shape is:\",y_train.shape, \"\\n\")\n",
    "print(\"x_val shape is:\",x_val.shape)\n",
    "print(\"y_val shape is:\",y_val.shape, \"\\n\")\n",
    "print(\"x_test shape is:\",x_test.shape)\n",
    "print(\"y_test shape is:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f198fb3",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5959ecb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_25 (Dense)            (None, 64)                1344      \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 128)               8320      \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 256)               33024     \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 128)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 64)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 83,905\n",
      "Trainable params: 83,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# end with 3 neurons for each class --> 1 (Normal), 2 (Suspect) and 3 (Pathological)\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Input(shape=x_train.shape[1], name='input'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(256))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(64))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(1, activation='linear', name='output'))\n",
    "\n",
    "\n",
    "opt = Adam(learning_rate=0.001)\n",
    "\n",
    "model.compile(\n",
    "            loss='mean_absolute_error',\n",
    "            optimizer=opt,\n",
    "            metrics= ['mean_absolute_error']\n",
    "            )\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6be966",
   "metadata": {},
   "source": [
    "# 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3abb1918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "27/27 [==============================] - 1s 12ms/step - loss: 3.7670 - msle: 3.7670 - val_loss: 0.3715 - val_msle: 0.3715\n",
      "Epoch 2/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.3261 - msle: 0.3261 - val_loss: 0.2001 - val_msle: 0.2001\n",
      "Epoch 3/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.1860 - msle: 0.1860 - val_loss: 0.1632 - val_msle: 0.1632\n",
      "Epoch 4/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1463 - msle: 0.1463 - val_loss: 0.1355 - val_msle: 0.1355\n",
      "Epoch 5/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1208 - msle: 0.1208 - val_loss: 0.1158 - val_msle: 0.1158\n",
      "Epoch 6/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.1029 - msle: 0.1029 - val_loss: 0.1012 - val_msle: 0.1012\n",
      "Epoch 7/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0908 - msle: 0.0908 - val_loss: 0.0920 - val_msle: 0.0920\n",
      "Epoch 8/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0825 - msle: 0.0825 - val_loss: 0.0836 - val_msle: 0.0836\n",
      "Epoch 9/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0757 - msle: 0.0757 - val_loss: 0.0776 - val_msle: 0.0776\n",
      "Epoch 10/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0701 - msle: 0.0701 - val_loss: 0.0710 - val_msle: 0.0710\n",
      "Epoch 11/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0652 - msle: 0.0652 - val_loss: 0.0667 - val_msle: 0.0667\n",
      "Epoch 12/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0603 - msle: 0.0603 - val_loss: 0.0628 - val_msle: 0.0628\n",
      "Epoch 13/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0567 - msle: 0.0567 - val_loss: 0.0594 - val_msle: 0.0594\n",
      "Epoch 14/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0533 - msle: 0.0533 - val_loss: 0.0553 - val_msle: 0.0553\n",
      "Epoch 15/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0490 - msle: 0.0490 - val_loss: 0.0526 - val_msle: 0.0526\n",
      "Epoch 16/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0462 - msle: 0.0462 - val_loss: 0.0509 - val_msle: 0.0509\n",
      "Epoch 17/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0433 - msle: 0.0433 - val_loss: 0.0471 - val_msle: 0.0471\n",
      "Epoch 18/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0410 - msle: 0.0410 - val_loss: 0.0450 - val_msle: 0.0450\n",
      "Epoch 19/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0384 - msle: 0.0384 - val_loss: 0.0445 - val_msle: 0.0445\n",
      "Epoch 20/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0359 - msle: 0.0359 - val_loss: 0.0417 - val_msle: 0.0417\n",
      "Epoch 21/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0338 - msle: 0.0338 - val_loss: 0.0402 - val_msle: 0.0402\n",
      "Epoch 22/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0319 - msle: 0.0319 - val_loss: 0.0393 - val_msle: 0.0393\n",
      "Epoch 23/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0302 - msle: 0.0302 - val_loss: 0.0375 - val_msle: 0.0375\n",
      "Epoch 24/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0292 - msle: 0.0292 - val_loss: 0.0386 - val_msle: 0.0386\n",
      "Epoch 25/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0276 - msle: 0.0276 - val_loss: 0.0367 - val_msle: 0.0367\n",
      "Epoch 26/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0261 - msle: 0.0261 - val_loss: 0.0356 - val_msle: 0.0356\n",
      "Epoch 27/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0251 - msle: 0.0251 - val_loss: 0.0349 - val_msle: 0.0349\n",
      "Epoch 28/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0241 - msle: 0.0241 - val_loss: 0.0352 - val_msle: 0.0352\n",
      "Epoch 29/100\n",
      "27/27 [==============================] - 0s 9ms/step - loss: 0.0232 - msle: 0.0232 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 30/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0223 - msle: 0.0223 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 31/100\n",
      "27/27 [==============================] - 0s 10ms/step - loss: 0.0221 - msle: 0.0221 - val_loss: 0.0352 - val_msle: 0.0352\n",
      "Epoch 32/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0212 - msle: 0.0212 - val_loss: 0.0346 - val_msle: 0.0346\n",
      "Epoch 33/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0201 - msle: 0.0201 - val_loss: 0.0341 - val_msle: 0.0341\n",
      "Epoch 34/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0197 - msle: 0.0197 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 35/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0188 - msle: 0.0188 - val_loss: 0.0354 - val_msle: 0.0354\n",
      "Epoch 36/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0183 - msle: 0.0183 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 37/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0180 - msle: 0.0180 - val_loss: 0.0337 - val_msle: 0.0337\n",
      "Epoch 38/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0174 - msle: 0.0174 - val_loss: 0.0334 - val_msle: 0.0334\n",
      "Epoch 39/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0169 - msle: 0.0169 - val_loss: 0.0337 - val_msle: 0.0337\n",
      "Epoch 40/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0163 - msle: 0.0163 - val_loss: 0.0345 - val_msle: 0.0345\n",
      "Epoch 41/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0162 - msle: 0.0162 - val_loss: 0.0356 - val_msle: 0.0356\n",
      "Epoch 42/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0158 - msle: 0.0158 - val_loss: 0.0332 - val_msle: 0.0332\n",
      "Epoch 43/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0152 - msle: 0.0152 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 44/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0152 - msle: 0.0152 - val_loss: 0.0333 - val_msle: 0.0333\n",
      "Epoch 45/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0146 - msle: 0.0146 - val_loss: 0.0340 - val_msle: 0.0340\n",
      "Epoch 46/100\n",
      "27/27 [==============================] - 0s 8ms/step - loss: 0.0143 - msle: 0.0143 - val_loss: 0.0359 - val_msle: 0.0359\n",
      "Epoch 47/100\n",
      "27/27 [==============================] - 0s 7ms/step - loss: 0.0139 - msle: 0.0139 - val_loss: 0.0351 - val_msle: 0.0351\n",
      "Epoch 48/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0136 - msle: 0.0136 - val_loss: 0.0344 - val_msle: 0.0344\n",
      "Epoch 49/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0133 - msle: 0.0133 - val_loss: 0.0339 - val_msle: 0.0339\n",
      "Epoch 50/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0129 - msle: 0.0129 - val_loss: 0.0342 - val_msle: 0.0342\n",
      "Epoch 51/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0124 - msle: 0.0124 - val_loss: 0.0346 - val_msle: 0.0346\n",
      "Epoch 52/100\n",
      "27/27 [==============================] - 0s 6ms/step - loss: 0.0125 - msle: 0.0125 - val_loss: 0.0345 - val_msle: 0.0345\n"
     ]
    }
   ],
   "source": [
    "earlystopping = callbacks.EarlyStopping(monitor=\"val_loss\", mode=\"min\", \n",
    "                                        patience=10, restore_best_weights = True)\n",
    "# loss function\n",
    "msle = MeanSquaredLogarithmicError()\n",
    "\n",
    "model.compile(\n",
    "    loss=msle, \n",
    "    optimizer=Adam(learning_rate=0.001), \n",
    "    metrics=['msle']\n",
    ")\n",
    "# train the model\n",
    "hist = model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=128,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val), \n",
    "    callbacks = [earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d408819",
   "metadata": {},
   "source": [
    "# 6. Analysis & Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e48fb03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance: 0.828164883556715\n",
      "Max Error: 33.99394226074219\n",
      "Mean absolute error: 6.014110430231634\n",
      "Mean squared error: 67.98508997972411\n",
      "Root Mean squared error: 8.245307148901373\n",
      "R2: 0.8266078968373053\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "\n",
    "print(\"Explained variance: \" + str(metrics.explained_variance_score(y_test, y_pred)))\n",
    "print(\"Max Error: \" + str(metrics.max_error(y_test, y_pred)))\n",
    "print(\"Mean absolute error: \" + str(metrics.mean_absolute_error(y_test, y_pred)))\n",
    "print(\"Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred)))\n",
    "print(\"Root Mean squared error: \" + str(metrics.mean_squared_error(y_test, y_pred, squared=False)))\n",
    "print(\"R2: \" + str(metrics.r2_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4866cfdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = list(range(52))\n",
    "vy = hist.history['val_loss']\n",
    "ty = hist.history['loss']\n",
    "\n",
    "plt.plot( x, vy, label='Val Mean Average Error')\n",
    "plt.plot( x, ty, label='Train Mean Average Error')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6d4cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
